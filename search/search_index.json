{"config":{"lang":["en"],"min_search_length":2,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to the documentation hub for the CMS Machine Learning Group! The goal of this page is to provide CMS analyzers a centralized place to gather machine learning information relevant to their work. However, we are not seeking to rewrite external documentation. Whenever applicable, we will link to external documentation, such as the iML groups HEP Living Review or their ML Resources repository. What you will find here are pages covering: ML best practices How to optimize a NN Common pitfalls for CMS analyzers Direct and indirect inferencing using a variety of ML packages How to get a model integrated into CMSSW And much more! If you think we are missing some important information, please contact the ML Knowledge Subgroup !","title":"Home"},{"location":"Resources/Cloud_Resources/index.html","text":"Work in progress.","title":"Cloud Resources"},{"location":"Resources/FPGA_Resources/index.html","text":"Work in progress.","title":"FPGA Resource"},{"location":"Resources/GPU_Resources/index.html","text":"CMS GPU Resources \u00b6 SWAN lxplus-gpu.cern.ch HTCondor ml.cern.ch","title":"GPU Resources"},{"location":"Resources/GPU_Resources/index.html#cms-gpu-resources","text":"SWAN lxplus-gpu.cern.ch HTCondor ml.cern.ch","title":"CMS GPU Resources"},{"location":"Resources/GPU_Resources/CMS_Resources/SWAN.html","text":"SWAN \u00b6 Preparation \u00b6 Registration: To require GPU resources for SWAN: According to this thread , one can create a ticket through this link to ask for GPU support at SWAN, it is now in beta version and limited to a small scale. 2. Setup SWAN with GPU resources: Once the registration is done, one can login SWAN with Kerberes8 support and then create his SWAN environment. \ud83d\udca1 Note: When configuring the SWAN environment you will be given your choice of software stack. Be careful to use a software release with GPU support as well as an appropriate CUDA version. If you need to install additional software, it must be compatible with your chosen CUDA version. Another important option is the environment script, which will be discussed later in this document. Working with SWAN \u00b6 After creation, one will browse the SWAN main directory My Project where all existing projects are displayed. A new project can be created by clicking the upper right \"+\" button. After creation one will be redirected to the newly created project, at which point the \"+\" button on the upper right panel can be used for creating new notebook . It is possible to use the terminal for installing new packages or monitoring computational resources. For package installation, one can install packages with package management tools, e.g. pip for python . To use the installed packages, you will need to wrap the environment configuration in a scrip, which will be executed by SWAN. Detailed documentation can be found by clicking the upper right \"?\" button. In addition to using top and htop to monitor ordinary resources, you can use nvidia-smi to monitor GPU usage. Examples \u00b6 After installing package, you can then use GPU based machine learning algorithms. Two examples are supplied as an example. The first example aims at using a CNN to perform handwritten digits classification with MNIST dataset. The whole notebook can be found at PytorchMNIST . This example is modified from an official pytorch example . The second example is modified from the simple MLP example from weaver-benchmark . The whole notebook can be found at TopTaggingMLP .","title":"SWAN"},{"location":"Resources/GPU_Resources/CMS_Resources/SWAN.html#swan","text":"","title":"SWAN"},{"location":"Resources/GPU_Resources/CMS_Resources/SWAN.html#preparation","text":"Registration: To require GPU resources for SWAN: According to this thread , one can create a ticket through this link to ask for GPU support at SWAN, it is now in beta version and limited to a small scale. 2. Setup SWAN with GPU resources: Once the registration is done, one can login SWAN with Kerberes8 support and then create his SWAN environment. \ud83d\udca1 Note: When configuring the SWAN environment you will be given your choice of software stack. Be careful to use a software release with GPU support as well as an appropriate CUDA version. If you need to install additional software, it must be compatible with your chosen CUDA version. Another important option is the environment script, which will be discussed later in this document.","title":"Preparation"},{"location":"Resources/GPU_Resources/CMS_Resources/SWAN.html#working-with-swan","text":"After creation, one will browse the SWAN main directory My Project where all existing projects are displayed. A new project can be created by clicking the upper right \"+\" button. After creation one will be redirected to the newly created project, at which point the \"+\" button on the upper right panel can be used for creating new notebook . It is possible to use the terminal for installing new packages or monitoring computational resources. For package installation, one can install packages with package management tools, e.g. pip for python . To use the installed packages, you will need to wrap the environment configuration in a scrip, which will be executed by SWAN. Detailed documentation can be found by clicking the upper right \"?\" button. In addition to using top and htop to monitor ordinary resources, you can use nvidia-smi to monitor GPU usage.","title":"Working with SWAN"},{"location":"Resources/GPU_Resources/CMS_Resources/SWAN.html#examples","text":"After installing package, you can then use GPU based machine learning algorithms. Two examples are supplied as an example. The first example aims at using a CNN to perform handwritten digits classification with MNIST dataset. The whole notebook can be found at PytorchMNIST . This example is modified from an official pytorch example . The second example is modified from the simple MLP example from weaver-benchmark . The whole notebook can be found at TopTaggingMLP .","title":"Examples"},{"location":"Resources/GPU_Resources/CMS_Resources/lxplus_condor.html","text":"HTCondor With GPU resources \u00b6 In general, HTCondor supports GPU jobs if there are some worker nodes which are configured with GPU devices. CMS Connect and lxplus both have access to worker nodes equipped with GPUs. How to require GPUs in HTCondor \u00b6 People can require their jobs to have GPU support by adding the following requirements to the condor submission file. request_gpus = n # n equal to the number of GPUs required Further documentation \u00b6 There are good materials providing detailed documentation on how to run HTCondor jobs with GPU support at both machines. A complete documentation can be found from the GPUs section in CERN Batch Docs . Where a Tensorflow example is supplied. This documentation also contains instructions on advanced HTCondor configuration, for instance constraining GPU device or CUDA version. A good example on submitting GPU HTCondor job @ Lxplus is the weaver-benchmark project. It provides a concrete example on how to setup environment for weaver framework and operate trainning and testing process within a single job. Detailed description can be found at section ParticleNet of this documentation. In principle, this example can be run elsewhere as HTCondor jobs. However, paths to the datasets should be modified to meet the requirements. CMS Connect also provides a documentation on GPU job submission. In this documentation there is also a Tensorflow example. When submitting GPU jobs @ CMS Connect, especially for Machine Learning purpose, EOS space @ CERN are not accessible as a directory, therefore one should consider using xrootd utilities as documented in this page","title":"HTCondor With GPU resources"},{"location":"Resources/GPU_Resources/CMS_Resources/lxplus_condor.html#htcondor-with-gpu-resources","text":"In general, HTCondor supports GPU jobs if there are some worker nodes which are configured with GPU devices. CMS Connect and lxplus both have access to worker nodes equipped with GPUs.","title":"HTCondor With GPU resources"},{"location":"Resources/GPU_Resources/CMS_Resources/lxplus_condor.html#how-to-require-gpus-in-htcondor","text":"People can require their jobs to have GPU support by adding the following requirements to the condor submission file. request_gpus = n # n equal to the number of GPUs required","title":"How to require GPUs in HTCondor"},{"location":"Resources/GPU_Resources/CMS_Resources/lxplus_condor.html#further-documentation","text":"There are good materials providing detailed documentation on how to run HTCondor jobs with GPU support at both machines. A complete documentation can be found from the GPUs section in CERN Batch Docs . Where a Tensorflow example is supplied. This documentation also contains instructions on advanced HTCondor configuration, for instance constraining GPU device or CUDA version. A good example on submitting GPU HTCondor job @ Lxplus is the weaver-benchmark project. It provides a concrete example on how to setup environment for weaver framework and operate trainning and testing process within a single job. Detailed description can be found at section ParticleNet of this documentation. In principle, this example can be run elsewhere as HTCondor jobs. However, paths to the datasets should be modified to meet the requirements. CMS Connect also provides a documentation on GPU job submission. In this documentation there is also a Tensorflow example. When submitting GPU jobs @ CMS Connect, especially for Machine Learning purpose, EOS space @ CERN are not accessible as a directory, therefore one should consider using xrootd utilities as documented in this page","title":"Further documentation"},{"location":"Resources/GPU_Resources/CMS_Resources/lxplus_gpu.html","text":"lxplus-gpu.cern.ch \u00b6 How to use it? \u00b6 lxplus-gpu are special lxplus nodes with GPU support. You can access these nodes by executing ssh <your_user_name>@lxplus-gpu.cern.ch Software Environment \u00b6 Four examples are given below to show how to set up a software environment properly. Using LCG release software: after checking out an ideal software bundle with Cuda support at http://lcginfo.cern.ch/ , one can set up an LCG environment by executing source /cvmfs/sft.cern.ch/lcg/views/<name of bundle>/**x86_64-centos7-gcc8-opt**/setup.sh Using pip , especially with virtualenv : using pip only to install software may mess up the global environment. Thus, it is better to create a \"virtual environment\" with virtualenv in order to eliminate potential issues in the package environment. As on lxplus, the default virtualenv command is installed with python2 , it better to firstly install virtualenv with python3 pip3 install virtualenv --user # Add following line to .bashrc and re-log in or source .bashrc # export PATH=\"/afs/cern.ch/user/<first letter of your username>/<username>/.local/bin:$PATH\" Make sure you have virtualenv with python3 correctly. Then go to the desired directory and create a virtual environment virtualenv <env name> source <env name>/bin/activate # now you are inside the virtual environment, your shell prompt will begin with \"(<env name>)\" To install packages properly, one should carefully check the CUDA version with nvidia-smi (as shown in figure before), and then find a proper version, pytorch is used as an example. # Execute the command shown in your terminal pip3 install torch == 1 .10.0+cu113 torchvision == 0 .11.1+cu113 torchaudio == 0 .10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html pip3 install jupyterlab matplotlib scikit-hep # install other packages if they are needed 3. Using conda package manager: conda pacakge manager is more convenient to install and use. To begin with, obtaining an Anaconda or Miniconda installer for Linux x86_64 platform. Then execute it on Lxplus. Please note that if you update your shell configuration (e.g. .bashrc file) by conda init , you may encounter failure due to inconsistent environment configuration. Installing packages via conda also needs special consideration on selecting proper CUDA version as discussed in pip part. Container based solution: The unpacked.cern.ch mount on CVMFS contains many singularity images, some of which are suitable for machine learning applications. A description of each of the images is beyond the scope of this document. However, if you find an image which is useful for your application, you can use if by running a Singularity container with the appropriate options. For example: singularity run --nv --bind <bind_mount_path> /cvmfs/unpacked.cern.ch/<path_to_image> Examples \u00b6 After installing package, you can then use GPU based machine learning algorithms. Two examples are supplied as an example. The first example aims at using a CNN to perform handwritten digits classification with MNIST dataset. The whole notebook can be found at PytorchMNIST . This example is modified from an official pytorch example . The second example is modified from the simple MLP example from weaver-benchmark . The whole notebook can be found at TopTaggingMLP .","title":"lxplus-gpu.cern.ch"},{"location":"Resources/GPU_Resources/CMS_Resources/lxplus_gpu.html#lxplus-gpucernch","text":"","title":"lxplus-gpu.cern.ch"},{"location":"Resources/GPU_Resources/CMS_Resources/lxplus_gpu.html#how-to-use-it","text":"lxplus-gpu are special lxplus nodes with GPU support. You can access these nodes by executing ssh <your_user_name>@lxplus-gpu.cern.ch","title":"How to use it?"},{"location":"Resources/GPU_Resources/CMS_Resources/lxplus_gpu.html#software-environment","text":"Four examples are given below to show how to set up a software environment properly. Using LCG release software: after checking out an ideal software bundle with Cuda support at http://lcginfo.cern.ch/ , one can set up an LCG environment by executing source /cvmfs/sft.cern.ch/lcg/views/<name of bundle>/**x86_64-centos7-gcc8-opt**/setup.sh Using pip , especially with virtualenv : using pip only to install software may mess up the global environment. Thus, it is better to create a \"virtual environment\" with virtualenv in order to eliminate potential issues in the package environment. As on lxplus, the default virtualenv command is installed with python2 , it better to firstly install virtualenv with python3 pip3 install virtualenv --user # Add following line to .bashrc and re-log in or source .bashrc # export PATH=\"/afs/cern.ch/user/<first letter of your username>/<username>/.local/bin:$PATH\" Make sure you have virtualenv with python3 correctly. Then go to the desired directory and create a virtual environment virtualenv <env name> source <env name>/bin/activate # now you are inside the virtual environment, your shell prompt will begin with \"(<env name>)\" To install packages properly, one should carefully check the CUDA version with nvidia-smi (as shown in figure before), and then find a proper version, pytorch is used as an example. # Execute the command shown in your terminal pip3 install torch == 1 .10.0+cu113 torchvision == 0 .11.1+cu113 torchaudio == 0 .10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html pip3 install jupyterlab matplotlib scikit-hep # install other packages if they are needed 3. Using conda package manager: conda pacakge manager is more convenient to install and use. To begin with, obtaining an Anaconda or Miniconda installer for Linux x86_64 platform. Then execute it on Lxplus. Please note that if you update your shell configuration (e.g. .bashrc file) by conda init , you may encounter failure due to inconsistent environment configuration. Installing packages via conda also needs special consideration on selecting proper CUDA version as discussed in pip part. Container based solution: The unpacked.cern.ch mount on CVMFS contains many singularity images, some of which are suitable for machine learning applications. A description of each of the images is beyond the scope of this document. However, if you find an image which is useful for your application, you can use if by running a Singularity container with the appropriate options. For example: singularity run --nv --bind <bind_mount_path> /cvmfs/unpacked.cern.ch/<path_to_image>","title":"Software Environment"},{"location":"Resources/GPU_Resources/CMS_Resources/lxplus_gpu.html#examples","text":"After installing package, you can then use GPU based machine learning algorithms. Two examples are supplied as an example. The first example aims at using a CNN to perform handwritten digits classification with MNIST dataset. The whole notebook can be found at PytorchMNIST . This example is modified from an official pytorch example . The second example is modified from the simple MLP example from weaver-benchmark . The whole notebook can be found at TopTaggingMLP .","title":"Examples"},{"location":"Resources/GPU_Resources/CMS_Resources/ml_cern_ch.html","text":"ml.cern.ch \u00b6 ml.cern.ch is a Kubeflow based ML solution provided by CERN. Kubeflow \u00b6 Kubeflow is a Kubernetes based ML toolkits aiming at making deployments of ML workflows simple, portable and scalable. In Kubeflow, pipeline is an important concept. Machine Learning workflows are discribed as a Kubeflow pipeline for execution. How to access \u00b6 ml.cern.ch only accepts connections from within the CERN network. Therefore, if you are outside of CERN, you will need to use a network tunnel (eg. via ssh -D dynamic port forwarding as a SOCKS5 proxy)... The main website are shown below. Examples \u00b6 After logging into the main website, you can click on the Examples entry to browser a gitlab repository containing a lot of examples. For instance, below are two examples from that repository with a well-documented readme file. mnist-kfp is an example on how to use jupyter notebooks to create a Kubeflow pipeline (kfp) and how to access CERN EOS files. katib gives an example on how to use the katib to operate hyperparameter tuning for jet tagging with ParticleNet.","title":"ml.cern.ch"},{"location":"Resources/GPU_Resources/CMS_Resources/ml_cern_ch.html#mlcernch","text":"ml.cern.ch is a Kubeflow based ML solution provided by CERN.","title":"ml.cern.ch"},{"location":"Resources/GPU_Resources/CMS_Resources/ml_cern_ch.html#kubeflow","text":"Kubeflow is a Kubernetes based ML toolkits aiming at making deployments of ML workflows simple, portable and scalable. In Kubeflow, pipeline is an important concept. Machine Learning workflows are discribed as a Kubeflow pipeline for execution.","title":"Kubeflow"},{"location":"Resources/GPU_Resources/CMS_Resources/ml_cern_ch.html#how-to-access","text":"ml.cern.ch only accepts connections from within the CERN network. Therefore, if you are outside of CERN, you will need to use a network tunnel (eg. via ssh -D dynamic port forwarding as a SOCKS5 proxy)... The main website are shown below.","title":"How to access"},{"location":"Resources/GPU_Resources/CMS_Resources/ml_cern_ch.html#examples","text":"After logging into the main website, you can click on the Examples entry to browser a gitlab repository containing a lot of examples. For instance, below are two examples from that repository with a well-documented readme file. mnist-kfp is an example on how to use jupyter notebooks to create a Kubeflow pipeline (kfp) and how to access CERN EOS files. katib gives an example on how to use the katib to operate hyperparameter tuning for jet tagging with ParticleNet.","title":"Examples"},{"location":"Resources/GPU_Resources/CMS_Resources/Notebooks/PytorchMNIST.html","text":"from __future__ import print_function import argparse import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets , transforms from torch.optim.lr_scheduler import StepLR class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 1 , 32 , 3 , 1 ) self . conv2 = nn . Conv2d ( 32 , 64 , 3 , 1 ) self . dropout1 = nn . Dropout ( 0.25 ) self . dropout2 = nn . Dropout ( 0.5 ) self . fc1 = nn . Linear ( 9216 , 128 ) self . fc2 = nn . Linear ( 128 , 10 ) def forward ( self , x ): x = self . conv1 ( x ) x = F . relu ( x ) x = self . conv2 ( x ) x = F . relu ( x ) x = F . max_pool2d ( x , 2 ) x = self . dropout1 ( x ) x = torch . flatten ( x , 1 ) x = self . fc1 ( x ) x = F . relu ( x ) x = self . dropout2 ( x ) x = self . fc2 ( x ) output = F . log_softmax ( x , dim = 1 ) return output def train ( args , model , device , train_loader , optimizer , epoch ): model . train () for batch_idx , ( data , target ) in enumerate ( train_loader ): data , target = data . to ( device ), target . to ( device ) optimizer . zero_grad () output = model ( data ) loss = F . nll_loss ( output , target ) loss . backward () optimizer . step () if batch_idx % args [ \"log_interval\" ] == 0 : print ( 'Train Epoch: {} [ {} / {} ( {:.0f} %)] \\t Loss: {:.6f} ' . format ( epoch , batch_idx * len ( data ), len ( train_loader . dataset ), 100. * batch_idx / len ( train_loader ), loss . item ())) if args [ \"dry_run\" ]: break def test ( model , device , test_loader ): model . eval () test_loss = 0 correct = 0 with torch . no_grad (): for data , target in test_loader : data , target = data . to ( device ), target . to ( device ) output = model ( data ) test_loss += F . nll_loss ( output , target , reduction = 'sum' ) . item () # sum up batch loss pred = output . argmax ( dim = 1 , keepdim = True ) # get the index of the max log-probability correct += pred . eq ( target . view_as ( pred )) . sum () . item () test_loss /= len ( test_loader . dataset ) print ( ' \\n Test set: Average loss: {:.4f} , Accuracy: {} / {} ( {:.0f} %) \\n ' . format ( test_loss , correct , len ( test_loader . dataset ), 100. * correct / len ( test_loader . dataset ))) torch . cuda . is_available () # Check if cuda is available train_kwargs = { \"batch_size\" : 64 } test_kwargs = { \"batch_size\" : 1000 } cuda_kwargs = { 'num_workers' : 1 , 'pin_memory' : True , 'shuffle' : True } train_kwargs . update ( cuda_kwargs ) test_kwargs . update ( cuda_kwargs ) transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)) ]) dataset1 = datasets . MNIST ( './data' , train = True , download = True , transform = transform ) dataset2 = datasets . MNIST ( './data' , train = False , transform = transform ) train_loader = torch . utils . data . DataLoader ( dataset1 , ** train_kwargs ) test_loader = torch . utils . data . DataLoader ( dataset2 , ** test_kwargs ) device = torch . device ( \"cuda\" ) model = Net () . to ( device ) optimizer = optim . Adadelta ( model . parameters (), lr = 1.0 ) scheduler = StepLR ( optimizer , step_size = 1 , gamma = 0.7 ) args = { \"dry_run\" : False , \"log_interval\" : 100 } for epoch in range ( 1 , 14 + 1 ): train ( args , model , device , train_loader , optimizer , epoch ) test ( model , device , test_loader ) scheduler . step ()","title":"PytorchMNIST"},{"location":"Resources/GPU_Resources/CMS_Resources/Notebooks/TopTaggingMLP.html","text":"import torch import torch.nn as nn from torch.utils.data.dataset import Dataset import pandas as pd import numpy as np import uproot3 import torch.optim as optim from torch.optim.lr_scheduler import StepLR import torch.nn.functional as F import awkward0 class MultiLayerPerceptron ( nn . Module ): r \"\"\"Parameters ---------- input_dims : int Input feature dimensions. num_classes : int Number of output classes. layer_params : list List of the feature size for each layer. \"\"\" def __init__ ( self , input_dims , num_classes , layer_params = ( 256 , 64 , 16 ), ** kwargs ): super ( MultiLayerPerceptron , self ) . __init__ ( ** kwargs ) channels = [ input_dims ] + list ( layer_params ) + [ num_classes ] layers = [] for i in range ( len ( channels ) - 1 ): layers . append ( nn . Sequential ( nn . Linear ( channels [ i ], channels [ i + 1 ]), nn . ReLU ())) self . mlp = nn . Sequential ( * layers ) def forward ( self , x ): # x: the feature vector initally read from the data structure, in dimension (N, C, P) x = x . flatten ( start_dim = 1 ) # (N, L), where L = C * P return self . mlp ( x ) def predict ( self , x ): pred = F . softmax ( self . forward ( x )) ans = [] for t in pred : if t [ 0 ] > t [ 1 ]: ans . append ( 1 ) else : ans . append ( 0 ) return torch . tensor ( ans ) def train ( args , model , device , train_loader , optimizer , epoch ): model . train () for batch_idx , ( data , target ) in enumerate ( train_loader ): data , target = data . to ( device ), target . to ( device ) optimizer . zero_grad () output = model ( data ) loss = F . nll_loss ( output , target ) loss . backward () optimizer . step () if batch_idx % args [ \"log_interval\" ] == 0 : print ( 'Train Epoch: {} [ {} / {} ( {:.0f} %)] \\t Loss: {:.6f} ' . format ( epoch , batch_idx * len ( data ), len ( train_loader . dataset ), 100. * batch_idx / len ( train_loader ), loss . item ())) if args [ \"dry_run\" ]: break input_branches = [ 'Part_Etarel' , 'Part_Phirel' , 'Part_E_log' , 'Part_P_log' ] output_branches = [ 'is_signal_new' ] train_dataset = uproot3 . open ( \"TopTaggingMLP/train.root\" )[ \"Events\" ] . arrays ( input_branches + output_branches , namedecode = 'utf-8' ) train_dataset = { name : train_dataset [ name ] . astype ( \"float32\" ) for name in input_branches + output_branches } test_dataset = uproot3 . open ( \"/eos/user/c/coli/public/weaver-benchmark/top_tagging/samples/prep/top_test_0.root\" )[ \"Events\" ] . arrays ( input_branches + output_branches , namedecode = 'utf-8' ) test_dataset = { name : test_dataset [ name ] . astype ( \"float32\" ) for name in input_branches + output_branches } for ds in [ train_dataset , test_dataset ]: for name in ds . keys (): if isinstance ( ds [ name ], awkward0 . JaggedArray ): ds [ name ] = ds [ name ] . pad ( 30 , clip = True ) . fillna ( 0 ) . regular () . astype ( \"float32\" ) class PF_Features ( Dataset ): def __init__ ( self , mode = \"train\" ): if mode == \"train\" : self . x = { key : train_dataset [ key ] for key in input_branches } self . y = { 'is_signal_new' : train_dataset [ 'is_signal_new' ]} elif mode == \"test\" : self . x = { key : test_dataset [ key ] for key in input_branches } self . y = { 'is_signal_new' : test_dataset [ 'is_signal_new' ]} elif model == \"val\" : self . x = { key : test_dataset [ key ] for key in input_branches } self . y = { 'is_signal_new' : test_dataset [ 'is_signal_new' ]} def __len__ ( self ): return len ( self . y [ 'is_signal_new' ]) def __getitem__ ( self , idx ): X = [ self . x [ key ][ idx ] . copy () for key in input_branches ] X = np . vstack ( X ) y = self . y [ 'is_signal_new' ][ idx ] . copy () return X , y torch . cuda . is_available () # Check if cuda is available True device = torch . device ( \"cuda\" ) train_kwargs = { \"batch_size\" : 1000 } test_kwargs = { \"batch_size\" : 10 } cuda_kwargs = { 'num_workers' : 1 , 'pin_memory' : True , 'shuffle' : True } train_kwargs . update ( cuda_kwargs ) test_kwargs . update ( cuda_kwargs ) model = MultiLayerPerceptron ( input_dims = 4 * 30 , num_classes = 2 ) . to ( device ) optimizer = optim . Adam ( model . parameters (), lr = 0.01 ) train_loader = torch . utils . data . DataLoader ( PF_Features ( mode = \"train\" ), ** train_kwargs ) test_loader = torch . utils . data . DataLoader ( PF_Features ( mode = \"test\" ), ** test_kwargs ) loss_func = torch . nn . CrossEntropyLoss () args = { \"dry_run\" : False , \"log_interval\" : 500 } for epoch in range ( 1 , 10 + 1 ): for batch_idx , ( data , target ) in enumerate ( train_loader ): inputs = data . to ( device ) #.flatten(start_dim=1) target = target . long () . to ( device ) optimizer . zero_grad () output = model . forward ( inputs ) loss = loss_func ( output , target ) loss . backward () optimizer . step () if batch_idx % args [ \"log_interval\" ] == 0 : print ( 'Train Epoch: {} [ {} / {} ( {:.0f} %)] \\t Loss: {:.6f} ' . format ( epoch , batch_idx * len ( data ), len ( train_loader . dataset ), 100. * batch_idx / len ( train_loader ), loss . item ()))","title":"TopTaggingMLP"},{"location":"inference/checklist.html","text":"Integration checklist \u00b6 Todo.","title":"Integration checklist"},{"location":"inference/checklist.html#integration-checklist","text":"Todo.","title":"Integration checklist"},{"location":"inference/hls4ml.html","text":"Direct inference with hls4ml \u00b6 This page is still under construction. For now, please see the main hls4ml documentation .","title":"hls4ml"},{"location":"inference/hls4ml.html#direct-inference-with-hls4ml","text":"This page is still under construction. For now, please see the main hls4ml documentation .","title":"Direct inference with hls4ml"},{"location":"inference/onnx.html","text":"Direct inference with ONNX Runtime \u00b6 ONNX is an open format built to represent machine learning models. It is designed to improve interoperability across a variety of frameworks and platforms in the AI tools community\u2014most deep learning frameworks (e.g. XGBoost, TensorFlow, PyTorch which are frequently used in CMS) support converting their model into the ONNX format or loading a model from an ONNX format. The figure showing the ONNX interoperability. (Source from website .) ONNX Runtime is a tool aiming for the acceleration of machine learning inferencing across a variety of deployment platforms. It allows to \"run any ONNX model using a single set of inference APIs that provide access to the best hardware acceleration available\". It includes \"built-in optimization features that trim and consolidate nodes without impacting model accuracy.\" The CMSSW interface to ONNX Runtime is avaiable since CMSSW_11_1_X ( cmssw#28112 , cmsdist#5020 ). Its functionality is improved in CMSSW_11_2_X. The final implementation is also backported to CMSSW_10_6_X to facilitate Run 2 UL data reprocessing. The inference of a number of deep learning tagger models (e.g. DeepJet, DeepTauID, ParticleNet, DeepDoubleX, etc.) has been made with ONNX Runtime in the routine of UL processing and has gained substantial speedup. On this page, we will use a simple example to show how to use ONNX Runtime for deep learning model inference in the CMSSW framework , both in C++ (e.g. to process the MiniAOD file) and in Python (e.g. using NanoAOD-tools to process the NanoAODs). This may help readers who will deploy an ONNX model into their analyses or in the CMSSW framework. Software Setup \u00b6 We use CMSSW_11_2_5_patch2 to show the simple example for ONNX Runtime inference. The example can also work under the new 12 releases (note that inference with C++ can also run on CMSSW_10_6_X) 1 2 3 4 5 6 7 8 9 10 export SCRAM_ARCH = \"slc7_amd64_gcc900\" export CMSSW_VERSION = \"CMSSW_11_2_5_patch2\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b Converting model to ONNX \u00b6 The model deployed into CMSSW or our analysis needs to be converted to ONNX from the original framework format where it is trained. Please see here for a nice deck of tutorials on converting models from different mainstream frameworks into ONNX. Here we take PyTorch as an example. A PyTorch model can be converted by torch.onnx.export(...) . As a simple illustration, we convert a randomly initialized feed-forward network implemented in PyTorch, with 10 input nodes and 2 output nodes, and two hidden layers with 64 nodes each. The conversion code is presented below. The output model model.onnx will be deployed under the CMSSW framework in our following tutorial. Click to expand 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import torch import torch.nn as nn torch . manual_seed ( 42 ) class SimpleMLP ( nn . Module ): def __init__ ( self , ** kwargs ): super ( SimpleMLP , self ) . __init__ ( ** kwargs ) self . mlp = nn . Sequential ( nn . Linear ( 10 , 64 ), nn . BatchNorm1d ( 64 ), nn . ReLU (), nn . Linear ( 64 , 64 ), nn . BatchNorm1d ( 64 ), nn . ReLU (), nn . Linear ( 64 , 2 ), nn . ReLU (), ) def forward ( self , x ): # input x: (batch_size, feature_dim=10) x = self . mlp ( x ) return torch . softmax ( x , dim = 1 ) model = SimpleMLP () # create dummy input for the model dummy_input = torch . ones ( 1 , 10 , requires_grad = True ) # batch size = 1 # export model to ONNX torch . onnx . export ( model , dummy_input , \"model.onnx\" , verbose = True , input_names = [ 'my_input' ], output_names = [ 'my_output' ]) Inference in CMSSW (C++) \u00b6 We will introduce how to write a module to run inference on the ONNX model under the CMSSW framework. CMSSW is known for its multi-threaded ability. In a threaded framework, multiple threads are served for processing events in the event loop. The logic is straightforward: a new event is assigned to idled threads following the first-come-first-serve princlple. In most cases, each thread is able to process events individually as the majority of event processing workflow can be accomplished only by seeing the information of that event. Thus, the stream modules ( stream EDAnalyzer and stream EDFilter ) are used frequently as each thread holds an individual copy of the module instance\u2014they do not need to communicate with each other. It is however also possible to share a global cache object between all threads in case sharing information across threads is necessary. In all, such CMSSW EDAnalyzer modules are declared by class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> (similar for EDFilter ). Details can be found in documentation on the C++ interface of stream modules . Let's then think about what would happen when interfacing CMSSW with ONNX for model inference. When ONNX Runtime accepts a model, it converts the model into an in-memory representation, and performance a variety of optimizations depending on the operators in the model. The procedure is done when an ONNX Runtime Session is created with an inputting model. The economic method will then be to hold only one Session for all threads\u2014this may save memory to a large extent, as the model has only one copy in memory. Upon request from multiple threads to do inference with their input data, the Session accepts those requests and serializes them, then produces the output data. ONNX Runtime has by design accepted that multithread threads invoke the Run() method on the same inference Session object. Therefore, what has left us to do is to create a Session as a global object in our CMSSW module and share it among all threads; in each thread, we process the input data and then call the Run() method from that global Session . That's the main logic for implementing ONNX inference in CMSSW. For details of high-level designs of ONNX Runtime, please see documentation here . With this concept, let's build the module. 1. includes \u00b6 1 2 3 4 #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" #include \"PhysicsTools/ONNXRuntime/interface/ONNXRuntime.h\" // further framework includes ... We include stream/EDAnalyzer.h to build the stream CMSSW module. 2. Global cache object \u00b6 In CMSSW there exists a class ONNXRuntime which can be used directly as the global cache object. Upon initialization from a given model, it holds the ONNX Runtime Session object and provides the handle to invoke the Run() for model inference. We put the ONNXRuntime class in the edm :: GlobalCache template argument: 1 2 3 class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < ONNXRuntime >> { ... }; 3. Initiate objects \u00b6 In the stream EDAnlyzer module, it provides a hook initializeGlobalCache() to initiate the global object. We simply do 1 2 3 std :: unique_ptr < ONNXRuntime > MyPlugin :: initializeGlobalCache ( const edm :: ParameterSet & iConfig ) { return std :: make_unique < ONNXRuntime > ( iConfig . getParameter < edm :: FileInPath > ( \"model_path\" ). fullPath ()); } to initiate the ONNXRuntime object upon a given model path. 4. Inference \u00b6 We know the event processing step is implemented in the void EDAnalyzer::analyze method. When an event is assigned to a valid thread, the content will be processed in that thread. This can go in parallel with other threads processing other events. We need to first construct the input data dedicated to the event. Here we create a dummy input: a sequence of consecutive integers of length 10. The input is set by replacing the values of our pre-booked vector, data_ . This member variable has vector < vector < float >> format and is initialised as { { 0 , 0 , ..., 0 } } (contains only one element, which is a vector of 10 zeros). In processing of each event, the input data_ is modified: 1 2 3 4 std :: vector < float > & group_data = data_ [ 0 ]; for ( size_t i = 0 ; i < 10 ; i ++ ){ group_data [ i ] = float ( iEvent . id (). event () % 100 + i ); } Then, we send data_ to the inference engine and get the model output: 1 std :: vector < float > outputs = globalCache () -> run ( input_names_ , data_ , input_shapes_ )[ 0 ]; We clarify a few details here. First, we use globalCache () which is a class method in our stream CMSSW module to access the global object shared across all threads. In our case it is the ONNXRuntime instance. The run() method is a wrapper to call Run() on the ONNX Session . Definations on the method arguments are (code from link ): 1 2 3 4 5 6 7 8 9 10 11 12 13 // Run inference and get outputs // input_names: list of the names of the input nodes. // input_values: list of input arrays for each input node. The order of `input_values` must match `input_names`. // input_shapes: list of `int64_t` arrays specifying the shape of each input node. Can leave empty if the model does not have dynamic axes. // output_names: names of the output nodes to get outputs from. Empty list means all output nodes. // batch_size: number of samples in the batch. Each array in `input_values` must have a shape layout of (batch_size, ...). // Returns: a std::vector<std::vector<float>>, with the order matched to `output_names`. // When `output_names` is empty, will return all outputs ordered as in `getOutputNames()`. FloatArrays run ( const std :: vector < std :: string >& input_names , FloatArrays & input_values , const std :: vector < std :: vector < int64_t >>& input_shapes = {}, const std :: vector < std :: string >& output_names = {}, int64_t batch_size = 1 ) const ; where we have 1 typedef std :: vector < std :: vector < float >> FloatArrays ; In our case, input_names is set to { \"my_input\" } which corresponds to the names upon model creation. input_values is a length-1 vector, and input_values [ 0 ] is a vector of float of length 10, which are inputs to the 10 nodes. input_shapes can be set empty here and will be necessary for advanced usage, when our input has dynamic lengths (e.g., in boosed jet tagging, we use different numbers of particle-flow candidates and secondary vertices as input). For the usual model design, we have only one vector of output. In such a case, the output is simply a length-1 vector, and we use [ 0 ] to get the vector of two float numbers\u2014the output of the model. Full example \u00b6 Let's construct the full example. Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 model.onnx plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 /* * Example plugin to demonstrate the direct multi-threaded inference with ONNX Runtime. */ #include <memory> #include <iostream> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/ONNXRuntime/interface/ONNXRuntime.h\" using namespace cms :: Ort ; class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < ONNXRuntime >> { public : explicit MyPlugin ( const edm :: ParameterSet & , const ONNXRuntime * ); static void fillDescriptions ( edm :: ConfigurationDescriptions & ); static std :: unique_ptr < ONNXRuntime > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const ONNXRuntime * ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: vector < std :: string > input_names_ ; std :: vector < std :: vector < int64_t >> input_shapes_ ; FloatArrays data_ ; // each stream hosts its own data }; void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < edm :: FileInPath > ( \"model_path\" , edm :: FileInPath ( \"MySubsystem/MyModule/data/model.onnx\" )); desc . add < std :: vector < std :: string >> ( \"input_names\" , std :: vector < std :: string > ({ \"my_input\" })); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & iConfig , const ONNXRuntime * cache ) : input_names_ ( iConfig . getParameter < std :: vector < std :: string >> ( \"input_names\" )), input_shapes_ () { // initialize the input data arrays // note there is only one element in the FloatArrays type (i.e. vector<vector<float>>) variable data_ . emplace_back ( 10 , 0 ); } std :: unique_ptr < ONNXRuntime > MyPlugin :: initializeGlobalCache ( const edm :: ParameterSet & iConfig ) { return std :: make_unique < ONNXRuntime > ( iConfig . getParameter < edm :: FileInPath > ( \"model_path\" ). fullPath ()); } void MyPlugin :: globalEndJob ( const ONNXRuntime * cache ) {} void MyPlugin :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { // prepare dummy inputs for every event std :: vector < float > & group_data = data_ [ 0 ]; for ( size_t i = 0 ; i < 10 ; i ++ ){ group_data [ i ] = float ( iEvent . id (). event () % 100 + i ); } // run prediction and get outputs std :: vector < float > outputs = globalCache () -> run ( input_names_ , data_ , input_shapes_ )[ 0 ]; // print the input and output data std :: cout << \"input data -> \" ; for ( auto & i : group_data ) { std :: cout << i << \" \" ; } std :: cout << std :: endl << \"output data -> \" ; for ( auto & i : outputs ) { std :: cout << i << \" \" ; } std :: cout << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/ONNXRuntime\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"/store/mc/RunIISummer20UL18MiniAODv2/DYJetsToLL_M-50_TuneCP5_13TeV-amcatnloFXFX-pythia8/MINIAODSIM/106X_upgrade2018_realistic_v16_L1v1-v2/230000/4C8619B2-D0C0-4647-B946-B33754F4ED16.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup options for multithreaded process . options . numberOfThreads = cms . untracked . uint32 ( 1 ) process . options . numberOfStreams = cms . untracked . uint32 ( 0 ) process . options . numberOfConcurrentLuminosityBlocks = cms . untracked . uint32 ( 1 ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) # specify the path of the ONNX model process . myPlugin . model_path = \"MySubsystem/MyModule/data/model.onnx\" # input names as defined in the model # the order of name strings should also corresponds to the order of input data array feed to the model process . myPlugin . input_names = [ \"my_input\" ] # define what to run in the path process . p = cms . Path ( process . myPlugin ) data/model.onnx The model is produced by code in the section \"Converting model to ONNX\" and can be downloaded here . Test our module \u00b6 Under MySubsystem/MyModule/test , run cmsRun my_plugin_cfg.py to launch our module. You may see the following from the output, which include the input and output vectors in the inference process. Click to see the output ... 19-Jul-2022 10:50:41 CEST Successfully opened file root://xrootd-cms.infn.it//store/mc/RunIISummer20UL18MiniAODv2/DYJetsToLL_M-50_TuneCP5_13TeV-amcatnloFXFX-pythia8/MINIAODSIM/106X_upgrade2018_realistic_v16_L1v1-v2/230000/4C8619B2-D0C0-4647-B946-B33754F4ED16.root Begin processing the 1st record. Run 1, Event 27074045, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.494 CEST input data -> 45 46 47 48 49 50 51 52 53 54 output data -> 0.995657 0.00434343 Begin processing the 2nd record. Run 1, Event 27074048, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.495 CEST input data -> 48 49 50 51 52 53 54 55 56 57 output data -> 0.996884 0.00311563 Begin processing the 3rd record. Run 1, Event 27074059, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.495 CEST input data -> 59 60 61 62 63 64 65 66 67 68 output data -> 0.999081 0.000919373 Begin processing the 4th record. Run 1, Event 27074061, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.495 CEST input data -> 61 62 63 64 65 66 67 68 69 70 output data -> 0.999264 0.000736247 Begin processing the 5th record. Run 1, Event 27074046, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.496 CEST input data -> 46 47 48 49 50 51 52 53 54 55 output data -> 0.996112 0.00388828 Begin processing the 6th record. Run 1, Event 27074047, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.496 CEST input data -> 47 48 49 50 51 52 53 54 55 56 output data -> 0.996519 0.00348065 Begin processing the 7th record. Run 1, Event 27074064, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.496 CEST input data -> 64 65 66 67 68 69 70 71 72 73 output data -> 0.999472 0.000527586 Begin processing the 8th record. Run 1, Event 27074074, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.496 CEST input data -> 74 75 76 77 78 79 80 81 82 83 output data -> 0.999826 0.000173664 Begin processing the 9th record. Run 1, Event 27074050, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.496 CEST input data -> 50 51 52 53 54 55 56 57 58 59 output data -> 0.997504 0.00249614 Begin processing the 10th record. Run 1, Event 27074060, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.496 CEST input data -> 60 61 62 63 64 65 66 67 68 69 output data -> 0.999177 0.000822734 19-Jul-2022 10:50:43 CEST Closed file root://xrootd-cms.infn.it//store/mc/RunIISummer20UL18MiniAODv2/DYJetsToLL_M-50_TuneCP5_13TeV-amcatnloFXFX-pythia8/MINIAODSIM/106X_upgrade2018_realistic_v16_L1v1-v2/230000/4C8619B2-D0C0-4647-B946-B33754F4ED16.root Also we could try launching the script with more threads. Change the corresponding line in my_plugin_cfg.py as follows to activate the multi-threaded mode with 4 threads. 31 process . options . numberOfThreads = cms . untracked . uint32 ( 4 ) Launch the script again, and one could see the same results, but with the inference processed concurrently on 4 threads. Inference in CMSSW (Python) \u00b6 Doing ONNX Runtime inference with python is possible as well. For those releases that have the ONNX Runtime C++ package installed, the onnxruntime python package is also installed in python3 (except for CMSSW_10_6_X). We still use CMSSW_11_2_5_patch2 to run our examples. We could quickly check if onnxruntime is available by: 1 python3 - c \"import onnxruntime; print('onnxruntime available')\" The python code is simple to construct: following the quick examples \"Get started with ORT for Python\" , we create the file MySubsystem/MyModule/test/my_standalone_test.py as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import onnxruntime as ort import numpy as np # create input data in the float format (32 bit) data = np . arange ( 45 , 55 ) . astype ( np . float32 ) # create inference session using ort.InferenceSession from a given model ort_sess = ort . InferenceSession ( '../data/model.onnx' ) # run inference outputs = ort_sess . run ( None , { 'my_input' : np . array ([ data ])})[ 0 ] # print input and output print ( 'input ->' , data ) print ( 'output ->' , outputs ) Under the directory MySubsystem/MyModule/test , run the example with python3 my_standalone_test.py . Then we see the output: input -> [45. 46. 47. 48. 49. 50. 51. 52. 53. 54.] output -> [[0.9956566 0.00434343]] Using ONNX Runtime on NanoAOD-tools follows the same logic. Here we create the ONNX Session in the beginning stage and run inference in the event loop. Note that NanoAOD-tools runs the event loop in the single-thread mode. Please find details in the following block. Click to see the NanoAOD-tools example We run the NanoAOD-tools example following the above CMSSW_11_2_5_patch2 environment. According to the setup instruction in NanoAOD-tools , do cd $CMSSW_BASE /src git clone https://github.com/cms-nanoAOD/nanoAOD-tools.git PhysicsTools/NanoAODTools cd PhysicsTools/NanoAODTools cmsenv scram b Now we add our custom module to run ONNX Runtime inference. Create a file PhysicsTools/NanoAODTools/python/postprocessing/examples/exampleOrtModule.py with the content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from PhysicsTools.NanoAODTools.postprocessing.framework.datamodel import Collection from PhysicsTools.NanoAODTools.postprocessing.framework.eventloop import Module import ROOT ROOT . PyConfig . IgnoreCommandLineOptions = True import onnxruntime as ort import numpy as np import os class exampleOrtProducer ( Module ): def __init__ ( self ): pass def beginJob ( self ): model_path = os . path . join ( os . getenv ( \"CMSSW_BASE\" ), 'src' , 'MySubsystem/MyModule/data/model.onnx' ) self . ort_sess = ort . InferenceSession ( model_path ) def endJob ( self ): pass def beginFile ( self , inputFile , outputFile , inputTree , wrappedOutputTree ): self . out = wrappedOutputTree self . out . branch ( \"OrtScore\" , \"F\" ) def endFile ( self , inputFile , outputFile , inputTree , wrappedOutputTree ): pass def analyze ( self , event ): \"\"\"process event, return True (go to next module) or False (fail, go to next event)\"\"\" # create input data data = np . arange ( event . event % 100 , event . event % 100 + 10 ) . astype ( np . float32 ) # run inference outputs = self . ort_sess . run ( None , { 'my_input' : np . array ([ data ])})[ 0 ] # print input and output print ( 'input ->' , data ) print ( 'output ->' , outputs ) self . out . fillBranch ( \"OrtScore\" , outputs [ 0 ][ 0 ]) return True # define modules using the syntax 'name = lambda : constructor' to avoid having them loaded when not needed exampleOrtModuleConstr = lambda : exampleOrtProducer () Please notice the highlighted lines for the creation of ONNX Runtime Session and launching the inference. Finally, following the test command from NanoAOD-tools, we run our custom module in python3 by python3 scripts/nano_postproc.py outDir /eos/cms/store/user/andrey/f.root -I PhysicsTools.NanoAODTools.postprocessing.examples.exampleOrtModule exampleOrtModuleConstr -N 10 We should see the output as follows processing.examples.exampleOrtModule exampleOrtModuleConstr -N 10 Loading exampleOrtModuleConstr from PhysicsTools.NanoAODTools.postprocessing.examples.exampleOrtModule Will write selected trees to outDir Pre-select 10 entries out of 10 (100.00%) input -> [11. 12. 13. 14. 15. 16. 17. 18. 19. 20.] output -> [[0.83919346 0.16080655]] input -> [ 7. 8. 9. 10. 11. 12. 13. 14. 15. 16.] output -> [[0.76994413 0.2300559 ]] input -> [ 4. 5. 6. 7. 8. 9. 10. 11. 12. 13.] output -> [[0.7116992 0.2883008]] input -> [ 2. 3. 4. 5. 6. 7. 8. 9. 10. 11.] output -> [[0.66414535 0.33585465]] input -> [ 9. 10. 11. 12. 13. 14. 15. 16. 17. 18.] output -> [[0.80617136 0.19382869]] input -> [ 6. 7. 8. 9. 10. 11. 12. 13. 14. 15.] output -> [[0.75187963 0.2481204 ]] input -> [16. 17. 18. 19. 20. 21. 22. 23. 24. 25.] output -> [[0.9014619 0.09853811]] input -> [18. 19. 20. 21. 22. 23. 24. 25. 26. 27.] output -> [[0.9202239 0.07977609]] input -> [ 5. 6. 7. 8. 9. 10. 11. 12. 13. 14.] output -> [[0.7330253 0.26697478]] input -> [10. 11. 12. 13. 14. 15. 16. 17. 18. 19.] output -> [[0.82333535 0.17666471]] Processed 10 preselected entries from /eos/cms/store/user/andrey/f.root (10 entries). Finally selected 10 entries Done outDir/f_Skim.root Total time 1.1 sec. to process 10 events. Rate = 9.3 Hz. Links and further reading \u00b6 ONNX/ONNX Runtime Tutorials on converting models to ONNX format ONNX Runtime C++ example ONNX Runtime C++ API ONNX Runtime python example ONNX Runtime python API ONNX Runtime in CMSSW (talk) Developers: Huilin Qu Authors: Congqiao Li","title":"ONNX"},{"location":"inference/onnx.html#direct-inference-with-onnx-runtime","text":"ONNX is an open format built to represent machine learning models. It is designed to improve interoperability across a variety of frameworks and platforms in the AI tools community\u2014most deep learning frameworks (e.g. XGBoost, TensorFlow, PyTorch which are frequently used in CMS) support converting their model into the ONNX format or loading a model from an ONNX format. The figure showing the ONNX interoperability. (Source from website .) ONNX Runtime is a tool aiming for the acceleration of machine learning inferencing across a variety of deployment platforms. It allows to \"run any ONNX model using a single set of inference APIs that provide access to the best hardware acceleration available\". It includes \"built-in optimization features that trim and consolidate nodes without impacting model accuracy.\" The CMSSW interface to ONNX Runtime is avaiable since CMSSW_11_1_X ( cmssw#28112 , cmsdist#5020 ). Its functionality is improved in CMSSW_11_2_X. The final implementation is also backported to CMSSW_10_6_X to facilitate Run 2 UL data reprocessing. The inference of a number of deep learning tagger models (e.g. DeepJet, DeepTauID, ParticleNet, DeepDoubleX, etc.) has been made with ONNX Runtime in the routine of UL processing and has gained substantial speedup. On this page, we will use a simple example to show how to use ONNX Runtime for deep learning model inference in the CMSSW framework , both in C++ (e.g. to process the MiniAOD file) and in Python (e.g. using NanoAOD-tools to process the NanoAODs). This may help readers who will deploy an ONNX model into their analyses or in the CMSSW framework.","title":"Direct inference with ONNX Runtime"},{"location":"inference/onnx.html#software-setup","text":"We use CMSSW_11_2_5_patch2 to show the simple example for ONNX Runtime inference. The example can also work under the new 12 releases (note that inference with C++ can also run on CMSSW_10_6_X) 1 2 3 4 5 6 7 8 9 10 export SCRAM_ARCH = \"slc7_amd64_gcc900\" export CMSSW_VERSION = \"CMSSW_11_2_5_patch2\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b","title":"Software Setup"},{"location":"inference/onnx.html#converting-model-to-onnx","text":"The model deployed into CMSSW or our analysis needs to be converted to ONNX from the original framework format where it is trained. Please see here for a nice deck of tutorials on converting models from different mainstream frameworks into ONNX. Here we take PyTorch as an example. A PyTorch model can be converted by torch.onnx.export(...) . As a simple illustration, we convert a randomly initialized feed-forward network implemented in PyTorch, with 10 input nodes and 2 output nodes, and two hidden layers with 64 nodes each. The conversion code is presented below. The output model model.onnx will be deployed under the CMSSW framework in our following tutorial. Click to expand 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import torch import torch.nn as nn torch . manual_seed ( 42 ) class SimpleMLP ( nn . Module ): def __init__ ( self , ** kwargs ): super ( SimpleMLP , self ) . __init__ ( ** kwargs ) self . mlp = nn . Sequential ( nn . Linear ( 10 , 64 ), nn . BatchNorm1d ( 64 ), nn . ReLU (), nn . Linear ( 64 , 64 ), nn . BatchNorm1d ( 64 ), nn . ReLU (), nn . Linear ( 64 , 2 ), nn . ReLU (), ) def forward ( self , x ): # input x: (batch_size, feature_dim=10) x = self . mlp ( x ) return torch . softmax ( x , dim = 1 ) model = SimpleMLP () # create dummy input for the model dummy_input = torch . ones ( 1 , 10 , requires_grad = True ) # batch size = 1 # export model to ONNX torch . onnx . export ( model , dummy_input , \"model.onnx\" , verbose = True , input_names = [ 'my_input' ], output_names = [ 'my_output' ])","title":"Converting model to ONNX"},{"location":"inference/onnx.html#inference-in-cmssw-c","text":"We will introduce how to write a module to run inference on the ONNX model under the CMSSW framework. CMSSW is known for its multi-threaded ability. In a threaded framework, multiple threads are served for processing events in the event loop. The logic is straightforward: a new event is assigned to idled threads following the first-come-first-serve princlple. In most cases, each thread is able to process events individually as the majority of event processing workflow can be accomplished only by seeing the information of that event. Thus, the stream modules ( stream EDAnalyzer and stream EDFilter ) are used frequently as each thread holds an individual copy of the module instance\u2014they do not need to communicate with each other. It is however also possible to share a global cache object between all threads in case sharing information across threads is necessary. In all, such CMSSW EDAnalyzer modules are declared by class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> (similar for EDFilter ). Details can be found in documentation on the C++ interface of stream modules . Let's then think about what would happen when interfacing CMSSW with ONNX for model inference. When ONNX Runtime accepts a model, it converts the model into an in-memory representation, and performance a variety of optimizations depending on the operators in the model. The procedure is done when an ONNX Runtime Session is created with an inputting model. The economic method will then be to hold only one Session for all threads\u2014this may save memory to a large extent, as the model has only one copy in memory. Upon request from multiple threads to do inference with their input data, the Session accepts those requests and serializes them, then produces the output data. ONNX Runtime has by design accepted that multithread threads invoke the Run() method on the same inference Session object. Therefore, what has left us to do is to create a Session as a global object in our CMSSW module and share it among all threads; in each thread, we process the input data and then call the Run() method from that global Session . That's the main logic for implementing ONNX inference in CMSSW. For details of high-level designs of ONNX Runtime, please see documentation here . With this concept, let's build the module.","title":"Inference in CMSSW (C++)"},{"location":"inference/onnx.html#1-includes","text":"1 2 3 4 #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" #include \"PhysicsTools/ONNXRuntime/interface/ONNXRuntime.h\" // further framework includes ... We include stream/EDAnalyzer.h to build the stream CMSSW module.","title":"1. includes"},{"location":"inference/onnx.html#2-global-cache-object","text":"In CMSSW there exists a class ONNXRuntime which can be used directly as the global cache object. Upon initialization from a given model, it holds the ONNX Runtime Session object and provides the handle to invoke the Run() for model inference. We put the ONNXRuntime class in the edm :: GlobalCache template argument: 1 2 3 class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < ONNXRuntime >> { ... };","title":"2. Global cache object"},{"location":"inference/onnx.html#3-initiate-objects","text":"In the stream EDAnlyzer module, it provides a hook initializeGlobalCache() to initiate the global object. We simply do 1 2 3 std :: unique_ptr < ONNXRuntime > MyPlugin :: initializeGlobalCache ( const edm :: ParameterSet & iConfig ) { return std :: make_unique < ONNXRuntime > ( iConfig . getParameter < edm :: FileInPath > ( \"model_path\" ). fullPath ()); } to initiate the ONNXRuntime object upon a given model path.","title":"3. Initiate objects"},{"location":"inference/onnx.html#4-inference","text":"We know the event processing step is implemented in the void EDAnalyzer::analyze method. When an event is assigned to a valid thread, the content will be processed in that thread. This can go in parallel with other threads processing other events. We need to first construct the input data dedicated to the event. Here we create a dummy input: a sequence of consecutive integers of length 10. The input is set by replacing the values of our pre-booked vector, data_ . This member variable has vector < vector < float >> format and is initialised as { { 0 , 0 , ..., 0 } } (contains only one element, which is a vector of 10 zeros). In processing of each event, the input data_ is modified: 1 2 3 4 std :: vector < float > & group_data = data_ [ 0 ]; for ( size_t i = 0 ; i < 10 ; i ++ ){ group_data [ i ] = float ( iEvent . id (). event () % 100 + i ); } Then, we send data_ to the inference engine and get the model output: 1 std :: vector < float > outputs = globalCache () -> run ( input_names_ , data_ , input_shapes_ )[ 0 ]; We clarify a few details here. First, we use globalCache () which is a class method in our stream CMSSW module to access the global object shared across all threads. In our case it is the ONNXRuntime instance. The run() method is a wrapper to call Run() on the ONNX Session . Definations on the method arguments are (code from link ): 1 2 3 4 5 6 7 8 9 10 11 12 13 // Run inference and get outputs // input_names: list of the names of the input nodes. // input_values: list of input arrays for each input node. The order of `input_values` must match `input_names`. // input_shapes: list of `int64_t` arrays specifying the shape of each input node. Can leave empty if the model does not have dynamic axes. // output_names: names of the output nodes to get outputs from. Empty list means all output nodes. // batch_size: number of samples in the batch. Each array in `input_values` must have a shape layout of (batch_size, ...). // Returns: a std::vector<std::vector<float>>, with the order matched to `output_names`. // When `output_names` is empty, will return all outputs ordered as in `getOutputNames()`. FloatArrays run ( const std :: vector < std :: string >& input_names , FloatArrays & input_values , const std :: vector < std :: vector < int64_t >>& input_shapes = {}, const std :: vector < std :: string >& output_names = {}, int64_t batch_size = 1 ) const ; where we have 1 typedef std :: vector < std :: vector < float >> FloatArrays ; In our case, input_names is set to { \"my_input\" } which corresponds to the names upon model creation. input_values is a length-1 vector, and input_values [ 0 ] is a vector of float of length 10, which are inputs to the 10 nodes. input_shapes can be set empty here and will be necessary for advanced usage, when our input has dynamic lengths (e.g., in boosed jet tagging, we use different numbers of particle-flow candidates and secondary vertices as input). For the usual model design, we have only one vector of output. In such a case, the output is simply a length-1 vector, and we use [ 0 ] to get the vector of two float numbers\u2014the output of the model.","title":"4. Inference"},{"location":"inference/onnx.html#full-example","text":"Let's construct the full example. Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 model.onnx plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 /* * Example plugin to demonstrate the direct multi-threaded inference with ONNX Runtime. */ #include <memory> #include <iostream> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/ONNXRuntime/interface/ONNXRuntime.h\" using namespace cms :: Ort ; class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < ONNXRuntime >> { public : explicit MyPlugin ( const edm :: ParameterSet & , const ONNXRuntime * ); static void fillDescriptions ( edm :: ConfigurationDescriptions & ); static std :: unique_ptr < ONNXRuntime > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const ONNXRuntime * ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: vector < std :: string > input_names_ ; std :: vector < std :: vector < int64_t >> input_shapes_ ; FloatArrays data_ ; // each stream hosts its own data }; void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < edm :: FileInPath > ( \"model_path\" , edm :: FileInPath ( \"MySubsystem/MyModule/data/model.onnx\" )); desc . add < std :: vector < std :: string >> ( \"input_names\" , std :: vector < std :: string > ({ \"my_input\" })); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & iConfig , const ONNXRuntime * cache ) : input_names_ ( iConfig . getParameter < std :: vector < std :: string >> ( \"input_names\" )), input_shapes_ () { // initialize the input data arrays // note there is only one element in the FloatArrays type (i.e. vector<vector<float>>) variable data_ . emplace_back ( 10 , 0 ); } std :: unique_ptr < ONNXRuntime > MyPlugin :: initializeGlobalCache ( const edm :: ParameterSet & iConfig ) { return std :: make_unique < ONNXRuntime > ( iConfig . getParameter < edm :: FileInPath > ( \"model_path\" ). fullPath ()); } void MyPlugin :: globalEndJob ( const ONNXRuntime * cache ) {} void MyPlugin :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { // prepare dummy inputs for every event std :: vector < float > & group_data = data_ [ 0 ]; for ( size_t i = 0 ; i < 10 ; i ++ ){ group_data [ i ] = float ( iEvent . id (). event () % 100 + i ); } // run prediction and get outputs std :: vector < float > outputs = globalCache () -> run ( input_names_ , data_ , input_shapes_ )[ 0 ]; // print the input and output data std :: cout << \"input data -> \" ; for ( auto & i : group_data ) { std :: cout << i << \" \" ; } std :: cout << std :: endl << \"output data -> \" ; for ( auto & i : outputs ) { std :: cout << i << \" \" ; } std :: cout << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/ONNXRuntime\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"/store/mc/RunIISummer20UL18MiniAODv2/DYJetsToLL_M-50_TuneCP5_13TeV-amcatnloFXFX-pythia8/MINIAODSIM/106X_upgrade2018_realistic_v16_L1v1-v2/230000/4C8619B2-D0C0-4647-B946-B33754F4ED16.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup options for multithreaded process . options . numberOfThreads = cms . untracked . uint32 ( 1 ) process . options . numberOfStreams = cms . untracked . uint32 ( 0 ) process . options . numberOfConcurrentLuminosityBlocks = cms . untracked . uint32 ( 1 ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) # specify the path of the ONNX model process . myPlugin . model_path = \"MySubsystem/MyModule/data/model.onnx\" # input names as defined in the model # the order of name strings should also corresponds to the order of input data array feed to the model process . myPlugin . input_names = [ \"my_input\" ] # define what to run in the path process . p = cms . Path ( process . myPlugin ) data/model.onnx The model is produced by code in the section \"Converting model to ONNX\" and can be downloaded here .","title":"Full example"},{"location":"inference/onnx.html#test-our-module","text":"Under MySubsystem/MyModule/test , run cmsRun my_plugin_cfg.py to launch our module. You may see the following from the output, which include the input and output vectors in the inference process. Click to see the output ... 19-Jul-2022 10:50:41 CEST Successfully opened file root://xrootd-cms.infn.it//store/mc/RunIISummer20UL18MiniAODv2/DYJetsToLL_M-50_TuneCP5_13TeV-amcatnloFXFX-pythia8/MINIAODSIM/106X_upgrade2018_realistic_v16_L1v1-v2/230000/4C8619B2-D0C0-4647-B946-B33754F4ED16.root Begin processing the 1st record. Run 1, Event 27074045, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.494 CEST input data -> 45 46 47 48 49 50 51 52 53 54 output data -> 0.995657 0.00434343 Begin processing the 2nd record. Run 1, Event 27074048, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.495 CEST input data -> 48 49 50 51 52 53 54 55 56 57 output data -> 0.996884 0.00311563 Begin processing the 3rd record. Run 1, Event 27074059, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.495 CEST input data -> 59 60 61 62 63 64 65 66 67 68 output data -> 0.999081 0.000919373 Begin processing the 4th record. Run 1, Event 27074061, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.495 CEST input data -> 61 62 63 64 65 66 67 68 69 70 output data -> 0.999264 0.000736247 Begin processing the 5th record. Run 1, Event 27074046, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.496 CEST input data -> 46 47 48 49 50 51 52 53 54 55 output data -> 0.996112 0.00388828 Begin processing the 6th record. Run 1, Event 27074047, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.496 CEST input data -> 47 48 49 50 51 52 53 54 55 56 output data -> 0.996519 0.00348065 Begin processing the 7th record. Run 1, Event 27074064, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.496 CEST input data -> 64 65 66 67 68 69 70 71 72 73 output data -> 0.999472 0.000527586 Begin processing the 8th record. Run 1, Event 27074074, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.496 CEST input data -> 74 75 76 77 78 79 80 81 82 83 output data -> 0.999826 0.000173664 Begin processing the 9th record. Run 1, Event 27074050, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.496 CEST input data -> 50 51 52 53 54 55 56 57 58 59 output data -> 0.997504 0.00249614 Begin processing the 10th record. Run 1, Event 27074060, LumiSection 10021 on stream 0 at 19-Jul-2022 10:50:43.496 CEST input data -> 60 61 62 63 64 65 66 67 68 69 output data -> 0.999177 0.000822734 19-Jul-2022 10:50:43 CEST Closed file root://xrootd-cms.infn.it//store/mc/RunIISummer20UL18MiniAODv2/DYJetsToLL_M-50_TuneCP5_13TeV-amcatnloFXFX-pythia8/MINIAODSIM/106X_upgrade2018_realistic_v16_L1v1-v2/230000/4C8619B2-D0C0-4647-B946-B33754F4ED16.root Also we could try launching the script with more threads. Change the corresponding line in my_plugin_cfg.py as follows to activate the multi-threaded mode with 4 threads. 31 process . options . numberOfThreads = cms . untracked . uint32 ( 4 ) Launch the script again, and one could see the same results, but with the inference processed concurrently on 4 threads.","title":"Test our module"},{"location":"inference/onnx.html#inference-in-cmssw-python","text":"Doing ONNX Runtime inference with python is possible as well. For those releases that have the ONNX Runtime C++ package installed, the onnxruntime python package is also installed in python3 (except for CMSSW_10_6_X). We still use CMSSW_11_2_5_patch2 to run our examples. We could quickly check if onnxruntime is available by: 1 python3 - c \"import onnxruntime; print('onnxruntime available')\" The python code is simple to construct: following the quick examples \"Get started with ORT for Python\" , we create the file MySubsystem/MyModule/test/my_standalone_test.py as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import onnxruntime as ort import numpy as np # create input data in the float format (32 bit) data = np . arange ( 45 , 55 ) . astype ( np . float32 ) # create inference session using ort.InferenceSession from a given model ort_sess = ort . InferenceSession ( '../data/model.onnx' ) # run inference outputs = ort_sess . run ( None , { 'my_input' : np . array ([ data ])})[ 0 ] # print input and output print ( 'input ->' , data ) print ( 'output ->' , outputs ) Under the directory MySubsystem/MyModule/test , run the example with python3 my_standalone_test.py . Then we see the output: input -> [45. 46. 47. 48. 49. 50. 51. 52. 53. 54.] output -> [[0.9956566 0.00434343]] Using ONNX Runtime on NanoAOD-tools follows the same logic. Here we create the ONNX Session in the beginning stage and run inference in the event loop. Note that NanoAOD-tools runs the event loop in the single-thread mode. Please find details in the following block. Click to see the NanoAOD-tools example We run the NanoAOD-tools example following the above CMSSW_11_2_5_patch2 environment. According to the setup instruction in NanoAOD-tools , do cd $CMSSW_BASE /src git clone https://github.com/cms-nanoAOD/nanoAOD-tools.git PhysicsTools/NanoAODTools cd PhysicsTools/NanoAODTools cmsenv scram b Now we add our custom module to run ONNX Runtime inference. Create a file PhysicsTools/NanoAODTools/python/postprocessing/examples/exampleOrtModule.py with the content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from PhysicsTools.NanoAODTools.postprocessing.framework.datamodel import Collection from PhysicsTools.NanoAODTools.postprocessing.framework.eventloop import Module import ROOT ROOT . PyConfig . IgnoreCommandLineOptions = True import onnxruntime as ort import numpy as np import os class exampleOrtProducer ( Module ): def __init__ ( self ): pass def beginJob ( self ): model_path = os . path . join ( os . getenv ( \"CMSSW_BASE\" ), 'src' , 'MySubsystem/MyModule/data/model.onnx' ) self . ort_sess = ort . InferenceSession ( model_path ) def endJob ( self ): pass def beginFile ( self , inputFile , outputFile , inputTree , wrappedOutputTree ): self . out = wrappedOutputTree self . out . branch ( \"OrtScore\" , \"F\" ) def endFile ( self , inputFile , outputFile , inputTree , wrappedOutputTree ): pass def analyze ( self , event ): \"\"\"process event, return True (go to next module) or False (fail, go to next event)\"\"\" # create input data data = np . arange ( event . event % 100 , event . event % 100 + 10 ) . astype ( np . float32 ) # run inference outputs = self . ort_sess . run ( None , { 'my_input' : np . array ([ data ])})[ 0 ] # print input and output print ( 'input ->' , data ) print ( 'output ->' , outputs ) self . out . fillBranch ( \"OrtScore\" , outputs [ 0 ][ 0 ]) return True # define modules using the syntax 'name = lambda : constructor' to avoid having them loaded when not needed exampleOrtModuleConstr = lambda : exampleOrtProducer () Please notice the highlighted lines for the creation of ONNX Runtime Session and launching the inference. Finally, following the test command from NanoAOD-tools, we run our custom module in python3 by python3 scripts/nano_postproc.py outDir /eos/cms/store/user/andrey/f.root -I PhysicsTools.NanoAODTools.postprocessing.examples.exampleOrtModule exampleOrtModuleConstr -N 10 We should see the output as follows processing.examples.exampleOrtModule exampleOrtModuleConstr -N 10 Loading exampleOrtModuleConstr from PhysicsTools.NanoAODTools.postprocessing.examples.exampleOrtModule Will write selected trees to outDir Pre-select 10 entries out of 10 (100.00%) input -> [11. 12. 13. 14. 15. 16. 17. 18. 19. 20.] output -> [[0.83919346 0.16080655]] input -> [ 7. 8. 9. 10. 11. 12. 13. 14. 15. 16.] output -> [[0.76994413 0.2300559 ]] input -> [ 4. 5. 6. 7. 8. 9. 10. 11. 12. 13.] output -> [[0.7116992 0.2883008]] input -> [ 2. 3. 4. 5. 6. 7. 8. 9. 10. 11.] output -> [[0.66414535 0.33585465]] input -> [ 9. 10. 11. 12. 13. 14. 15. 16. 17. 18.] output -> [[0.80617136 0.19382869]] input -> [ 6. 7. 8. 9. 10. 11. 12. 13. 14. 15.] output -> [[0.75187963 0.2481204 ]] input -> [16. 17. 18. 19. 20. 21. 22. 23. 24. 25.] output -> [[0.9014619 0.09853811]] input -> [18. 19. 20. 21. 22. 23. 24. 25. 26. 27.] output -> [[0.9202239 0.07977609]] input -> [ 5. 6. 7. 8. 9. 10. 11. 12. 13. 14.] output -> [[0.7330253 0.26697478]] input -> [10. 11. 12. 13. 14. 15. 16. 17. 18. 19.] output -> [[0.82333535 0.17666471]] Processed 10 preselected entries from /eos/cms/store/user/andrey/f.root (10 entries). Finally selected 10 entries Done outDir/f_Skim.root Total time 1.1 sec. to process 10 events. Rate = 9.3 Hz.","title":"Inference in CMSSW (Python)"},{"location":"inference/onnx.html#links-and-further-reading","text":"ONNX/ONNX Runtime Tutorials on converting models to ONNX format ONNX Runtime C++ example ONNX Runtime C++ API ONNX Runtime python example ONNX Runtime python API ONNX Runtime in CMSSW (talk) Developers: Huilin Qu Authors: Congqiao Li","title":"Links and further reading"},{"location":"inference/particlenet.html","text":"ParticleNet \u00b6 ParticleNet [ arXiv:1902.08570 ] is an advanced neural network architecture that has many applications in CMS, including heavy flavour jet tagging, jet mass regression, etc. The network is fed by various low-level point-like objects as input, e.g., the particle-flow candidates, to predict a feature of a jet. The full architecture of the ParticleNet model. We'll walk through the details in the following sections. On this page, we introduce several user-specific aspects of the ParticleNet model. We cover the following items in three sections: An introduction to ParticleNet , including a general description of ParticleNet the advantages brought from the architecture by concept a sketch of ParticleNet applications in CMS and other relevant works An introduction to Weaver and model implementations , introduced in a step-by-step manner: build three network models and understand them from the technical side; use the out-of-the-box commands to run these examples on a benchmark task. The three networks are (1) a simple feed-forward NN, (2) a DeepAK8 model (based on 1D CNN), and eventually (3) the ParticleNet model (based on DGCNN). try to reproduce the original performance and make the ROC plots. This section is friendly to the ML newcomers. The goal is to help readers understand the underlying structure of the \"ParticleNet\". Tuning the ParticleNet model , including tips for readers who are using/modifying the ParticleNet model to achieve a better performance This section can be helpful in practice. It provides tips on model training, tunning, validation, etc. It targets the situations when readers apply their own ParticleNet (or ParticleNet-like) model to the custom task. Corresponding persons: Huilin Qu, Loukas Gouskos (original developers of ParticleNet) Congqiao Li (author of the page) Introduction to ParticleNet \u00b6 1. General description \u00b6 ParticleNet is a graph neural net (GNN) model. The key ingredient of ParticleNet is the graph convolutional operation, i.e., the edge convolution (EdgeConv) and the dynamic graph CNN (DGCNN) method [ arXiv:1801.07829 ] applied on the \"point cloud\" data structure. We will disassemble the ParticleNet model and provide a detailed exploration in the next section, but here we briefly explain the key features of the model. Intuitively, ParticleNet treats all candidates inside an object as a \"point cloud\", which is a permutational-invariant set of points (e.g. a set of PF candidates), each carrying a feature vector ( \u03b7 , \u03c6 , p T , charge, etc.). The DGCNN uses the EdgeConv operation to exploit their spatial correlations (two-dimensional on the \u03b7 - \u03c6 plain) by finding the k -nearest neighbours of each point and generate a new latent graph layer where points are scattered on a high-dimensional latent space. This is a graph-type analogue of the classical 2D convolution operation, which acts on a regular 2D grid (e.g., a picture) using a 3\u00d73 local patch to explore the relations of a single-pixel with its 8 nearest pixels, then generates a new 2D grid. The cartoon illustrates the convolutional operation acted on the regular grid and on the point cloud (plot from ML4Jets 2018 talk). As a consequence, the EdgeConv operation transforms the graph to a new graph, which has a changed spatial relationship among points. It then acts on the second graph to produce the third graph, showing the stackability of the convolution operation. This illustrates the \"dynamic\" property as the graph topology changes after each EdgeConv layer. 2. Advantage \u00b6 By concept, the advantage of the network may come from exploiting the permutational-invariant symmetry of the points, which is intrinsic to our physics objects. This symmetry is held naturally in a point cloud representation. In a recent study on jet physics or event-based analysis using ML techniques, there are increasing interest to explore the point cloud data structure. We explain here conceptually why a \"point cloud\" representation outperforms the classical ones, including the variable-length 2D vector structure passing to a 1D CNN or any type of RNN, and imaged-based representation passing through a 2D CNN. By using the 1D CNN, the points (PF candidates) are more often ordered by p T to fix on the 1D grid. Only correlations with neighbouring points with similar p T are learned by the network with a convolution operation. The Long Short-Term Memory (LSTM) type recurrent neural network (RNN) provides the flexibility to feed in a variant-length sequence and has a \"memory\" mechanism to cooperate the information it learns from an early node to the latest node. The concern is that such ordering of the sequence is somewhat artificial, and not an underlying property that an NN must learn to accomplish the classification task. As a comparison, in the task of the natural language processing where LSTM has a huge advantage, the order of words are important characteristic of a language itself (reflects the \"grammar\" in some circumstances) and is a feature the NN must learn to master the language. The imaged-based data explored by a 2D CNN stems from the image recognition task. A jet image with proper standardization is usually performed before feeding into the network. In this sense, it lacks local features which the 2D local patch is better at capturing, e.g. the ear of the cat that a local patch can capture by scanning over the entire image. The jet image is appearing to hold the features globally (e.g. two-prong structure for W-tagging). The sparsity of data is another concern in that it introduces redundant information to present a jet on the regular grid, making the network hard to capture the key properties. 3. Applications and other related work \u00b6 Here we briefly summarize the applications and ongoing works on ParticleNet. Public CMS results include large- R jet with R =0.8 tagging (for W/Z/H/t) using ParticleNet [ CMS-DP-2020/002 ] regression on the large- R jet mass based on the ParticleNet model [ CMS-DP-2021/017 ] ParticleNet architecture is also applied on small radius R =0.4 jets for the b/c-tagging and quark/gluon classification (see this talk (CMS internal) ). A recent ongoing work applies the ParticleNet architecture in heavy flavour tagging at HLT (see this talk (CMS internal) ). The ParticleNet model is recently updated to ParticleNeXt and see further improvement (see the ML4Jets 2021 talk ). Recent works in the joint field of HEP and ML also shed light on exploiting the point cloud data structure and GNN-based architectures. We see very active progress in recent years. Here list some useful materials for the reader's reference. Some pheno-based work are summarized in the HEP \u00d7 ML living review , especially in the \"graph\" and \"sets\" categories. An overview of GNN applications to CMS, see CMS ML forum (CMS internal) . Also see more recent GNN application progress in ML forums: Oct 20 , Nov 3 . At the time of writing, various novel GNN-based models are explored and introduced in the recent ML4Jets2021 meeting. Introduction to Weaver and model implementations \u00b6 Weaver is a machine learning R&D framework for high energy physics (HEP) applications. It trains the neural net with PyTorch and is capable of exporting the model to the ONNX format for fast inference. A detailed guide is presented on Weaver README page. Now we walk through three solid examples to get you familiar with Weaver . We use the benchmark of the top tagging task [ arXiv:1707.08966 ] in the following example. Some useful information can be found in the \"top tagging\" section in the IML public datasets webpage (the gDoc ). Our goal is to do some warm-up with Weaver , and more importantly, to explore from a technical side the neural net architectures: a simple multi-layer perceptron (MLP) model, a more complicated \"DeepAK8 tagger\" model based on 1D CNN with ResNet, and the \"ParticleNet model,\" which is based on DGCNN. We will dig deeper into their implementations in Weaver and try to illustrate as many details as possible. Finally, we compare their performance and see if we can reproduce the benchmark record with the model. Please clone the repo weaver-benchmark and we'll get started. The Weaver repo will be cloned as a submodule. git clone --recursive https://github.com/colizz/weaver-benchmark.git # Create a soft link inside weaver so that it can find data/model cards ln -s ../top_tagging weaver-benchmark/weaver/top_tagging 1. Build models in Weaver \u00b6 When implementing a new training in Weaver , two key elements are crucial: the model and the data configuration file. The model defines the network architecture we are using, and the data configuration includes which variables to use for training, which pre-selection to apply, how to assign truth labels, etc. Technically, The model configuration file includes a get_model function that returns a torch.nn.Module type model and a dictionary of model info used to export an ONNX-format model. The data configuration is a YAML file describing how to process the input data. Please see the Weaver README for details. Before moving on, we need a preprocessing of the benchmark datasets. The original sample is an H5 file including branches like energy E_i and 3-momenta PX_i , PY_i , PZ_i for each jet constituent i ( i =0, ..., 199) inside a jet. All branches are in the 1D flat structure. We reconstruct the data in a way that the jet features are 2D vectors (e.g., in the vector<float> format): Part_E , Part_PX , Part_PY , Part_PZ , with variable-length that corresponds to the number of constituents. Note that this is a commonly used data structure, similar to the NanoAOD format in CMS. The datasets can be found at CERN EOS space /eos/user/c/coli/public/weaver-benchmark/top_tagging/samples . The input files used in this page are in fact the ROOT files produced by the preprocessing step, stored under the prep/ subdirectory. It includes three sets of data for training, validation, and test. Note To preprocess the input files from the original datasets manually, direct to the weaver-benchmark base directory and run python utils / convert_top_datasets . py - i < your - sample - dir > This will convert the .h5 file to ROOT ntuples and create some new variables for each jet, including the relative \u03b7 and \u03c6 value w.r.t. main axis of the jet of each jet constituent. The converted files are stored in prep/ subfolder of the original directory. Then, we show three NN model configurations below and provide detailed explanations of the code. We make meticulous efforts on the illustration of the model architecture, especially in the ParticleNet case. A simple MLP The full architecture of the proof-of-concept multi-layer perceptron model. A simple multi-layer perceptron model is first provided here as proof of the concept. All layers are based on the linear transformation of the 1D vectors. The model configuration card is shown in top_tagging/networks/mlp_pf.py . First, we implement an MLP network in the nn.Module class. MLP implementation Also, see top_tagging/networks/mlp_pf.py . We elaborate here on several aspects. A sequence of linear layers and ReLU activation functions is defined in nn.Sequential(nn.Linear(channels[i], channels[i + 1]), nn.ReLU()) . By combining multiple of them, we construct a simple multi-layer perceptron. The input data x takes the 3D format, in the dimension (N, C, P) , which is decided by our data structure and the data configuration card. Here, N is the mini-batch size, C is the feature size, and P is the size of constituents per jet. To feed into our MLP, we flatten the last two dimensions by x = x.flatten(start_dim=1) to form the vector of dimension (N, L) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class MultiLayerPerceptron ( nn . Module ): r \"\"\"Parameters ---------- input_dims : int Input feature dimensions. num_classes : int Number of output classes. layer_params : list List of the feature size for each layer. \"\"\" def __init__ ( self , input_dims , num_classes , layer_params = ( 1024 , 256 , 256 ), ** kwargs ): super ( MultiLayerPerceptron , self ) . __init__ ( ** kwargs ) channels = [ input_dims ] + list ( layer_params ) + [ num_classes ] layers = [] for i in range ( len ( channels ) - 1 ): layers . append ( nn . Sequential ( nn . Linear ( channels [ i ], channels [ i + 1 ]), nn . ReLU ())) self . mlp = nn . Sequential ( * layers ) def forward ( self , x ): # x: the feature vector initally read from the data structure, in dimension (N, C, P) x = x . flatten ( start_dim = 1 ) # (N, L), where L = C * P return self . mlp ( x ) Then, we write the get_model and get_loss functions which will be sent into Weaver 's training code. get_model and get_loss function Also see top_tagging/networks/mlp_pf.py . We elaborate here on several aspects. Inside get_model , the model is essentially the MLP class we define, and the model_info takes the default definition, including the input/output shape, the dimensions of the dynamic axes for the input/output data shape that will guide the ONNX model exportation. The get_loss function is not changed as in the classification task we always use the cross-entropy loss function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def get_model ( data_config , ** kwargs ): layer_params = ( 1024 , 256 , 256 ) _ , pf_length , pf_features_dims = data_config . input_shapes [ 'pf_features' ] input_dims = pf_length * pf_features_dims num_classes = len ( data_config . label_value ) model = MultiLayerPerceptron ( input_dims , num_classes , layer_params = layer_params ) model_info = { 'input_names' : list ( data_config . input_names ), 'input_shapes' :{ k :(( 1 ,) + s [ 1 :]) for k , s in data_config . input_shapes . items ()}, 'output_names' :[ 'softmax' ], 'dynamic_axes' :{ ** { k :{ 0 : 'N' , 2 : 'n_' + k . split ( '_' )[ 0 ]} for k in data_config . input_names }, ** { 'softmax' :{ 0 : 'N' }}}, } print ( model , model_info ) return model , model_info def get_loss ( data_config , ** kwargs ): return torch . nn . CrossEntropyLoss () The output below shows the full structure of the MLP network printed by PyTorch. You will see it in the Weaver output during the training. The full-scale structure of the MLP network MultiLayerPerceptron( |0.739 M, 100.000% Params, 0.001 GMac, 100.000% MACs| (mlp): Sequential( |0.739 M, 100.000% Params, 0.001 GMac, 100.000% MACs| (0): Sequential( |0.411 M, 55.540% Params, 0.0 GMac, 55.563% MACs| (0): Linear(in_features=400, out_features=1024, bias=True, |0.411 M, 55.540% Params, 0.0 GMac, 55.425% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.138% MACs|) ) (1): Sequential( |0.262 M, 35.492% Params, 0.0 GMac, 35.452% MACs| (0): Linear(in_features=1024, out_features=256, bias=True, |0.262 M, 35.492% Params, 0.0 GMac, 35.418% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.035% MACs|) ) (2): Sequential( |0.066 M, 8.899% Params, 0.0 GMac, 8.915% MACs| (0): Linear(in_features=256, out_features=256, bias=True, |0.066 M, 8.899% Params, 0.0 GMac, 8.880% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.035% MACs|) ) (3): Sequential( |0.001 M, 0.070% Params, 0.0 GMac, 0.070% MACs| (0): Linear(in_features=256, out_features=2, bias=True, |0.001 M, 0.070% Params, 0.0 GMac, 0.069% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) ) ) ) The data card is shown in top_tagging/data/pf_features.yaml . It defines one input group, pf_features , which takes four variables Etarel , Phirel , E_log , P_log . This is based on our data structure, where these variables are 2D vectors with variable lengths. The length is chosen as 100 in a way that the last dimension (the jet constituent dimension) is always truncated or padded to have length 100. MLP data config top_tagging/data/pf_features.yaml Also see top_tagging/data/pf_features.yaml . See a tour guide to the data configuration card in Weaver README . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 selection : ### use `&`, `|`, `~` for logical operations on numpy arrays ### can use functions from `math`, `np` (numpy), and `awkward` in the expression new_variables : ### [format] name: formula ### can use functions from `math`, `np` (numpy), and `awkward` in the expression is_bkg : np.logical_not(is_signal_new) preprocess : ### method: [manual, auto] - whether to use manually specified parameters for variable standardization method : manual ### data_fraction: fraction of events to use when calculating the mean/scale for the standardization data_fraction : inputs : pf_features : length : 100 vars : ### [format 1]: var_name (no transformation) ### [format 2]: [var_name, ### subtract_by(optional, default=None, no transf. if preprocess.method=manual, auto transf. if preprocess.method=auto), ### multiply_by(optional, default=1), ### clip_min(optional, default=-5), ### clip_max(optional, default=5), ### pad_value(optional, default=0)] - Part_Etarel - Part_Phirel - [ Part_E_log , 2 , 1 ] - [ Part_P_log , 2 , 1 ] labels : ### type can be `simple`, `custom` ### [option 1] use `simple` for binary/multi-class classification, then `value` is a list of 0-1 labels type : simple value : [ is_signal_new , is_bkg ] ### [option 2] otherwise use `custom` to define the label, then `value` is a map # type: custom # value: # target_mass: np.where(fj_isQCD, fj_genjet_sdmass, fj_gen_mass) observers : - origIdx - idx - Part_E_tot - Part_PX_tot - Part_PY_tot - Part_PZ_tot - Part_P_tot - Part_Eta_tot - Part_Phi_tot # weights: ### [option 1] use precomputed weights stored in the input files # use_precomputed_weights: true # weight_branches: [weight, class_weight] ### [option 2] compute weights on-the-fly using reweighting histograms In the following two models (i.e., the DeepAK8 and the ParticleNet model) you will see that the data card is very similar. The change will only be the way we present the input group(s). DeepAK8 (1D CNN) The full architecture of the DeepAK8 model, which is based on 1D CNN with ResNet architecture. Note The DeepAK8 tagger is a widely used highly-boosted jet tagger in the CMS community. The design of the model can be found in the CMS paper [ arXiv:2004.08262 ]. The original model is trained on MXNet and its configuration can be found here . We now migrate the model architecture to Weaver and train it on PyTorch. Also, we narrow the multi-class output score to the binary output to adapt our binary classification task (top vs. QCD jet). The model card is given in top_tagging/networks/deepak8_pf.py . The DeepAK8 model is inspired by the ResNet architecture. The key ingredient is the ResNet unit constructed by multiple CNN layers with a shortcut connection. First, we define the ResNet unit in the model card. ResNet unit implementation See top_tagging/networks/deepak8_pf.py . We elaborate here on several aspects. A ResNet unit is made of two 1D CNNs with batch normalization and ReLU activation function. The shortcut is introduced here by directly adding the input data to the processed data after passing the CNN layers. The shortcut connection help to ease the training for the \"deeper\" model [ arXiv:1512.03385 ]. Note that a trivial linear transformation is applied ( self.conv_sc ) if the feature dimension of the input and output data does not match. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class ResNetUnit ( nn . Module ): r \"\"\"Parameters ---------- in_channels : int Number of channels in the input vectors. out_channels : int Number of channels in the output vectors. strides: tuple Strides of the two convolutional layers, in the form of (stride0, stride1) \"\"\" def __init__ ( self , in_channels , out_channels , strides = ( 1 , 1 ), ** kwargs ): super ( ResNetUnit , self ) . __init__ ( ** kwargs ) self . conv1 = nn . Conv1d ( in_channels , out_channels , kernel_size = 3 , stride = strides [ 0 ], padding = 1 ) self . bn1 = nn . BatchNorm1d ( out_channels ) self . conv2 = nn . Conv1d ( out_channels , out_channels , kernel_size = 3 , stride = strides [ 1 ], padding = 1 ) self . bn2 = nn . BatchNorm1d ( out_channels ) self . relu = nn . ReLU () self . dim_match = True if not in_channels == out_channels or not strides == ( 1 , 1 ): # dimensions not match self . dim_match = False self . conv_sc = nn . Conv1d ( in_channels , out_channels , kernel_size = 1 , stride = strides [ 0 ] * strides [ 1 ], bias = False ) def forward ( self , x ): identity = x x = self . conv1 ( x ) x = self . bn1 ( x ) x = self . relu ( x ) x = self . conv2 ( x ) x = self . bn2 ( x ) x = self . relu ( x ) # print('resnet unit', identity.shape, x.shape, self.dim_match) if self . dim_match : return identity + x else : return self . conv_sc ( identity ) + x With the ResNet unit, we construct the DeepAK8 model. The model hyperparameters are chosen as follows. conv_params = [( 32 ,), ( 64 , 64 ), ( 64 , 64 ), ( 128 , 128 )] fc_params = [( 512 , 0.2 )] DeepAK8 model implementation See top_tagging/networks/deepak8_pf.py . Note that the main architecture is a PyTorch re-implementation of the code here based on the MXNet. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class ResNet ( nn . Module ): r \"\"\"Parameters ---------- features_dims : int Input feature dimensions. num_classes : int Number of output classes. conv_params : list List of the convolution layer parameters. The first element is a tuple of size 1, defining the transformed feature size for the initial feature convolution layer. The following are tuples of feature size for multiple stages of the ResNet units. Each number defines an individual ResNet unit. fc_params: list List of fully connected layer parameters after all EdgeConv blocks, each element in the format of (n_feat, drop_rate) \"\"\" def __init__ ( self , features_dims , num_classes , conv_params = [( 32 ,), ( 64 , 64 ), ( 64 , 64 ), ( 128 , 128 )], fc_params = [( 512 , 0.2 )], ** kwargs ): super ( ResNet , self ) . __init__ ( ** kwargs ) self . conv_params = conv_params self . num_stages = len ( conv_params ) - 1 self . fts_conv = nn . Sequential ( nn . Conv1d ( in_channels = features_dims , out_channels = conv_params [ 0 ][ 0 ], kernel_size = 3 , stride = 1 , padding = 1 ), nn . BatchNorm1d ( conv_params [ 0 ][ 0 ]), nn . ReLU ()) # define ResNet units for each stage. Each unit is composed of a sequence of ResNetUnit block self . resnet_units = nn . ModuleDict () for i in range ( self . num_stages ): # stack units[i] layers in this stage unit_layers = [] for j in range ( len ( conv_params [ i + 1 ])): in_channels , out_channels = ( conv_params [ i ][ - 1 ], conv_params [ i + 1 ][ 0 ]) if j == 0 \\ else ( conv_params [ i + 1 ][ j - 1 ], conv_params [ i + 1 ][ j ]) strides = ( 2 , 1 ) if ( j == 0 and i > 0 ) else ( 1 , 1 ) unit_layers . append ( ResNetUnit ( in_channels , out_channels , strides )) self . resnet_units . add_module ( 'resnet_unit_ %d ' % i , nn . Sequential ( * unit_layers )) # define fully connected layers fcs = [] for idx , layer_param in enumerate ( fc_params ): channels , drop_rate = layer_param in_chn = conv_params [ - 1 ][ - 1 ] if idx == 0 else fc_params [ idx - 1 ][ 0 ] fcs . append ( nn . Sequential ( nn . Linear ( in_chn , channels ), nn . ReLU (), nn . Dropout ( drop_rate ))) fcs . append ( nn . Linear ( fc_params [ - 1 ][ 0 ], num_classes )) self . fc = nn . Sequential ( * fcs ) def forward ( self , x ): # x: the feature vector, (N, C, P) x = self . fts_conv ( x ) for i in range ( self . num_stages ): x = self . resnet_units [ 'resnet_unit_ %d ' % i ]( x ) # (N, C', P'), P'<P due to kernal_size>1 or stride>1 # global average pooling x = x . sum ( dim =- 1 ) / x . shape [ - 1 ] # (N, C') # fully connected x = self . fc ( x ) # (N, out_chn) return x def get_model ( data_config , ** kwargs ): conv_params = [( 32 ,), ( 64 , 64 ), ( 64 , 64 ), ( 128 , 128 )] fc_params = [( 512 , 0.2 )] pf_features_dims = len ( data_config . input_dicts [ 'pf_features' ]) num_classes = len ( data_config . label_value ) model = ResNet ( pf_features_dims , num_classes , conv_params = conv_params , fc_params = fc_params ) model_info = { 'input_names' : list ( data_config . input_names ), 'input_shapes' :{ k :(( 1 ,) + s [ 1 :]) for k , s in data_config . input_shapes . items ()}, 'output_names' :[ 'softmax' ], 'dynamic_axes' :{ ** { k :{ 0 : 'N' , 2 : 'n_' + k . split ( '_' )[ 0 ]} for k in data_config . input_names }, ** { 'softmax' :{ 0 : 'N' }}}, } print ( model , model_info ) print ( data_config . input_shapes ) return model , model_info def get_loss ( data_config , ** kwargs ): return torch . nn . CrossEntropyLoss () The output below shows the full structure of the DeepAK8 model based on 1D CNN with ResNet. It is printed by PyTorch and you will see it in the Weaver output during training. The full-scale structure of the DeepAK8 architecture ResNet( |0.349 M, 100.000% Params, 0.012 GMac, 100.000% MACs| (fts_conv): Sequential( |0.0 M, 0.137% Params, 0.0 GMac, 0.427% MACs| (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=(1,), |0.0 M, 0.119% Params, 0.0 GMac, 0.347% MACs|) (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.018% Params, 0.0 GMac, 0.053% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.027% MACs|) ) (resnet_units): ModuleDict( |0.282 M, 80.652% Params, 0.012 GMac, 99.010% MACs| (resnet_unit_0): Sequential( |0.046 M, 13.124% Params, 0.005 GMac, 38.409% MACs| (0): ResNetUnit( |0.021 M, 5.976% Params, 0.002 GMac, 17.497% MACs| (conv1): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.006 M, 1.778% Params, 0.001 GMac, 5.175% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 10.296% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.107% MACs|) (conv_sc): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False, |0.002 M, 0.587% Params, 0.0 GMac, 1.707% MACs|) ) (1): ResNetUnit( |0.025 M, 7.149% Params, 0.003 GMac, 20.912% MACs| (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 10.296% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 10.296% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.107% MACs|) ) ) (resnet_unit_1): Sequential( |0.054 M, 15.471% Params, 0.003 GMac, 22.619% MACs| (0): ResNetUnit( |0.029 M, 8.322% Params, 0.001 GMac, 12.163% MACs| (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) (conv_sc): Conv1d(64, 64, kernel_size=(1,), stride=(2,), bias=False, |0.004 M, 1.173% Params, 0.0 GMac, 1.707% MACs|) ) (1): ResNetUnit( |0.025 M, 7.149% Params, 0.001 GMac, 10.456% MACs| (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) ) ) (resnet_unit_2): Sequential( |0.182 M, 52.057% Params, 0.005 GMac, 37.982% MACs| (0): ResNetUnit( |0.083 M, 23.682% Params, 0.002 GMac, 17.284% MACs| (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), |0.025 M, 7.075% Params, 0.001 GMac, 5.148% MACs|) (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), |0.049 M, 14.114% Params, 0.001 GMac, 10.269% MACs|) (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) (conv_sc): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False, |0.008 M, 2.346% Params, 0.0 GMac, 1.707% MACs|) ) (1): ResNetUnit( |0.099 M, 28.375% Params, 0.002 GMac, 20.698% MACs| (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), |0.049 M, 14.114% Params, 0.001 GMac, 10.269% MACs|) (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), |0.049 M, 14.114% Params, 0.001 GMac, 10.269% MACs|) (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) ) ) ) (fc): Sequential( |0.067 M, 19.210% Params, 0.0 GMac, 0.563% MACs| (0): Sequential( |0.066 M, 18.917% Params, 0.0 GMac, 0.555% MACs| (0): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 18.917% Params, 0.0 GMac, 0.551% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.004% MACs|) (2): Dropout(p=0.2, inplace=False, |0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) ) (1): Linear(in_features=512, out_features=2, bias=True, |0.001 M, 0.294% Params, 0.0 GMac, 0.009% MACs|) ) ) The data card is the same as the MLP case, shown in top_tagging/data/pf_features.yaml . ParticleNet (DGCNN) The full architecture of the ParticleNet model, which is based on DGCNN and EdgeConv. Note The ParticleNet model applied to the CMS analysis is provided in weaver/networks/particle_net_pf_sv.py , and the data card in weaver/data/ak15_points_pf_sv.yaml . Here we use a similar configuration card to deal with the benchmark task. We will elaborate on the ParticleNet model and focus more on the technical side in this section. The model is defined in top_tagging/networks/particlenet_pf.py , but it imports some constructor, the EdgeConv block, in weaver/utils/nn/model/ParticleNet.py . The EdgeConv is illustrated in the cartoon. Illustration of the EdgeConv block From an EdgeConv block's point of view, it requires two classes of features as input: the \"coordinates\" and the \"features\". These features are the per point properties, in the 2D shape with dimensions (C, P) , where C is the size of the features (the feature size of \"coordinates\" and the \"features\" can be different, marked as C_pts , C_fts in the following code), and P is the number of points. The block outputs the new features that the model learns, also in the 2D shape with dimensions (C_fts_out, P) . What happens inside the EdgeConv block? And how is the output feature vector transferred from the input features using the topology of the point cloud? The answer is encoded in the edge convolution (EdgeConv). The edge convolution is an analogue convolution method defined on a point cloud, whose shape is given by the \"coordinates\" of points. Specifically, the input \"coordinates\" provide a view of spatial relations of the points in the Euclidean space. It determines the k -nearest neighbouring points for each point that will guide the update of the feature vector of a point. For each point, the updated feature vector is based on the current state of the point and its k neighbours. Guided by this spirit, all features of the point cloud forms a 3D vector with dimensions (C, P, K) , where C is the per-point feature size (e.g., \u03b7 , \u03c6 , p T \uff0c...), P is the number of points, and K the k -NN number. The structured vector is linearly transformed by acting 2D CNN on the feature dimension C . This helps to aggregate the feature information and exploit the correlations of each point with its adjacent points. A shortcut connection is also introduced inspired by the ResNet. Note The feature dimension C after exploring the k neighbours of each point actually doubles the value of the initial feature dimension. Here, a new set of features is constructed by subtracting the feature a point carries to the features its k neighbours carry (namely x i \u2013 x i_j for point i , and j =1,..., k ). This way, the correlation of each point with its neighbours are well captured. Below shows how the EdgeConv structure is implemented in the code. EdgeConv block implementation See weaver/utils/nn/model/ParticleNet.py , or the following code block annotated with more comments. We elaborate here on several aspects. The EdgeConvBlock takes the feature dimension in_feat , out_feats which are C_fts , C_fts_out we introduced above. The input data vectors to forward() are \"coordinates\" and \"features\" vector, in the dimension of (N, C_pts(C_fts), P) as introduced above. The first dimension is the mini-batch size. self.get_graph_feature() helps to aggregate k -nearest neighbours for each point. The resulting vector is in the dimension of (N, C_fts(0), P, K) as we discussed above, K being the k -NN number. Note that the C_fts(0) doubles the value of the original input feature dimension C_fts as mentioned above. After convolutions, the per-point features are merged by taking the mean of all k -nearest neighbouring vectors: fts = x . mean ( dim =- 1 ) # (N, C, P) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class EdgeConvBlock ( nn . Module ): r \"\"\"EdgeConv layer. Introduced in \"`Dynamic Graph CNN for Learning on Point Clouds <https://arxiv.org/pdf/1801.07829>`__\". Can be described as follows: .. math:: x_i^{(l+1)} = \\max_{j \\in \\mathcal{N}(i)} \\mathrm{ReLU}( \\Theta \\cdot (x_j^{(l)} - x_i^{(l)}) + \\Phi \\cdot x_i^{(l)}) where :math:`\\mathcal{N}(i)` is the neighbor of :math:`i`. Parameters ---------- in_feat : int Input feature size. out_feat : int Output feature size. batch_norm : bool Whether to include batch normalization on messages. \"\"\" def __init__ ( self , k , in_feat , out_feats , batch_norm = True , activation = True , cpu_mode = False ): super ( EdgeConvBlock , self ) . __init__ () self . k = k self . batch_norm = batch_norm self . activation = activation self . num_layers = len ( out_feats ) self . get_graph_feature = get_graph_feature_v2 if cpu_mode else get_graph_feature_v1 self . convs = nn . ModuleList () for i in range ( self . num_layers ): self . convs . append ( nn . Conv2d ( 2 * in_feat if i == 0 else out_feats [ i - 1 ], out_feats [ i ], kernel_size = 1 , bias = False if self . batch_norm else True )) if batch_norm : self . bns = nn . ModuleList () for i in range ( self . num_layers ): self . bns . append ( nn . BatchNorm2d ( out_feats [ i ])) if activation : self . acts = nn . ModuleList () for i in range ( self . num_layers ): self . acts . append ( nn . ReLU ()) if in_feat == out_feats [ - 1 ]: self . sc = None else : self . sc = nn . Conv1d ( in_feat , out_feats [ - 1 ], kernel_size = 1 , bias = False ) self . sc_bn = nn . BatchNorm1d ( out_feats [ - 1 ]) if activation : self . sc_act = nn . ReLU () def forward ( self , points , features ): # points: (N, C_pts, P) # features: (N, C_fts, P) # N: batch size, C: feature size per point, P: number of points topk_indices = knn ( points , self . k ) # (N, P, K) x = self . get_graph_feature ( features , self . k , topk_indices ) # (N, C_fts(0), P, K) for conv , bn , act in zip ( self . convs , self . bns , self . acts ): x = conv ( x ) # (N, C', P, K) if bn : x = bn ( x ) if act : x = act ( x ) fts = x . mean ( dim =- 1 ) # (N, C, P) # shortcut if self . sc : sc = self . sc ( features ) # (N, C_out, P) sc = self . sc_bn ( sc ) else : sc = features return self . sc_act ( sc + fts ) # (N, C_out, P) With the EdgeConv architecture as the building block, the ParticleNet model is constructed as follow. The ParticleNet model stacks three EdgeConv blocks to construct higher-level features and passing them through the pipeline. The points (i.e., in our case, the particle candidates inside a jet) are not changing, but the per-point \"coordinates\" and \"features\" vectors changes, in both values and dimensions. For the first EdgeConv block, the \"coordinates\" only include the relative \u03b7 and \u03c6 value of each particle. The \"features\" is a vector with a standard length of 32, which is linearly transformed from the initial feature vectors including the components of relative \u03b7 , \u03c6 , the log of p T , etc. The first EdgeConv block outputs a per-point feature vector of length 64, which is taken as both the \"coordinates\" and \"features\" to the next EdgeConv block. That is to say, the next k -NN is applied on the 64D high-dimensional spatial space to capture the new relations of points learned by the model. This is visualized by the input/output arrows showing the data flow of the model. We see that this architecture illustrates the stackability of the EdgeConv block, and is the core to the Dynamic Graph CNN (DGCNN), as the model can dynamically change the correlations of each point based on learnable features. A fusion technique is also used by concatenating the three EdgeConv output vectors together (adding the dimensions), instead of using the last EdgeConv output, to form an output vector. This is also one form of shortcut implementations that helps to ease the training for a complex and deep convolutional network model. The concatenated vectors per point are then averaged over points to produce a single 1D vector of the whole point cloud. The vector passes through one fully connected layer, with a dropout rate of p=0.1 to prevent overfitting. Then, in our example, the full network outputs two scores after a softmax, representing the one-hot encoding of the top vs. QCD class. The ParticleNet implementation is shown below. ParticleNet model implementation See weaver/utils/nn/model/ParticleNet.py , or the following code block annotated with more comments. We elaborate here on several mean points. The stack of multiple EdgeConv blocks are implemented in for idx , conv in enumerate ( self . edge_convs ): pts = ( points if idx == 0 else fts ) + coord_shift fts = conv ( pts , fts ) * mask The multiple EdgeConv layer parameters are given by conv_params , which takes a list of tuples, each tuple in the format of (K, (C1, C2, C3)) . K for the k -NN number, C1,2,3 for convolution feature sizes of three layers in an EdgeConv block. The fully connected layer parameters are given by fc_params , which takes a list of tuples, each tuple in the format of (n_feat, drop_rate) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class ParticleNet ( nn . Module ): r \"\"\"Parameters ---------- input_dims : int Input feature dimensions (C_fts). num_classes : int Number of output classes. conv_params : list List of convolution parameters of EdgeConv blocks, each element in the format of (K, (C1, C2, C3)). K for the kNN number, C1,2,3 for convolution feature sizes of three layers in an EdgeConv block. fc_params: list List of fully connected layer parameters after all EdgeConv blocks, each element in the format of (n_feat, drop_rate) use_fusion: bool If true, concatenates all output features from each EdgeConv before the fully connected layer. use_fts_bn: bool If true, applies a batch norm before feeding to the EdgeConv block. use_counts: bool If true, uses the real count of points instead of the padded size (the max point size). for_inference: bool Whether this is an inference routine. If true, applies a softmax to the output. for_segmentation: bool Whether the model is set up for the point cloud segmentation (instead of classification) task. If true, does not merge the features after the last EdgeConv, and apply Conv1D instead of the linear layer. The output is hence each output_features per point, instead of output_features. \"\"\" def __init__ ( self , input_dims , num_classes , conv_params = [( 7 , ( 32 , 32 , 32 )), ( 7 , ( 64 , 64 , 64 ))], fc_params = [( 128 , 0.1 )], use_fusion = True , use_fts_bn = True , use_counts = True , for_inference = False , for_segmentation = False , ** kwargs ): super ( ParticleNet , self ) . __init__ ( ** kwargs ) self . use_fts_bn = use_fts_bn if self . use_fts_bn : self . bn_fts = nn . BatchNorm1d ( input_dims ) self . use_counts = use_counts self . edge_convs = nn . ModuleList () for idx , layer_param in enumerate ( conv_params ): k , channels = layer_param in_feat = input_dims if idx == 0 else conv_params [ idx - 1 ][ 1 ][ - 1 ] self . edge_convs . append ( EdgeConvBlock ( k = k , in_feat = in_feat , out_feats = channels , cpu_mode = for_inference )) self . use_fusion = use_fusion if self . use_fusion : in_chn = sum ( x [ - 1 ] for _ , x in conv_params ) out_chn = np . clip (( in_chn // 128 ) * 128 , 128 , 1024 ) self . fusion_block = nn . Sequential ( nn . Conv1d ( in_chn , out_chn , kernel_size = 1 , bias = False ), nn . BatchNorm1d ( out_chn ), nn . ReLU ()) self . for_segmentation = for_segmentation fcs = [] for idx , layer_param in enumerate ( fc_params ): channels , drop_rate = layer_param if idx == 0 : in_chn = out_chn if self . use_fusion else conv_params [ - 1 ][ 1 ][ - 1 ] else : in_chn = fc_params [ idx - 1 ][ 0 ] if self . for_segmentation : fcs . append ( nn . Sequential ( nn . Conv1d ( in_chn , channels , kernel_size = 1 , bias = False ), nn . BatchNorm1d ( channels ), nn . ReLU (), nn . Dropout ( drop_rate ))) else : fcs . append ( nn . Sequential ( nn . Linear ( in_chn , channels ), nn . ReLU (), nn . Dropout ( drop_rate ))) if self . for_segmentation : fcs . append ( nn . Conv1d ( fc_params [ - 1 ][ 0 ], num_classes , kernel_size = 1 )) else : fcs . append ( nn . Linear ( fc_params [ - 1 ][ 0 ], num_classes )) self . fc = nn . Sequential ( * fcs ) self . for_inference = for_inference def forward ( self , points , features , mask = None ): # print('points:\\n', points) # print('features:\\n', features) if mask is None : mask = ( features . abs () . sum ( dim = 1 , keepdim = True ) != 0 ) # (N, 1, P) points *= mask features *= mask coord_shift = ( mask == 0 ) * 1e9 if self . use_counts : counts = mask . float () . sum ( dim =- 1 ) counts = torch . max ( counts , torch . ones_like ( counts )) # >=1 if self . use_fts_bn : fts = self . bn_fts ( features ) * mask else : fts = features outputs = [] for idx , conv in enumerate ( self . edge_convs ): pts = ( points if idx == 0 else fts ) + coord_shift fts = conv ( pts , fts ) * mask if self . use_fusion : outputs . append ( fts ) if self . use_fusion : fts = self . fusion_block ( torch . cat ( outputs , dim = 1 )) * mask # assert(((fts.abs().sum(dim=1, keepdim=True) != 0).float() - mask.float()).abs().sum().item() == 0) if self . for_segmentation : x = fts else : if self . use_counts : x = fts . sum ( dim =- 1 ) / counts # divide by the real counts else : x = fts . mean ( dim =- 1 ) output = self . fc ( x ) if self . for_inference : output = torch . softmax ( output , dim = 1 ) # print('output:\\n', output) return output Above are the capsulation of all ParticleNet building blocks. Eventually, we have the model defined in the model card top_tagging/networks/particlenet_pf.py , in the ParticleNetTagger1Path class, meaning we only use the ParticleNet pipeline that deals with one set of the point cloud (i.e., the particle candidates). Info Two sets of point clouds in the CMS application, namely the particle-flow candidates and secondary vertices, are used. This requires special handling to merge the clouds before feeding them to the first layer of EdgeConv. ParticleNet model config Also see top_tagging/networks/particlenet_pf.py . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 import torch import torch.nn as nn from utils.nn.model.ParticleNet import ParticleNet , FeatureConv class ParticleNetTagger1Path ( nn . Module ): def __init__ ( self , pf_features_dims , num_classes , conv_params = [( 7 , ( 32 , 32 , 32 )), ( 7 , ( 64 , 64 , 64 ))], fc_params = [( 128 , 0.1 )], use_fusion = True , use_fts_bn = True , use_counts = True , pf_input_dropout = None , for_inference = False , ** kwargs ): super ( ParticleNetTagger1Path , self ) . __init__ ( ** kwargs ) self . pf_input_dropout = nn . Dropout ( pf_input_dropout ) if pf_input_dropout else None self . pf_conv = FeatureConv ( pf_features_dims , 32 ) self . pn = ParticleNet ( input_dims = 32 , num_classes = num_classes , conv_params = conv_params , fc_params = fc_params , use_fusion = use_fusion , use_fts_bn = use_fts_bn , use_counts = use_counts , for_inference = for_inference ) def forward ( self , pf_points , pf_features , pf_mask ): if self . pf_input_dropout : pf_mask = ( self . pf_input_dropout ( pf_mask ) != 0 ) . float () pf_points *= pf_mask pf_features *= pf_mask return self . pn ( pf_points , self . pf_conv ( pf_features * pf_mask ) * pf_mask , pf_mask ) def get_model ( data_config , ** kwargs ): conv_params = [ ( 16 , ( 64 , 64 , 64 )), ( 16 , ( 128 , 128 , 128 )), ( 16 , ( 256 , 256 , 256 )), ] fc_params = [( 256 , 0.1 )] use_fusion = True pf_features_dims = len ( data_config . input_dicts [ 'pf_features' ]) num_classes = len ( data_config . label_value ) model = ParticleNetTagger1Path ( pf_features_dims , num_classes , conv_params , fc_params , use_fusion = use_fusion , use_fts_bn = kwargs . get ( 'use_fts_bn' , False ), use_counts = kwargs . get ( 'use_counts' , True ), pf_input_dropout = kwargs . get ( 'pf_input_dropout' , None ), for_inference = kwargs . get ( 'for_inference' , False ) ) model_info = { 'input_names' : list ( data_config . input_names ), 'input_shapes' :{ k :(( 1 ,) + s [ 1 :]) for k , s in data_config . input_shapes . items ()}, 'output_names' :[ 'softmax' ], 'dynamic_axes' :{ ** { k :{ 0 : 'N' , 2 : 'n_' + k . split ( '_' )[ 0 ]} for k in data_config . input_names }, ** { 'softmax' :{ 0 : 'N' }}}, } print ( model , model_info ) print ( data_config . input_shapes ) return model , model_info def get_loss ( data_config , ** kwargs ): return torch . nn . CrossEntropyLoss () The most important parameters are conv_params and fc_params , which decides the model parameters of EdgeConv blocks and the fully connected layer. See details in the above \"ParticleNet model implementation\" box. conv_params = [ ( 16 , ( 64 , 64 , 64 )), ( 16 , ( 128 , 128 , 128 )), ( 16 , ( 256 , 256 , 256 )), ] fc_params = [( 256 , 0.1 )] A full structure printed from PyTorch is shown below. It will appear in the Weaver output during training. ParticleNet full-scale structure ParticleNetTagger1Path( |0.577 M, 100.000% Params, 0.441 GMac, 100.000% MACs| (pf_conv): FeatureConv( |0.0 M, 0.035% Params, 0.0 GMac, 0.005% MACs| (conv): Sequential( |0.0 M, 0.035% Params, 0.0 GMac, 0.005% MACs| (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.001% Params, 0.0 GMac, 0.000% MACs|) (1): Conv1d(4, 32, kernel_size=(1,), stride=(1,), bias=False, |0.0 M, 0.022% Params, 0.0 GMac, 0.003% MACs|) (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.011% Params, 0.0 GMac, 0.001% MACs|) (3): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs|) ) ) (pn): ParticleNet( |0.577 M, 99.965% Params, 0.441 GMac, 99.995% MACs| (edge_convs): ModuleList( |0.305 M, 52.823% Params, 0.424 GMac, 96.047% MACs| (0): EdgeConvBlock( |0.015 M, 2.575% Params, 0.021 GMac, 4.716% MACs| (convs): ModuleList( |0.012 M, 2.131% Params, 0.02 GMac, 4.456% MACs| (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.004 M, 0.710% Params, 0.007 GMac, 1.485% MACs|) (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.004 M, 0.710% Params, 0.007 GMac, 1.485% MACs|) (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.004 M, 0.710% Params, 0.007 GMac, 1.485% MACs|) ) (bns): ModuleList( |0.0 M, 0.067% Params, 0.001 GMac, 0.139% MACs| (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.046% MACs|) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.046% MACs|) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.046% MACs|) ) (acts): ModuleList( |0.0 M, 0.000% Params, 0.0 GMac, 0.070% MACs| (0): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs|) ) (sc): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False, |0.002 M, 0.355% Params, 0.0 GMac, 0.046% MACs|) (sc_bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.003% MACs|) (sc_act): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs|) ) (1): EdgeConvBlock( |0.058 M, 10.121% Params, 0.081 GMac, 18.437% MACs| (convs): ModuleList( |0.049 M, 8.523% Params, 0.079 GMac, 17.825% MACs| (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.016 M, 2.841% Params, 0.026 GMac, 5.942% MACs|) (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.016 M, 2.841% Params, 0.026 GMac, 5.942% MACs|) (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.016 M, 2.841% Params, 0.026 GMac, 5.942% MACs|) ) (bns): ModuleList( |0.001 M, 0.133% Params, 0.001 GMac, 0.279% MACs| (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.093% MACs|) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.093% MACs|) (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.093% MACs|) ) (acts): ModuleList( |0.0 M, 0.000% Params, 0.001 GMac, 0.139% MACs| (0): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.046% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.046% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.046% MACs|) ) (sc): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False, |0.008 M, 1.420% Params, 0.001 GMac, 0.186% MACs|) (sc_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.006% MACs|) (sc_act): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.003% MACs|) ) (2): EdgeConvBlock( |0.231 M, 40.128% Params, 0.322 GMac, 72.894% MACs| (convs): ModuleList( |0.197 M, 34.091% Params, 0.315 GMac, 71.299% MACs| (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.066 M, 11.364% Params, 0.105 GMac, 23.766% MACs|) (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.066 M, 11.364% Params, 0.105 GMac, 23.766% MACs|) (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.066 M, 11.364% Params, 0.105 GMac, 23.766% MACs|) ) (bns): ModuleList( |0.002 M, 0.266% Params, 0.002 GMac, 0.557% MACs| (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.001 GMac, 0.186% MACs|) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.001 GMac, 0.186% MACs|) (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.001 GMac, 0.186% MACs|) ) (acts): ModuleList( |0.0 M, 0.000% Params, 0.001 GMac, 0.279% MACs| (0): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.093% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.093% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.093% MACs|) ) (sc): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False, |0.033 M, 5.682% Params, 0.003 GMac, 0.743% MACs|) (sc_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.0 GMac, 0.012% MACs|) (sc_act): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.006% MACs|) ) ) (fusion_block): Sequential( |0.173 M, 29.963% Params, 0.017 GMac, 3.925% MACs| (0): Conv1d(448, 384, kernel_size=(1,), stride=(1,), bias=False, |0.172 M, 29.830% Params, 0.017 GMac, 3.899% MACs|) (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.133% Params, 0.0 GMac, 0.017% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.009% MACs|) ) (fc): Sequential( |0.099 M, 17.179% Params, 0.0 GMac, 0.023% MACs| (0): Sequential( |0.099 M, 17.090% Params, 0.0 GMac, 0.022% MACs| (0): Linear(in_features=384, out_features=256, bias=True, |0.099 M, 17.090% Params, 0.0 GMac, 0.022% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) (2): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) ) (1): Linear(in_features=256, out_features=2, bias=True, |0.001 M, 0.089% Params, 0.0 GMac, 0.000% MACs|) ) ) ) The data card is shown in top_tagging/data/pf_points_features.yaml , given in a similar way as in the MLP example. Here we group the inputs into three classes: pf_points , pf_features and pf_masks . They correspond to the forward(self, pf_points, pf_features, pf_mask) prototype of our nn.Module model, and will send in these 2D vectors in the mini-batch size for each iteration during training/prediction. ParticleNet data config top_tagging/data/pf_points_features.yaml See top_tagging/data/pf_points_features.yaml . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 selection : ### use `&`, `|`, `~` for logical operations on numpy arrays ### can use functions from `math`, `np` (numpy), and `awkward` in the expression new_variables : ### [format] name: formula ### can use functions from `math`, `np` (numpy), and `awkward` in the expression pf_mask : awkward.JaggedArray.ones_like(Part_E) is_bkg : np.logical_not(is_signal_new) preprocess : ### method: [manual, auto] - whether to use manually specified parameters for variable standardization method : manual ### data_fraction: fraction of events to use when calculating the mean/scale for the standardization data_fraction : inputs : pf_points : length : 100 vars : - Part_Etarel - Part_Phirel pf_features : length : 100 vars : ### [format 1]: var_name (no transformation) ### [format 2]: [var_name, ### subtract_by(optional, default=None, no transf. if preprocess.method=manual, auto transf. if preprocess.method=auto), ### multiply_by(optional, default=1), ### clip_min(optional, default=-5), ### clip_max(optional, default=5), ### pad_value(optional, default=0)] - Part_Etarel - Part_Phirel - [ Part_E_log , 2 , 1 ] - [ Part_P_log , 2 , 1 ] pf_mask : length : 100 vars : - pf_mask labels : ### type can be `simple`, `custom` ### [option 1] use `simple` for binary/multi-class classification, then `value` is a list of 0-1 labels type : simple value : [ is_signal_new , is_bkg ] ### [option 2] otherwise use `custom` to define the label, then `value` is a map # type: custom # value: # target_mass: np.where(fj_isQCD, fj_genjet_sdmass, fj_gen_mass) observers : - origIdx - idx - Part_E_tot - Part_PX_tot - Part_PY_tot - Part_PZ_tot - Part_P_tot - Part_Eta_tot - Part_Phi_tot # weights: ### [option 1] use precomputed weights stored in the input files # use_precomputed_weights: true # weight_branches: [weight, class_weight] ### [option 2] compute weights on-the-fly using reweighting histograms Now we have walked through the detailed description of three networks in their architecture as well as their implementations in Weaver . Before ending this section, we summarize the three networks on their (1) model and data configuration cards, (2) the number of parameters, and (3) computational complexity in the following table. Note that we'll refer to the shell variables provided here in the following training example. Model ${PREFIX} ${MODEL_CONFIG} ${DATA_CONFIG} Parameters Computational complexity MLP mlp mlp_pf.py pf_features.yaml 739k 0.001 GMac DeepAK8 (1D CNN) deepak8 deepak8_pf.py pf_features.yaml 349k 0.012 GMac ParticleNet (DGCNN) particlenet particlenet_pf.py pf_points_features.yaml 577k 0.441 GMac 2. Start training! \u00b6 Now we train the three neural networks based on the provided model and data configurations. Here we present three ways of training. For readers who have a local machine with CUDA GPUs, please try out training on the local GPUs. Readers who would like to try on CPUs can also refer to the local GPU instruction. It is also possible to borrow the GPU resources from the lxplus HTCondor or CMS Connect. Please find in the following that meets your situation. Train on local GPUs The three networks can be trained with a universal script. Enter the weaver base folder and run the following command. Note that ${DATA_CONFIG} , ${MODEL_CONFIG} , and ${PREFIX} refers to the value in the above table for each example, and the fake path should be replaced with the correct one. PREFIX = '<prefix-from-table>' MODEL_CONFIG = '<model-config-from-table>' DATA_CONFIG = '<data-config-from-table>' PATH_TO_SAMPLES = '<your-path-to-samples>' python train . py \\ -- data - train $ { PATH_TO_SAMPLES } '/prep/top_train_*.root' \\ -- data - val $ { PATH_TO_SAMPLES } '/prep/top_val_*.root' \\ -- fetch - by - file -- fetch - step 1 -- num - workers 3 \\ -- data - config top_tagging / data / $ { DATA_CONFIG } \\ -- network - config top_tagging / networks / $ { MODEL_CONFIG } \\ -- model - prefix output / $ { PREFIX } \\ -- gpus 0 , 1 -- batch - size 1024 -- start - lr 5e-3 -- num - epochs 20 -- optimizer ranger \\ -- log output / $ { PREFIX } . train . log Here --gpus 0,1 specifies the GPUs to run with the device ID 1 and 2. For training on CPUs, please use --gpu '' . A detailed description of the training command can be found in Weaver README . Below we will note a few more caveats about the data loading options, though the specific settings will depend on the specifics of the input data. Caveats on the data loading options Our goal in data loading is to guarantee that the data loaded in every mini-batch is evenly distributed with different labels, though they are not necessarily stored evenly in the file. Besides, we also need to ensure that the on-the-fly loading and preprocessing of data should be smooth and not be a bottleneck of the data delivering pipeline. The total amount of loaded data also needs to be controlled so as not to explode the entire memory. The following guidelines should be used to choose the best options for your use case: in the default case, data are loaded from every input file with a small proportion per fetch-step, provided by --fetch-step (default is 0.01). This adapts to the case when we have multiple classes of input, each class having multiple files (e.g., it adapts to the real CMS application because we may have multiple nano_i.root files for different input classes). The strategy gathered all pieces per fetch-step from all input files, shuffle them, and present the data we need in each regular mini-batch. One can also append --num-workers n with n being the number of paralleled workers to load the data. --fetch-step 1 --num-workers 1 . This strategy helps in the case we have few input files with data in different labels not evenly distributed. In the extreme case, we only have 1 file, with all data at the top being one class (signal) and data at the bottom being another class (background), or we have 2 or multiple files, each containing a specific class. In this option, --fetch-step 1 guarantees the entire data in the file is loaded and participate in the shuffle. Therefore all classes are safely mixed before sending to the mini-batch. --num-workers 1 means we only use one worker that takes care of all files to avoid inconsistent loading speeds of multiple workers (depending on CPUs). This strategy can further cooperate with --in-memory so that all data are put permanently in memory and will not be reloaded every epoch. --fetch-by-file is the option we can use when all input files have a similar structure. See Weaver README : An alternative approach is the \"file-based\" strategy, which can be enabled with --fetch-by-files . This approach will instead read all events from every file for each step, and it will read m input files ( m is set by --fetch-step ) before mixing and shuffling the loaded events. This strategy is more suitable when each input file is already a mixture of all types of events (e.g., pre-processed with NNTools), otherwise it may lead to suboptimal training performance. However, a higher data loading speed can generally be achieved with this approach. Please note that you can test if all data classes are well mixed by printing the truth label in each mini-batch. Also, remember to test if data are loaded just-in-time by monitoring the GPU performance \u2014 if switching the data loading strategy helps improve the GPU efficiency, it means the previous data loader is the bottleneck in the pipeline to deliver and use the data. After training, we predict the score on the test datasets using the best model: PREFIX = '<prefix-from-table>' MODEL_CONFIG = '<model-config-from-table>' DATA_CONFIG = '<data-config-from-table>' PATH_TO_SAMPLES = '<your-path-to-samples>' python train . py -- predict \\ -- data - test $ { PATH_TO_SAMPLES } '/prep/top_test_*.root' \\ -- num - workers 3 \\ -- data - config top_tagging / data / $ { DATA_CONFIG } \\ -- network - config top_tagging / networks / $ { MODEL_CONFIG } \\ -- model - prefix output / $ { PREFIX } _best_epoch_state . pt \\ -- gpus 0 , 1 -- batch - size 1024 \\ -- predict - output output / $ { PREFIX } _predict . root Use GPUs on lxplus HTCondor On lxplus HTCondor, the GPU(s) can be booked via the arguments request_gpus . To get familiar with the GPU service, please refer to the documentation here . While it is not possible to test the script locally, you can try out the condor_ssh_to_job command to connect to the remote condor machine that runs the jobs. This interesting feature will help you with debugging or monitoring the condor job. Here we provide the example executed script and the condor submitted file for the training and predicting task. Create the following two files: The executable: run.sh Still, please remember to specify ${DATA_CONFIG} , ${MODEL_CONFIG} , and ${PREFIX} as shown in the above table, and replace the fake path with the correct one. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 #!/bin/bash PREFIX = $1 MODEL_CONFIG = $2 DATA_CONFIG = $3 PATH_TO_SAMPLES = $4 WORKDIR = ` pwd ` # Download miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda_install.sh bash miniconda_install.sh -b -p ${ WORKDIR } /miniconda export PATH = $WORKDIR /miniconda/bin: $PATH pip install numpy pandas scikit-learn scipy matplotlib tqdm PyYAML pip install uproot3 awkward0 lz4 xxhash pip install tables pip install onnxruntime-gpu pip install tensorboard pip install torch # CUDA environment setup export PATH = $PATH :/usr/local/cuda-10.2/bin export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/usr/local/cuda-10.2/lib64 export LIBRARY_PATH = $LIBRARY_PATH :/usr/local/cuda-10.2/lib64 # Clone weaver-benchmark git clone --recursive https://github.com/colizz/weaver-benchmark.git ln -s ../top_tagging weaver-benchmark/weaver/top_tagging cd weaver-benchmark/weaver/ mkdir output # Training, using 1 GPU python train.py \\ --data-train ${ PATH_TO_SAMPLES } '/prep/top_train_*.root' \\ --data-val ${ PATH_TO_SAMPLES } '/prep/top_val_*.root' \\ --fetch-by-file --fetch-step 1 --num-workers 3 \\ --data-config top_tagging/data/ ${ DATA_CONFIG } \\ --network-config top_tagging/networks/ ${ MODEL_CONFIG } \\ --model-prefix output/ ${ PREFIX } \\ --gpus 0 --batch-size 1024 --start-lr 5e-3 --num-epochs 20 --optimizer ranger \\ --log output/ ${ PREFIX } .train.log # Predicting score, using 1 GPU python train.py --predict \\ --data-test ${ PATH_TO_SAMPLES } '/prep/top_test_*.root' \\ --num-workers 3 \\ --data-config top_tagging/data/ ${ DATA_CONFIG } \\ --network-config top_tagging/networks/ ${ MODEL_CONFIG } \\ --model-prefix output/ ${ PREFIX } _best_epoch_state.pt \\ --gpus 0 --batch-size 1024 \\ --predict-output output/ ${ PREFIX } _predict.root [ -d \"runs/\" ] && tar -caf output.tar output/ runs/ || tar -caf output.tar output/ HTCondor submitted file: submit.sub Modify the argument line. These are the bash variable PREFIX , MODEL_CONFIG , DATA_CONFIG , PATH_TO_SAMPLES used in the Weaver command. Since the EOS directory is accessable accross all condor nodes on lxplus, one may directly specify <your-path-to-samples> as the EOS path provided above. An example is shown in the commented line. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Universe = vanilla executable = run.sh arguments = <prefix> <model-config> <data-config> <your-path-to-samples> #arguments = mlp mlp_pf.py pf_features.yaml /eos/user/c/coli/public/weaver-benchmark/top_tagging/samples output = job. $( ClusterId ) . $( ProcId ) .out error = job. $( ClusterId ) . $( ProcId ) .err log = job. $( ClusterId ) .log should_transfer_files = YES when_to_transfer_output = ON_EXIT_OR_EVICT transfer_output_files = weaver-benchmark/weaver/output.tar transfer_output_remaps = \"output.tar = output. $( ClusterId ) . $( ProcId ) .tar\" request_GPUs = 1 request_CPUs = 4 +MaxRuntime = 604800 queue Make the run.sh script an executable, then submit the job. chmod +x run.sh condor_submit submit.sub A tarball will be transfered back with the weaver/output directory where the trained models and the predicted ROOT file are stored. Use GPUs on CMS Connect CMS Connect provides several GPU nodes. One can request to run GPU condor jobs in a similar way as on lxplus, please refer to the link: https://ci-connect.atlassian.net/wiki/spaces/CMS/pages/80117822/Requesting+GPUs As the EOS user space may not be accessed from the remote node launched by CMS Connect, one may consider either (1) migrating the input files by condor, or (2) using XRootD to transfer the input file from EOS space to the condor node, before running the Weaver train command. 3. Evaluation of models \u00b6 In the output folder, we find the trained PyTorch models after every epoch and the log file that records the loss and accuracy in the runtime. The predict step also produces a predicted root file in the output folder, including the truth label, the predicted store, and several observer variables we provided in the data card. With the predicted root file, we make the ROC curve comparing the performance of the three trained models. Here is the result from my training: Model AUC Accuracy 1/ e B (@ e S =0.3) MLP 0.961 0.898 186 DeepAK8 (1D CNN) 0.979 0.927 585 ParticleNet (DGCNN) 0.984 0.936 1030 We see that the ParticleNet model shows an outstanding performance in this classification task. Besides, the DeepAK8 and ParticleNet results are similar to the benchmark values found in the gDoc . We address that the performance can be further improved by some following tricks: Train an ensemble of models with different initial parametrization. For each event/jet, take the final predicted score as the mean/median of the score ensembles predicted by each model. This is a widely used ML technique to pursue an extra few percent of improvements. Use more input variables for training. We note that in the above training example, only four input variables are used instead of a full suite of input features as done in the ParticleNet paper [ arXiv:1902.08570 ]. Additional variables (e.g. \u0394R or log( p T / p T (jet))) can be designed based on the given 4-momenta, and, although providing redundant information in principle, can still help the network fully exploit the point cloud structure and thus do a better discrimination job. The fine-tuning of the model will also bring some performance gain. See details in the next section. Tuning the ParticleNet model \u00b6 When it comes to the real application of any DNN model, tunning the hyperparameters is an important path towards a better performance. In this section, we provide some tips on the ParticleNet model tunning. For a more detailed discussion on this topic, see more in the \"validation\" chapter in the documentation. 1. Choices on the optimizer and the learning rate \u00b6 The optimizer decides how our neural network update all its parameters, and the learning rate means how fast the parameters changes in one training iteration. Learning rate is the most important hyperparameter to choose from before concrete training is done. Here we quote from a suggested strategy: if you only have the opportunity to optimize one hyperparameter, choose the learning rate. The optimizer is also important because a wiser strategy usually means avoid the zig-zagging updating route, avoid falling into the local minima and even adapting different strategies for the fast-changing parameters and the slow ones. Adam (and its several variations) is a widely used optimizer. Another recently developed advanced optimizer is Ranger that combines RAdam and LookAhead. However, one should note that the few percent level improvement by using different optimizers is likely to be smeared by an unoptimized learning rate. The above training scheme uses a start learning rate of 5e-3, and Ranger as the optimizer. It uses a flat+decay schedular, in a way that the LR starts to decay after processing 70% of epochs, and gradually reduce to 0.01 of its original value when nearing the completion of all epochs. First, we note that the current case is already well optimized. Therefore, by simply reuse the current choice, the training will converge to a stable result in general. But it is always good in practice to test several choices of the optimizer and reoptimize the learning rate. Weaver integrates multiple optimizers. In the above training command, we use --optimizer ranger to adopt the Ranger optimizer. It is also possible to switch to --optimizer adam or --optimizer adamW . Weaver also provides the interface to optimize the learning rate before real training is performed. In the ParticleNet model training, we append --lr-finder 5e-6,5e0,200 in the command, then a specific learning-rate finder program will be launched. This setup scans over the LR from 5e-6 to 5e0 by applying 200 mini-batches of training. It outputs a plot showing the training loss for different starting learning rates. In general, a lower training loss means a better choice of the learning rate parameter. Below shows the results from LR finder by specifying --lr-finder 5e-6,5e0,200 , for the --optimizer adamW (left) and the --optimizer ranger (right) case. The training loss forms a basin shape which indicates that the optimal learning rate falls somewhere in the middle. We extract two aspects from the plots. First, the basin covers a wide range, meaning that the LR finder only provides a rough estimation. But it is a good attempt to first run the LR finder to have an overall feeling. For the Ranger case (right figure), one can choose the range 1e-3 to 1e-2 and further determine the optminal learning rate by delivering the full training. Second, we should be aware that different optimizer takes different optimal LR values. As can be seen here, the AdamW in general requires a small LR than Ranger. 2. Visualize the training with TensorBoard \u00b6 To monitor the full training/evaluation accuracy and the loss for each mini-batch, we can draw support from a nicely integrated utility, TensorBoard, to employ real-time monitoring. See the introduction page from PyTorch: https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html To activate TensorBoard, append (note that replace ${PREFIX} according to the above table) --tensorboard ${ PREFIX } to the training command. The runs/ subfolder containing the TensorBoard monitoring log will appear in the Weaver directory (if you are launching condor jobs, the runs/ folder will be transferred back in the tarball). Then, one can run tensorboard --logdir = runs to start the TensorBoard service and go to URL https://localhost:6006 to view the TensorBoard dashboard. The below plots show the training and evaluation loss, in our standard choice with LR being 5e-3, and in the case of a small LR 2e-3 and a large LR 1e-2. Note that all tested LR values are within the basin in the LR finder plots. We see that in the evaluated loss plot, the standard LR outperforms two variational choices. The reason may be that a larger LR finds difficulty in converging to the global minima, while a smaller LR may not be adequate to reach the minima point in a journey of 20 epochs. Overall, we see 5e-3 as a good choice as the starting LR for the Ranger optimizer. 3. Optimize the model \u00b6 In practice, tuning the model size is also an important task. By concept, a smaller model tends to have unsatisfactory performance due to the limited ability to learn many local features. As the model size goes up, the performance will climb to some extent, but may further decrease due to the network \"degradation\" (deeper models have difficulty learning features). Besides, a heavier model may also cause the overfitting issue. In practice, it also leads to larger inference time which is the main concern when coming to real applications. For the ParticleNet model case, we also test between a smaller and larger variation of the model size. Recall that the original model is defined by the following layer parameters. conv_params = [ ( 16 , ( 64 , 64 , 64 )), ( 16 , ( 128 , 128 , 128 )), ( 16 , ( 256 , 256 , 256 )), ] fc_params = [( 256 , 0.1 )] We can replace the code block with ec_k = kwargs . get ( 'ec_k' , 16 ) ec_c1 = kwargs . get ( 'ec_c1' , 64 ) ec_c2 = kwargs . get ( 'ec_c2' , 128 ) ec_c3 = kwargs . get ( 'ec_c3' , 256 ) fc_c , fc_p = kwargs . get ( 'fc_c' , 256 ), kwargs . get ( 'fc_p' , 0.1 ) conv_params = [ ( ec_k , ( ec_c1 , ec_c1 , ec_c1 )), ( ec_k , ( ec_c2 , ec_c2 , ec_c2 )), ( ec_k , ( ec_c3 , ec_c3 , ec_c3 )), ] fc_params = [( fc_c , fc_p )] Then we have the ability to tune the model parameters from the command line. Append the extra arguments in the training command --network-option ec_k 32 --network-option ec_c1 128 --network-option ec_c2 192 --network-option ec_c3 256 and the model parameters will take the new values as specified. We test over two cases, one with the above setting to enlarge the model, and another by using --network-option ec_c1 64 --network-option ec_c2 64 --network-option ec_c3 96 to adopt a lite version. The Tensorboard monitoring plots in the training/evaluation loss is shown as follows. We see that the \"heavy\" model reaches even smaller training loss, meaning that the model does not meet the degradation issue yet. However, the evaluation loss is not catching up with the training loss, showing some degree of overtraining in this scheme. From the evaluation result, we see no improvement by moving to a heavy model. 4. Apply preselection and class weights \u00b6 In HEP applications, it is sometimes required to train a multi-class classifier. While it is simple to specify the input classes in the label section of the Weaver data config, it is sometimes ignored to set up the preselection and assign the suitable class weights for training. Using an unoptimized configuration, the trained model will not reach the best performance although no error message will result. Since our top tagging example is a binary classification problem, there is no specific need to configure the preselection and class weights. Below we summarize some experiences that may be applicable in reader's custom multi-class training task. The preselection should be chosen in a way that all remaining events passing the selection should fall into one and only one category. In other words, events with no labels attached should not be kept since it will confuse the training process. Class weights (the class_weights option under weights in the data config) control the relative importance of input sample categories for training. Implementation-wise, it changes the event probability in a specific category chosen as training input events. The class weight comes into effect when one trains a multi-class classifier. Take 3-class case (denoted as [A, B, C]) as an example, the class_weights: [1, 1, 1] gives equal weights to all categories. Retraining the input with class_weights: [10, 1, 1] may result in a better discriminating power for class A vs. B or A vs. C; while the power of B separating with C will be weakened. As a trade-off between separating A vs. C and B vs. C, the class weights need to be intentionally tuned to achieve reasonable performance. After the class weights are tuned, one can use another method to further factor out the interplay across categories, i.e., to define a \"binarized\" score between two classes only. Suppose the raw score for the three classes are P (A), P (B), and P (C) (their sum should be 1), then one can define the discriminant P (BvsC) = P (B) / ( P (B)+ P (C)) to separate B vs. C. In this way, the saparating power of B vs. C will remain unchanged for class_weights configured as either [1, 1, 1] or [10, 1, 1] . This strategy has been widely used in CMS to define composite tagger discrimant which are applied analysis-wise. Above, we discuss in a very detailed manner on various attempts we can make to optimize the model. We hope the practical experiences presented here will help readers develop and deploy the complex ML model.","title":"ParticleNet"},{"location":"inference/particlenet.html#particlenet","text":"ParticleNet [ arXiv:1902.08570 ] is an advanced neural network architecture that has many applications in CMS, including heavy flavour jet tagging, jet mass regression, etc. The network is fed by various low-level point-like objects as input, e.g., the particle-flow candidates, to predict a feature of a jet. The full architecture of the ParticleNet model. We'll walk through the details in the following sections. On this page, we introduce several user-specific aspects of the ParticleNet model. We cover the following items in three sections: An introduction to ParticleNet , including a general description of ParticleNet the advantages brought from the architecture by concept a sketch of ParticleNet applications in CMS and other relevant works An introduction to Weaver and model implementations , introduced in a step-by-step manner: build three network models and understand them from the technical side; use the out-of-the-box commands to run these examples on a benchmark task. The three networks are (1) a simple feed-forward NN, (2) a DeepAK8 model (based on 1D CNN), and eventually (3) the ParticleNet model (based on DGCNN). try to reproduce the original performance and make the ROC plots. This section is friendly to the ML newcomers. The goal is to help readers understand the underlying structure of the \"ParticleNet\". Tuning the ParticleNet model , including tips for readers who are using/modifying the ParticleNet model to achieve a better performance This section can be helpful in practice. It provides tips on model training, tunning, validation, etc. It targets the situations when readers apply their own ParticleNet (or ParticleNet-like) model to the custom task. Corresponding persons: Huilin Qu, Loukas Gouskos (original developers of ParticleNet) Congqiao Li (author of the page)","title":"ParticleNet"},{"location":"inference/particlenet.html#introduction-to-particlenet","text":"","title":"Introduction to ParticleNet"},{"location":"inference/particlenet.html#1-general-description","text":"ParticleNet is a graph neural net (GNN) model. The key ingredient of ParticleNet is the graph convolutional operation, i.e., the edge convolution (EdgeConv) and the dynamic graph CNN (DGCNN) method [ arXiv:1801.07829 ] applied on the \"point cloud\" data structure. We will disassemble the ParticleNet model and provide a detailed exploration in the next section, but here we briefly explain the key features of the model. Intuitively, ParticleNet treats all candidates inside an object as a \"point cloud\", which is a permutational-invariant set of points (e.g. a set of PF candidates), each carrying a feature vector ( \u03b7 , \u03c6 , p T , charge, etc.). The DGCNN uses the EdgeConv operation to exploit their spatial correlations (two-dimensional on the \u03b7 - \u03c6 plain) by finding the k -nearest neighbours of each point and generate a new latent graph layer where points are scattered on a high-dimensional latent space. This is a graph-type analogue of the classical 2D convolution operation, which acts on a regular 2D grid (e.g., a picture) using a 3\u00d73 local patch to explore the relations of a single-pixel with its 8 nearest pixels, then generates a new 2D grid. The cartoon illustrates the convolutional operation acted on the regular grid and on the point cloud (plot from ML4Jets 2018 talk). As a consequence, the EdgeConv operation transforms the graph to a new graph, which has a changed spatial relationship among points. It then acts on the second graph to produce the third graph, showing the stackability of the convolution operation. This illustrates the \"dynamic\" property as the graph topology changes after each EdgeConv layer.","title":"1. General description"},{"location":"inference/particlenet.html#2-advantage","text":"By concept, the advantage of the network may come from exploiting the permutational-invariant symmetry of the points, which is intrinsic to our physics objects. This symmetry is held naturally in a point cloud representation. In a recent study on jet physics or event-based analysis using ML techniques, there are increasing interest to explore the point cloud data structure. We explain here conceptually why a \"point cloud\" representation outperforms the classical ones, including the variable-length 2D vector structure passing to a 1D CNN or any type of RNN, and imaged-based representation passing through a 2D CNN. By using the 1D CNN, the points (PF candidates) are more often ordered by p T to fix on the 1D grid. Only correlations with neighbouring points with similar p T are learned by the network with a convolution operation. The Long Short-Term Memory (LSTM) type recurrent neural network (RNN) provides the flexibility to feed in a variant-length sequence and has a \"memory\" mechanism to cooperate the information it learns from an early node to the latest node. The concern is that such ordering of the sequence is somewhat artificial, and not an underlying property that an NN must learn to accomplish the classification task. As a comparison, in the task of the natural language processing where LSTM has a huge advantage, the order of words are important characteristic of a language itself (reflects the \"grammar\" in some circumstances) and is a feature the NN must learn to master the language. The imaged-based data explored by a 2D CNN stems from the image recognition task. A jet image with proper standardization is usually performed before feeding into the network. In this sense, it lacks local features which the 2D local patch is better at capturing, e.g. the ear of the cat that a local patch can capture by scanning over the entire image. The jet image is appearing to hold the features globally (e.g. two-prong structure for W-tagging). The sparsity of data is another concern in that it introduces redundant information to present a jet on the regular grid, making the network hard to capture the key properties.","title":"2. Advantage"},{"location":"inference/particlenet.html#3-applications-and-other-related-work","text":"Here we briefly summarize the applications and ongoing works on ParticleNet. Public CMS results include large- R jet with R =0.8 tagging (for W/Z/H/t) using ParticleNet [ CMS-DP-2020/002 ] regression on the large- R jet mass based on the ParticleNet model [ CMS-DP-2021/017 ] ParticleNet architecture is also applied on small radius R =0.4 jets for the b/c-tagging and quark/gluon classification (see this talk (CMS internal) ). A recent ongoing work applies the ParticleNet architecture in heavy flavour tagging at HLT (see this talk (CMS internal) ). The ParticleNet model is recently updated to ParticleNeXt and see further improvement (see the ML4Jets 2021 talk ). Recent works in the joint field of HEP and ML also shed light on exploiting the point cloud data structure and GNN-based architectures. We see very active progress in recent years. Here list some useful materials for the reader's reference. Some pheno-based work are summarized in the HEP \u00d7 ML living review , especially in the \"graph\" and \"sets\" categories. An overview of GNN applications to CMS, see CMS ML forum (CMS internal) . Also see more recent GNN application progress in ML forums: Oct 20 , Nov 3 . At the time of writing, various novel GNN-based models are explored and introduced in the recent ML4Jets2021 meeting.","title":"3. Applications and other related work"},{"location":"inference/particlenet.html#introduction-to-weaver-and-model-implementations","text":"Weaver is a machine learning R&D framework for high energy physics (HEP) applications. It trains the neural net with PyTorch and is capable of exporting the model to the ONNX format for fast inference. A detailed guide is presented on Weaver README page. Now we walk through three solid examples to get you familiar with Weaver . We use the benchmark of the top tagging task [ arXiv:1707.08966 ] in the following example. Some useful information can be found in the \"top tagging\" section in the IML public datasets webpage (the gDoc ). Our goal is to do some warm-up with Weaver , and more importantly, to explore from a technical side the neural net architectures: a simple multi-layer perceptron (MLP) model, a more complicated \"DeepAK8 tagger\" model based on 1D CNN with ResNet, and the \"ParticleNet model,\" which is based on DGCNN. We will dig deeper into their implementations in Weaver and try to illustrate as many details as possible. Finally, we compare their performance and see if we can reproduce the benchmark record with the model. Please clone the repo weaver-benchmark and we'll get started. The Weaver repo will be cloned as a submodule. git clone --recursive https://github.com/colizz/weaver-benchmark.git # Create a soft link inside weaver so that it can find data/model cards ln -s ../top_tagging weaver-benchmark/weaver/top_tagging","title":"Introduction to Weaver and model implementations"},{"location":"inference/particlenet.html#1-build-models-in-weaver","text":"When implementing a new training in Weaver , two key elements are crucial: the model and the data configuration file. The model defines the network architecture we are using, and the data configuration includes which variables to use for training, which pre-selection to apply, how to assign truth labels, etc. Technically, The model configuration file includes a get_model function that returns a torch.nn.Module type model and a dictionary of model info used to export an ONNX-format model. The data configuration is a YAML file describing how to process the input data. Please see the Weaver README for details. Before moving on, we need a preprocessing of the benchmark datasets. The original sample is an H5 file including branches like energy E_i and 3-momenta PX_i , PY_i , PZ_i for each jet constituent i ( i =0, ..., 199) inside a jet. All branches are in the 1D flat structure. We reconstruct the data in a way that the jet features are 2D vectors (e.g., in the vector<float> format): Part_E , Part_PX , Part_PY , Part_PZ , with variable-length that corresponds to the number of constituents. Note that this is a commonly used data structure, similar to the NanoAOD format in CMS. The datasets can be found at CERN EOS space /eos/user/c/coli/public/weaver-benchmark/top_tagging/samples . The input files used in this page are in fact the ROOT files produced by the preprocessing step, stored under the prep/ subdirectory. It includes three sets of data for training, validation, and test. Note To preprocess the input files from the original datasets manually, direct to the weaver-benchmark base directory and run python utils / convert_top_datasets . py - i < your - sample - dir > This will convert the .h5 file to ROOT ntuples and create some new variables for each jet, including the relative \u03b7 and \u03c6 value w.r.t. main axis of the jet of each jet constituent. The converted files are stored in prep/ subfolder of the original directory. Then, we show three NN model configurations below and provide detailed explanations of the code. We make meticulous efforts on the illustration of the model architecture, especially in the ParticleNet case. A simple MLP The full architecture of the proof-of-concept multi-layer perceptron model. A simple multi-layer perceptron model is first provided here as proof of the concept. All layers are based on the linear transformation of the 1D vectors. The model configuration card is shown in top_tagging/networks/mlp_pf.py . First, we implement an MLP network in the nn.Module class. MLP implementation Also, see top_tagging/networks/mlp_pf.py . We elaborate here on several aspects. A sequence of linear layers and ReLU activation functions is defined in nn.Sequential(nn.Linear(channels[i], channels[i + 1]), nn.ReLU()) . By combining multiple of them, we construct a simple multi-layer perceptron. The input data x takes the 3D format, in the dimension (N, C, P) , which is decided by our data structure and the data configuration card. Here, N is the mini-batch size, C is the feature size, and P is the size of constituents per jet. To feed into our MLP, we flatten the last two dimensions by x = x.flatten(start_dim=1) to form the vector of dimension (N, L) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class MultiLayerPerceptron ( nn . Module ): r \"\"\"Parameters ---------- input_dims : int Input feature dimensions. num_classes : int Number of output classes. layer_params : list List of the feature size for each layer. \"\"\" def __init__ ( self , input_dims , num_classes , layer_params = ( 1024 , 256 , 256 ), ** kwargs ): super ( MultiLayerPerceptron , self ) . __init__ ( ** kwargs ) channels = [ input_dims ] + list ( layer_params ) + [ num_classes ] layers = [] for i in range ( len ( channels ) - 1 ): layers . append ( nn . Sequential ( nn . Linear ( channels [ i ], channels [ i + 1 ]), nn . ReLU ())) self . mlp = nn . Sequential ( * layers ) def forward ( self , x ): # x: the feature vector initally read from the data structure, in dimension (N, C, P) x = x . flatten ( start_dim = 1 ) # (N, L), where L = C * P return self . mlp ( x ) Then, we write the get_model and get_loss functions which will be sent into Weaver 's training code. get_model and get_loss function Also see top_tagging/networks/mlp_pf.py . We elaborate here on several aspects. Inside get_model , the model is essentially the MLP class we define, and the model_info takes the default definition, including the input/output shape, the dimensions of the dynamic axes for the input/output data shape that will guide the ONNX model exportation. The get_loss function is not changed as in the classification task we always use the cross-entropy loss function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def get_model ( data_config , ** kwargs ): layer_params = ( 1024 , 256 , 256 ) _ , pf_length , pf_features_dims = data_config . input_shapes [ 'pf_features' ] input_dims = pf_length * pf_features_dims num_classes = len ( data_config . label_value ) model = MultiLayerPerceptron ( input_dims , num_classes , layer_params = layer_params ) model_info = { 'input_names' : list ( data_config . input_names ), 'input_shapes' :{ k :(( 1 ,) + s [ 1 :]) for k , s in data_config . input_shapes . items ()}, 'output_names' :[ 'softmax' ], 'dynamic_axes' :{ ** { k :{ 0 : 'N' , 2 : 'n_' + k . split ( '_' )[ 0 ]} for k in data_config . input_names }, ** { 'softmax' :{ 0 : 'N' }}}, } print ( model , model_info ) return model , model_info def get_loss ( data_config , ** kwargs ): return torch . nn . CrossEntropyLoss () The output below shows the full structure of the MLP network printed by PyTorch. You will see it in the Weaver output during the training. The full-scale structure of the MLP network MultiLayerPerceptron( |0.739 M, 100.000% Params, 0.001 GMac, 100.000% MACs| (mlp): Sequential( |0.739 M, 100.000% Params, 0.001 GMac, 100.000% MACs| (0): Sequential( |0.411 M, 55.540% Params, 0.0 GMac, 55.563% MACs| (0): Linear(in_features=400, out_features=1024, bias=True, |0.411 M, 55.540% Params, 0.0 GMac, 55.425% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.138% MACs|) ) (1): Sequential( |0.262 M, 35.492% Params, 0.0 GMac, 35.452% MACs| (0): Linear(in_features=1024, out_features=256, bias=True, |0.262 M, 35.492% Params, 0.0 GMac, 35.418% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.035% MACs|) ) (2): Sequential( |0.066 M, 8.899% Params, 0.0 GMac, 8.915% MACs| (0): Linear(in_features=256, out_features=256, bias=True, |0.066 M, 8.899% Params, 0.0 GMac, 8.880% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.035% MACs|) ) (3): Sequential( |0.001 M, 0.070% Params, 0.0 GMac, 0.070% MACs| (0): Linear(in_features=256, out_features=2, bias=True, |0.001 M, 0.070% Params, 0.0 GMac, 0.069% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) ) ) ) The data card is shown in top_tagging/data/pf_features.yaml . It defines one input group, pf_features , which takes four variables Etarel , Phirel , E_log , P_log . This is based on our data structure, where these variables are 2D vectors with variable lengths. The length is chosen as 100 in a way that the last dimension (the jet constituent dimension) is always truncated or padded to have length 100. MLP data config top_tagging/data/pf_features.yaml Also see top_tagging/data/pf_features.yaml . See a tour guide to the data configuration card in Weaver README . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 selection : ### use `&`, `|`, `~` for logical operations on numpy arrays ### can use functions from `math`, `np` (numpy), and `awkward` in the expression new_variables : ### [format] name: formula ### can use functions from `math`, `np` (numpy), and `awkward` in the expression is_bkg : np.logical_not(is_signal_new) preprocess : ### method: [manual, auto] - whether to use manually specified parameters for variable standardization method : manual ### data_fraction: fraction of events to use when calculating the mean/scale for the standardization data_fraction : inputs : pf_features : length : 100 vars : ### [format 1]: var_name (no transformation) ### [format 2]: [var_name, ### subtract_by(optional, default=None, no transf. if preprocess.method=manual, auto transf. if preprocess.method=auto), ### multiply_by(optional, default=1), ### clip_min(optional, default=-5), ### clip_max(optional, default=5), ### pad_value(optional, default=0)] - Part_Etarel - Part_Phirel - [ Part_E_log , 2 , 1 ] - [ Part_P_log , 2 , 1 ] labels : ### type can be `simple`, `custom` ### [option 1] use `simple` for binary/multi-class classification, then `value` is a list of 0-1 labels type : simple value : [ is_signal_new , is_bkg ] ### [option 2] otherwise use `custom` to define the label, then `value` is a map # type: custom # value: # target_mass: np.where(fj_isQCD, fj_genjet_sdmass, fj_gen_mass) observers : - origIdx - idx - Part_E_tot - Part_PX_tot - Part_PY_tot - Part_PZ_tot - Part_P_tot - Part_Eta_tot - Part_Phi_tot # weights: ### [option 1] use precomputed weights stored in the input files # use_precomputed_weights: true # weight_branches: [weight, class_weight] ### [option 2] compute weights on-the-fly using reweighting histograms In the following two models (i.e., the DeepAK8 and the ParticleNet model) you will see that the data card is very similar. The change will only be the way we present the input group(s). DeepAK8 (1D CNN) The full architecture of the DeepAK8 model, which is based on 1D CNN with ResNet architecture. Note The DeepAK8 tagger is a widely used highly-boosted jet tagger in the CMS community. The design of the model can be found in the CMS paper [ arXiv:2004.08262 ]. The original model is trained on MXNet and its configuration can be found here . We now migrate the model architecture to Weaver and train it on PyTorch. Also, we narrow the multi-class output score to the binary output to adapt our binary classification task (top vs. QCD jet). The model card is given in top_tagging/networks/deepak8_pf.py . The DeepAK8 model is inspired by the ResNet architecture. The key ingredient is the ResNet unit constructed by multiple CNN layers with a shortcut connection. First, we define the ResNet unit in the model card. ResNet unit implementation See top_tagging/networks/deepak8_pf.py . We elaborate here on several aspects. A ResNet unit is made of two 1D CNNs with batch normalization and ReLU activation function. The shortcut is introduced here by directly adding the input data to the processed data after passing the CNN layers. The shortcut connection help to ease the training for the \"deeper\" model [ arXiv:1512.03385 ]. Note that a trivial linear transformation is applied ( self.conv_sc ) if the feature dimension of the input and output data does not match. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class ResNetUnit ( nn . Module ): r \"\"\"Parameters ---------- in_channels : int Number of channels in the input vectors. out_channels : int Number of channels in the output vectors. strides: tuple Strides of the two convolutional layers, in the form of (stride0, stride1) \"\"\" def __init__ ( self , in_channels , out_channels , strides = ( 1 , 1 ), ** kwargs ): super ( ResNetUnit , self ) . __init__ ( ** kwargs ) self . conv1 = nn . Conv1d ( in_channels , out_channels , kernel_size = 3 , stride = strides [ 0 ], padding = 1 ) self . bn1 = nn . BatchNorm1d ( out_channels ) self . conv2 = nn . Conv1d ( out_channels , out_channels , kernel_size = 3 , stride = strides [ 1 ], padding = 1 ) self . bn2 = nn . BatchNorm1d ( out_channels ) self . relu = nn . ReLU () self . dim_match = True if not in_channels == out_channels or not strides == ( 1 , 1 ): # dimensions not match self . dim_match = False self . conv_sc = nn . Conv1d ( in_channels , out_channels , kernel_size = 1 , stride = strides [ 0 ] * strides [ 1 ], bias = False ) def forward ( self , x ): identity = x x = self . conv1 ( x ) x = self . bn1 ( x ) x = self . relu ( x ) x = self . conv2 ( x ) x = self . bn2 ( x ) x = self . relu ( x ) # print('resnet unit', identity.shape, x.shape, self.dim_match) if self . dim_match : return identity + x else : return self . conv_sc ( identity ) + x With the ResNet unit, we construct the DeepAK8 model. The model hyperparameters are chosen as follows. conv_params = [( 32 ,), ( 64 , 64 ), ( 64 , 64 ), ( 128 , 128 )] fc_params = [( 512 , 0.2 )] DeepAK8 model implementation See top_tagging/networks/deepak8_pf.py . Note that the main architecture is a PyTorch re-implementation of the code here based on the MXNet. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class ResNet ( nn . Module ): r \"\"\"Parameters ---------- features_dims : int Input feature dimensions. num_classes : int Number of output classes. conv_params : list List of the convolution layer parameters. The first element is a tuple of size 1, defining the transformed feature size for the initial feature convolution layer. The following are tuples of feature size for multiple stages of the ResNet units. Each number defines an individual ResNet unit. fc_params: list List of fully connected layer parameters after all EdgeConv blocks, each element in the format of (n_feat, drop_rate) \"\"\" def __init__ ( self , features_dims , num_classes , conv_params = [( 32 ,), ( 64 , 64 ), ( 64 , 64 ), ( 128 , 128 )], fc_params = [( 512 , 0.2 )], ** kwargs ): super ( ResNet , self ) . __init__ ( ** kwargs ) self . conv_params = conv_params self . num_stages = len ( conv_params ) - 1 self . fts_conv = nn . Sequential ( nn . Conv1d ( in_channels = features_dims , out_channels = conv_params [ 0 ][ 0 ], kernel_size = 3 , stride = 1 , padding = 1 ), nn . BatchNorm1d ( conv_params [ 0 ][ 0 ]), nn . ReLU ()) # define ResNet units for each stage. Each unit is composed of a sequence of ResNetUnit block self . resnet_units = nn . ModuleDict () for i in range ( self . num_stages ): # stack units[i] layers in this stage unit_layers = [] for j in range ( len ( conv_params [ i + 1 ])): in_channels , out_channels = ( conv_params [ i ][ - 1 ], conv_params [ i + 1 ][ 0 ]) if j == 0 \\ else ( conv_params [ i + 1 ][ j - 1 ], conv_params [ i + 1 ][ j ]) strides = ( 2 , 1 ) if ( j == 0 and i > 0 ) else ( 1 , 1 ) unit_layers . append ( ResNetUnit ( in_channels , out_channels , strides )) self . resnet_units . add_module ( 'resnet_unit_ %d ' % i , nn . Sequential ( * unit_layers )) # define fully connected layers fcs = [] for idx , layer_param in enumerate ( fc_params ): channels , drop_rate = layer_param in_chn = conv_params [ - 1 ][ - 1 ] if idx == 0 else fc_params [ idx - 1 ][ 0 ] fcs . append ( nn . Sequential ( nn . Linear ( in_chn , channels ), nn . ReLU (), nn . Dropout ( drop_rate ))) fcs . append ( nn . Linear ( fc_params [ - 1 ][ 0 ], num_classes )) self . fc = nn . Sequential ( * fcs ) def forward ( self , x ): # x: the feature vector, (N, C, P) x = self . fts_conv ( x ) for i in range ( self . num_stages ): x = self . resnet_units [ 'resnet_unit_ %d ' % i ]( x ) # (N, C', P'), P'<P due to kernal_size>1 or stride>1 # global average pooling x = x . sum ( dim =- 1 ) / x . shape [ - 1 ] # (N, C') # fully connected x = self . fc ( x ) # (N, out_chn) return x def get_model ( data_config , ** kwargs ): conv_params = [( 32 ,), ( 64 , 64 ), ( 64 , 64 ), ( 128 , 128 )] fc_params = [( 512 , 0.2 )] pf_features_dims = len ( data_config . input_dicts [ 'pf_features' ]) num_classes = len ( data_config . label_value ) model = ResNet ( pf_features_dims , num_classes , conv_params = conv_params , fc_params = fc_params ) model_info = { 'input_names' : list ( data_config . input_names ), 'input_shapes' :{ k :(( 1 ,) + s [ 1 :]) for k , s in data_config . input_shapes . items ()}, 'output_names' :[ 'softmax' ], 'dynamic_axes' :{ ** { k :{ 0 : 'N' , 2 : 'n_' + k . split ( '_' )[ 0 ]} for k in data_config . input_names }, ** { 'softmax' :{ 0 : 'N' }}}, } print ( model , model_info ) print ( data_config . input_shapes ) return model , model_info def get_loss ( data_config , ** kwargs ): return torch . nn . CrossEntropyLoss () The output below shows the full structure of the DeepAK8 model based on 1D CNN with ResNet. It is printed by PyTorch and you will see it in the Weaver output during training. The full-scale structure of the DeepAK8 architecture ResNet( |0.349 M, 100.000% Params, 0.012 GMac, 100.000% MACs| (fts_conv): Sequential( |0.0 M, 0.137% Params, 0.0 GMac, 0.427% MACs| (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=(1,), |0.0 M, 0.119% Params, 0.0 GMac, 0.347% MACs|) (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.018% Params, 0.0 GMac, 0.053% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.027% MACs|) ) (resnet_units): ModuleDict( |0.282 M, 80.652% Params, 0.012 GMac, 99.010% MACs| (resnet_unit_0): Sequential( |0.046 M, 13.124% Params, 0.005 GMac, 38.409% MACs| (0): ResNetUnit( |0.021 M, 5.976% Params, 0.002 GMac, 17.497% MACs| (conv1): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.006 M, 1.778% Params, 0.001 GMac, 5.175% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 10.296% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.107% MACs|) (conv_sc): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False, |0.002 M, 0.587% Params, 0.0 GMac, 1.707% MACs|) ) (1): ResNetUnit( |0.025 M, 7.149% Params, 0.003 GMac, 20.912% MACs| (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 10.296% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 10.296% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.107% MACs|) ) ) (resnet_unit_1): Sequential( |0.054 M, 15.471% Params, 0.003 GMac, 22.619% MACs| (0): ResNetUnit( |0.029 M, 8.322% Params, 0.001 GMac, 12.163% MACs| (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) (conv_sc): Conv1d(64, 64, kernel_size=(1,), stride=(2,), bias=False, |0.004 M, 1.173% Params, 0.0 GMac, 1.707% MACs|) ) (1): ResNetUnit( |0.025 M, 7.149% Params, 0.001 GMac, 10.456% MACs| (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) ) ) (resnet_unit_2): Sequential( |0.182 M, 52.057% Params, 0.005 GMac, 37.982% MACs| (0): ResNetUnit( |0.083 M, 23.682% Params, 0.002 GMac, 17.284% MACs| (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), |0.025 M, 7.075% Params, 0.001 GMac, 5.148% MACs|) (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), |0.049 M, 14.114% Params, 0.001 GMac, 10.269% MACs|) (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) (conv_sc): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False, |0.008 M, 2.346% Params, 0.0 GMac, 1.707% MACs|) ) (1): ResNetUnit( |0.099 M, 28.375% Params, 0.002 GMac, 20.698% MACs| (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), |0.049 M, 14.114% Params, 0.001 GMac, 10.269% MACs|) (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), |0.049 M, 14.114% Params, 0.001 GMac, 10.269% MACs|) (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) ) ) ) (fc): Sequential( |0.067 M, 19.210% Params, 0.0 GMac, 0.563% MACs| (0): Sequential( |0.066 M, 18.917% Params, 0.0 GMac, 0.555% MACs| (0): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 18.917% Params, 0.0 GMac, 0.551% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.004% MACs|) (2): Dropout(p=0.2, inplace=False, |0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) ) (1): Linear(in_features=512, out_features=2, bias=True, |0.001 M, 0.294% Params, 0.0 GMac, 0.009% MACs|) ) ) The data card is the same as the MLP case, shown in top_tagging/data/pf_features.yaml . ParticleNet (DGCNN) The full architecture of the ParticleNet model, which is based on DGCNN and EdgeConv. Note The ParticleNet model applied to the CMS analysis is provided in weaver/networks/particle_net_pf_sv.py , and the data card in weaver/data/ak15_points_pf_sv.yaml . Here we use a similar configuration card to deal with the benchmark task. We will elaborate on the ParticleNet model and focus more on the technical side in this section. The model is defined in top_tagging/networks/particlenet_pf.py , but it imports some constructor, the EdgeConv block, in weaver/utils/nn/model/ParticleNet.py . The EdgeConv is illustrated in the cartoon. Illustration of the EdgeConv block From an EdgeConv block's point of view, it requires two classes of features as input: the \"coordinates\" and the \"features\". These features are the per point properties, in the 2D shape with dimensions (C, P) , where C is the size of the features (the feature size of \"coordinates\" and the \"features\" can be different, marked as C_pts , C_fts in the following code), and P is the number of points. The block outputs the new features that the model learns, also in the 2D shape with dimensions (C_fts_out, P) . What happens inside the EdgeConv block? And how is the output feature vector transferred from the input features using the topology of the point cloud? The answer is encoded in the edge convolution (EdgeConv). The edge convolution is an analogue convolution method defined on a point cloud, whose shape is given by the \"coordinates\" of points. Specifically, the input \"coordinates\" provide a view of spatial relations of the points in the Euclidean space. It determines the k -nearest neighbouring points for each point that will guide the update of the feature vector of a point. For each point, the updated feature vector is based on the current state of the point and its k neighbours. Guided by this spirit, all features of the point cloud forms a 3D vector with dimensions (C, P, K) , where C is the per-point feature size (e.g., \u03b7 , \u03c6 , p T \uff0c...), P is the number of points, and K the k -NN number. The structured vector is linearly transformed by acting 2D CNN on the feature dimension C . This helps to aggregate the feature information and exploit the correlations of each point with its adjacent points. A shortcut connection is also introduced inspired by the ResNet. Note The feature dimension C after exploring the k neighbours of each point actually doubles the value of the initial feature dimension. Here, a new set of features is constructed by subtracting the feature a point carries to the features its k neighbours carry (namely x i \u2013 x i_j for point i , and j =1,..., k ). This way, the correlation of each point with its neighbours are well captured. Below shows how the EdgeConv structure is implemented in the code. EdgeConv block implementation See weaver/utils/nn/model/ParticleNet.py , or the following code block annotated with more comments. We elaborate here on several aspects. The EdgeConvBlock takes the feature dimension in_feat , out_feats which are C_fts , C_fts_out we introduced above. The input data vectors to forward() are \"coordinates\" and \"features\" vector, in the dimension of (N, C_pts(C_fts), P) as introduced above. The first dimension is the mini-batch size. self.get_graph_feature() helps to aggregate k -nearest neighbours for each point. The resulting vector is in the dimension of (N, C_fts(0), P, K) as we discussed above, K being the k -NN number. Note that the C_fts(0) doubles the value of the original input feature dimension C_fts as mentioned above. After convolutions, the per-point features are merged by taking the mean of all k -nearest neighbouring vectors: fts = x . mean ( dim =- 1 ) # (N, C, P) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class EdgeConvBlock ( nn . Module ): r \"\"\"EdgeConv layer. Introduced in \"`Dynamic Graph CNN for Learning on Point Clouds <https://arxiv.org/pdf/1801.07829>`__\". Can be described as follows: .. math:: x_i^{(l+1)} = \\max_{j \\in \\mathcal{N}(i)} \\mathrm{ReLU}( \\Theta \\cdot (x_j^{(l)} - x_i^{(l)}) + \\Phi \\cdot x_i^{(l)}) where :math:`\\mathcal{N}(i)` is the neighbor of :math:`i`. Parameters ---------- in_feat : int Input feature size. out_feat : int Output feature size. batch_norm : bool Whether to include batch normalization on messages. \"\"\" def __init__ ( self , k , in_feat , out_feats , batch_norm = True , activation = True , cpu_mode = False ): super ( EdgeConvBlock , self ) . __init__ () self . k = k self . batch_norm = batch_norm self . activation = activation self . num_layers = len ( out_feats ) self . get_graph_feature = get_graph_feature_v2 if cpu_mode else get_graph_feature_v1 self . convs = nn . ModuleList () for i in range ( self . num_layers ): self . convs . append ( nn . Conv2d ( 2 * in_feat if i == 0 else out_feats [ i - 1 ], out_feats [ i ], kernel_size = 1 , bias = False if self . batch_norm else True )) if batch_norm : self . bns = nn . ModuleList () for i in range ( self . num_layers ): self . bns . append ( nn . BatchNorm2d ( out_feats [ i ])) if activation : self . acts = nn . ModuleList () for i in range ( self . num_layers ): self . acts . append ( nn . ReLU ()) if in_feat == out_feats [ - 1 ]: self . sc = None else : self . sc = nn . Conv1d ( in_feat , out_feats [ - 1 ], kernel_size = 1 , bias = False ) self . sc_bn = nn . BatchNorm1d ( out_feats [ - 1 ]) if activation : self . sc_act = nn . ReLU () def forward ( self , points , features ): # points: (N, C_pts, P) # features: (N, C_fts, P) # N: batch size, C: feature size per point, P: number of points topk_indices = knn ( points , self . k ) # (N, P, K) x = self . get_graph_feature ( features , self . k , topk_indices ) # (N, C_fts(0), P, K) for conv , bn , act in zip ( self . convs , self . bns , self . acts ): x = conv ( x ) # (N, C', P, K) if bn : x = bn ( x ) if act : x = act ( x ) fts = x . mean ( dim =- 1 ) # (N, C, P) # shortcut if self . sc : sc = self . sc ( features ) # (N, C_out, P) sc = self . sc_bn ( sc ) else : sc = features return self . sc_act ( sc + fts ) # (N, C_out, P) With the EdgeConv architecture as the building block, the ParticleNet model is constructed as follow. The ParticleNet model stacks three EdgeConv blocks to construct higher-level features and passing them through the pipeline. The points (i.e., in our case, the particle candidates inside a jet) are not changing, but the per-point \"coordinates\" and \"features\" vectors changes, in both values and dimensions. For the first EdgeConv block, the \"coordinates\" only include the relative \u03b7 and \u03c6 value of each particle. The \"features\" is a vector with a standard length of 32, which is linearly transformed from the initial feature vectors including the components of relative \u03b7 , \u03c6 , the log of p T , etc. The first EdgeConv block outputs a per-point feature vector of length 64, which is taken as both the \"coordinates\" and \"features\" to the next EdgeConv block. That is to say, the next k -NN is applied on the 64D high-dimensional spatial space to capture the new relations of points learned by the model. This is visualized by the input/output arrows showing the data flow of the model. We see that this architecture illustrates the stackability of the EdgeConv block, and is the core to the Dynamic Graph CNN (DGCNN), as the model can dynamically change the correlations of each point based on learnable features. A fusion technique is also used by concatenating the three EdgeConv output vectors together (adding the dimensions), instead of using the last EdgeConv output, to form an output vector. This is also one form of shortcut implementations that helps to ease the training for a complex and deep convolutional network model. The concatenated vectors per point are then averaged over points to produce a single 1D vector of the whole point cloud. The vector passes through one fully connected layer, with a dropout rate of p=0.1 to prevent overfitting. Then, in our example, the full network outputs two scores after a softmax, representing the one-hot encoding of the top vs. QCD class. The ParticleNet implementation is shown below. ParticleNet model implementation See weaver/utils/nn/model/ParticleNet.py , or the following code block annotated with more comments. We elaborate here on several mean points. The stack of multiple EdgeConv blocks are implemented in for idx , conv in enumerate ( self . edge_convs ): pts = ( points if idx == 0 else fts ) + coord_shift fts = conv ( pts , fts ) * mask The multiple EdgeConv layer parameters are given by conv_params , which takes a list of tuples, each tuple in the format of (K, (C1, C2, C3)) . K for the k -NN number, C1,2,3 for convolution feature sizes of three layers in an EdgeConv block. The fully connected layer parameters are given by fc_params , which takes a list of tuples, each tuple in the format of (n_feat, drop_rate) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class ParticleNet ( nn . Module ): r \"\"\"Parameters ---------- input_dims : int Input feature dimensions (C_fts). num_classes : int Number of output classes. conv_params : list List of convolution parameters of EdgeConv blocks, each element in the format of (K, (C1, C2, C3)). K for the kNN number, C1,2,3 for convolution feature sizes of three layers in an EdgeConv block. fc_params: list List of fully connected layer parameters after all EdgeConv blocks, each element in the format of (n_feat, drop_rate) use_fusion: bool If true, concatenates all output features from each EdgeConv before the fully connected layer. use_fts_bn: bool If true, applies a batch norm before feeding to the EdgeConv block. use_counts: bool If true, uses the real count of points instead of the padded size (the max point size). for_inference: bool Whether this is an inference routine. If true, applies a softmax to the output. for_segmentation: bool Whether the model is set up for the point cloud segmentation (instead of classification) task. If true, does not merge the features after the last EdgeConv, and apply Conv1D instead of the linear layer. The output is hence each output_features per point, instead of output_features. \"\"\" def __init__ ( self , input_dims , num_classes , conv_params = [( 7 , ( 32 , 32 , 32 )), ( 7 , ( 64 , 64 , 64 ))], fc_params = [( 128 , 0.1 )], use_fusion = True , use_fts_bn = True , use_counts = True , for_inference = False , for_segmentation = False , ** kwargs ): super ( ParticleNet , self ) . __init__ ( ** kwargs ) self . use_fts_bn = use_fts_bn if self . use_fts_bn : self . bn_fts = nn . BatchNorm1d ( input_dims ) self . use_counts = use_counts self . edge_convs = nn . ModuleList () for idx , layer_param in enumerate ( conv_params ): k , channels = layer_param in_feat = input_dims if idx == 0 else conv_params [ idx - 1 ][ 1 ][ - 1 ] self . edge_convs . append ( EdgeConvBlock ( k = k , in_feat = in_feat , out_feats = channels , cpu_mode = for_inference )) self . use_fusion = use_fusion if self . use_fusion : in_chn = sum ( x [ - 1 ] for _ , x in conv_params ) out_chn = np . clip (( in_chn // 128 ) * 128 , 128 , 1024 ) self . fusion_block = nn . Sequential ( nn . Conv1d ( in_chn , out_chn , kernel_size = 1 , bias = False ), nn . BatchNorm1d ( out_chn ), nn . ReLU ()) self . for_segmentation = for_segmentation fcs = [] for idx , layer_param in enumerate ( fc_params ): channels , drop_rate = layer_param if idx == 0 : in_chn = out_chn if self . use_fusion else conv_params [ - 1 ][ 1 ][ - 1 ] else : in_chn = fc_params [ idx - 1 ][ 0 ] if self . for_segmentation : fcs . append ( nn . Sequential ( nn . Conv1d ( in_chn , channels , kernel_size = 1 , bias = False ), nn . BatchNorm1d ( channels ), nn . ReLU (), nn . Dropout ( drop_rate ))) else : fcs . append ( nn . Sequential ( nn . Linear ( in_chn , channels ), nn . ReLU (), nn . Dropout ( drop_rate ))) if self . for_segmentation : fcs . append ( nn . Conv1d ( fc_params [ - 1 ][ 0 ], num_classes , kernel_size = 1 )) else : fcs . append ( nn . Linear ( fc_params [ - 1 ][ 0 ], num_classes )) self . fc = nn . Sequential ( * fcs ) self . for_inference = for_inference def forward ( self , points , features , mask = None ): # print('points:\\n', points) # print('features:\\n', features) if mask is None : mask = ( features . abs () . sum ( dim = 1 , keepdim = True ) != 0 ) # (N, 1, P) points *= mask features *= mask coord_shift = ( mask == 0 ) * 1e9 if self . use_counts : counts = mask . float () . sum ( dim =- 1 ) counts = torch . max ( counts , torch . ones_like ( counts )) # >=1 if self . use_fts_bn : fts = self . bn_fts ( features ) * mask else : fts = features outputs = [] for idx , conv in enumerate ( self . edge_convs ): pts = ( points if idx == 0 else fts ) + coord_shift fts = conv ( pts , fts ) * mask if self . use_fusion : outputs . append ( fts ) if self . use_fusion : fts = self . fusion_block ( torch . cat ( outputs , dim = 1 )) * mask # assert(((fts.abs().sum(dim=1, keepdim=True) != 0).float() - mask.float()).abs().sum().item() == 0) if self . for_segmentation : x = fts else : if self . use_counts : x = fts . sum ( dim =- 1 ) / counts # divide by the real counts else : x = fts . mean ( dim =- 1 ) output = self . fc ( x ) if self . for_inference : output = torch . softmax ( output , dim = 1 ) # print('output:\\n', output) return output Above are the capsulation of all ParticleNet building blocks. Eventually, we have the model defined in the model card top_tagging/networks/particlenet_pf.py , in the ParticleNetTagger1Path class, meaning we only use the ParticleNet pipeline that deals with one set of the point cloud (i.e., the particle candidates). Info Two sets of point clouds in the CMS application, namely the particle-flow candidates and secondary vertices, are used. This requires special handling to merge the clouds before feeding them to the first layer of EdgeConv. ParticleNet model config Also see top_tagging/networks/particlenet_pf.py . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 import torch import torch.nn as nn from utils.nn.model.ParticleNet import ParticleNet , FeatureConv class ParticleNetTagger1Path ( nn . Module ): def __init__ ( self , pf_features_dims , num_classes , conv_params = [( 7 , ( 32 , 32 , 32 )), ( 7 , ( 64 , 64 , 64 ))], fc_params = [( 128 , 0.1 )], use_fusion = True , use_fts_bn = True , use_counts = True , pf_input_dropout = None , for_inference = False , ** kwargs ): super ( ParticleNetTagger1Path , self ) . __init__ ( ** kwargs ) self . pf_input_dropout = nn . Dropout ( pf_input_dropout ) if pf_input_dropout else None self . pf_conv = FeatureConv ( pf_features_dims , 32 ) self . pn = ParticleNet ( input_dims = 32 , num_classes = num_classes , conv_params = conv_params , fc_params = fc_params , use_fusion = use_fusion , use_fts_bn = use_fts_bn , use_counts = use_counts , for_inference = for_inference ) def forward ( self , pf_points , pf_features , pf_mask ): if self . pf_input_dropout : pf_mask = ( self . pf_input_dropout ( pf_mask ) != 0 ) . float () pf_points *= pf_mask pf_features *= pf_mask return self . pn ( pf_points , self . pf_conv ( pf_features * pf_mask ) * pf_mask , pf_mask ) def get_model ( data_config , ** kwargs ): conv_params = [ ( 16 , ( 64 , 64 , 64 )), ( 16 , ( 128 , 128 , 128 )), ( 16 , ( 256 , 256 , 256 )), ] fc_params = [( 256 , 0.1 )] use_fusion = True pf_features_dims = len ( data_config . input_dicts [ 'pf_features' ]) num_classes = len ( data_config . label_value ) model = ParticleNetTagger1Path ( pf_features_dims , num_classes , conv_params , fc_params , use_fusion = use_fusion , use_fts_bn = kwargs . get ( 'use_fts_bn' , False ), use_counts = kwargs . get ( 'use_counts' , True ), pf_input_dropout = kwargs . get ( 'pf_input_dropout' , None ), for_inference = kwargs . get ( 'for_inference' , False ) ) model_info = { 'input_names' : list ( data_config . input_names ), 'input_shapes' :{ k :(( 1 ,) + s [ 1 :]) for k , s in data_config . input_shapes . items ()}, 'output_names' :[ 'softmax' ], 'dynamic_axes' :{ ** { k :{ 0 : 'N' , 2 : 'n_' + k . split ( '_' )[ 0 ]} for k in data_config . input_names }, ** { 'softmax' :{ 0 : 'N' }}}, } print ( model , model_info ) print ( data_config . input_shapes ) return model , model_info def get_loss ( data_config , ** kwargs ): return torch . nn . CrossEntropyLoss () The most important parameters are conv_params and fc_params , which decides the model parameters of EdgeConv blocks and the fully connected layer. See details in the above \"ParticleNet model implementation\" box. conv_params = [ ( 16 , ( 64 , 64 , 64 )), ( 16 , ( 128 , 128 , 128 )), ( 16 , ( 256 , 256 , 256 )), ] fc_params = [( 256 , 0.1 )] A full structure printed from PyTorch is shown below. It will appear in the Weaver output during training. ParticleNet full-scale structure ParticleNetTagger1Path( |0.577 M, 100.000% Params, 0.441 GMac, 100.000% MACs| (pf_conv): FeatureConv( |0.0 M, 0.035% Params, 0.0 GMac, 0.005% MACs| (conv): Sequential( |0.0 M, 0.035% Params, 0.0 GMac, 0.005% MACs| (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.001% Params, 0.0 GMac, 0.000% MACs|) (1): Conv1d(4, 32, kernel_size=(1,), stride=(1,), bias=False, |0.0 M, 0.022% Params, 0.0 GMac, 0.003% MACs|) (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.011% Params, 0.0 GMac, 0.001% MACs|) (3): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs|) ) ) (pn): ParticleNet( |0.577 M, 99.965% Params, 0.441 GMac, 99.995% MACs| (edge_convs): ModuleList( |0.305 M, 52.823% Params, 0.424 GMac, 96.047% MACs| (0): EdgeConvBlock( |0.015 M, 2.575% Params, 0.021 GMac, 4.716% MACs| (convs): ModuleList( |0.012 M, 2.131% Params, 0.02 GMac, 4.456% MACs| (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.004 M, 0.710% Params, 0.007 GMac, 1.485% MACs|) (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.004 M, 0.710% Params, 0.007 GMac, 1.485% MACs|) (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.004 M, 0.710% Params, 0.007 GMac, 1.485% MACs|) ) (bns): ModuleList( |0.0 M, 0.067% Params, 0.001 GMac, 0.139% MACs| (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.046% MACs|) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.046% MACs|) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.046% MACs|) ) (acts): ModuleList( |0.0 M, 0.000% Params, 0.0 GMac, 0.070% MACs| (0): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs|) ) (sc): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False, |0.002 M, 0.355% Params, 0.0 GMac, 0.046% MACs|) (sc_bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.003% MACs|) (sc_act): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs|) ) (1): EdgeConvBlock( |0.058 M, 10.121% Params, 0.081 GMac, 18.437% MACs| (convs): ModuleList( |0.049 M, 8.523% Params, 0.079 GMac, 17.825% MACs| (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.016 M, 2.841% Params, 0.026 GMac, 5.942% MACs|) (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.016 M, 2.841% Params, 0.026 GMac, 5.942% MACs|) (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.016 M, 2.841% Params, 0.026 GMac, 5.942% MACs|) ) (bns): ModuleList( |0.001 M, 0.133% Params, 0.001 GMac, 0.279% MACs| (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.093% MACs|) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.093% MACs|) (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.093% MACs|) ) (acts): ModuleList( |0.0 M, 0.000% Params, 0.001 GMac, 0.139% MACs| (0): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.046% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.046% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.046% MACs|) ) (sc): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False, |0.008 M, 1.420% Params, 0.001 GMac, 0.186% MACs|) (sc_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.006% MACs|) (sc_act): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.003% MACs|) ) (2): EdgeConvBlock( |0.231 M, 40.128% Params, 0.322 GMac, 72.894% MACs| (convs): ModuleList( |0.197 M, 34.091% Params, 0.315 GMac, 71.299% MACs| (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.066 M, 11.364% Params, 0.105 GMac, 23.766% MACs|) (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.066 M, 11.364% Params, 0.105 GMac, 23.766% MACs|) (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.066 M, 11.364% Params, 0.105 GMac, 23.766% MACs|) ) (bns): ModuleList( |0.002 M, 0.266% Params, 0.002 GMac, 0.557% MACs| (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.001 GMac, 0.186% MACs|) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.001 GMac, 0.186% MACs|) (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.001 GMac, 0.186% MACs|) ) (acts): ModuleList( |0.0 M, 0.000% Params, 0.001 GMac, 0.279% MACs| (0): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.093% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.093% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.093% MACs|) ) (sc): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False, |0.033 M, 5.682% Params, 0.003 GMac, 0.743% MACs|) (sc_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.0 GMac, 0.012% MACs|) (sc_act): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.006% MACs|) ) ) (fusion_block): Sequential( |0.173 M, 29.963% Params, 0.017 GMac, 3.925% MACs| (0): Conv1d(448, 384, kernel_size=(1,), stride=(1,), bias=False, |0.172 M, 29.830% Params, 0.017 GMac, 3.899% MACs|) (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.133% Params, 0.0 GMac, 0.017% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.009% MACs|) ) (fc): Sequential( |0.099 M, 17.179% Params, 0.0 GMac, 0.023% MACs| (0): Sequential( |0.099 M, 17.090% Params, 0.0 GMac, 0.022% MACs| (0): Linear(in_features=384, out_features=256, bias=True, |0.099 M, 17.090% Params, 0.0 GMac, 0.022% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) (2): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) ) (1): Linear(in_features=256, out_features=2, bias=True, |0.001 M, 0.089% Params, 0.0 GMac, 0.000% MACs|) ) ) ) The data card is shown in top_tagging/data/pf_points_features.yaml , given in a similar way as in the MLP example. Here we group the inputs into three classes: pf_points , pf_features and pf_masks . They correspond to the forward(self, pf_points, pf_features, pf_mask) prototype of our nn.Module model, and will send in these 2D vectors in the mini-batch size for each iteration during training/prediction. ParticleNet data config top_tagging/data/pf_points_features.yaml See top_tagging/data/pf_points_features.yaml . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 selection : ### use `&`, `|`, `~` for logical operations on numpy arrays ### can use functions from `math`, `np` (numpy), and `awkward` in the expression new_variables : ### [format] name: formula ### can use functions from `math`, `np` (numpy), and `awkward` in the expression pf_mask : awkward.JaggedArray.ones_like(Part_E) is_bkg : np.logical_not(is_signal_new) preprocess : ### method: [manual, auto] - whether to use manually specified parameters for variable standardization method : manual ### data_fraction: fraction of events to use when calculating the mean/scale for the standardization data_fraction : inputs : pf_points : length : 100 vars : - Part_Etarel - Part_Phirel pf_features : length : 100 vars : ### [format 1]: var_name (no transformation) ### [format 2]: [var_name, ### subtract_by(optional, default=None, no transf. if preprocess.method=manual, auto transf. if preprocess.method=auto), ### multiply_by(optional, default=1), ### clip_min(optional, default=-5), ### clip_max(optional, default=5), ### pad_value(optional, default=0)] - Part_Etarel - Part_Phirel - [ Part_E_log , 2 , 1 ] - [ Part_P_log , 2 , 1 ] pf_mask : length : 100 vars : - pf_mask labels : ### type can be `simple`, `custom` ### [option 1] use `simple` for binary/multi-class classification, then `value` is a list of 0-1 labels type : simple value : [ is_signal_new , is_bkg ] ### [option 2] otherwise use `custom` to define the label, then `value` is a map # type: custom # value: # target_mass: np.where(fj_isQCD, fj_genjet_sdmass, fj_gen_mass) observers : - origIdx - idx - Part_E_tot - Part_PX_tot - Part_PY_tot - Part_PZ_tot - Part_P_tot - Part_Eta_tot - Part_Phi_tot # weights: ### [option 1] use precomputed weights stored in the input files # use_precomputed_weights: true # weight_branches: [weight, class_weight] ### [option 2] compute weights on-the-fly using reweighting histograms Now we have walked through the detailed description of three networks in their architecture as well as their implementations in Weaver . Before ending this section, we summarize the three networks on their (1) model and data configuration cards, (2) the number of parameters, and (3) computational complexity in the following table. Note that we'll refer to the shell variables provided here in the following training example. Model ${PREFIX} ${MODEL_CONFIG} ${DATA_CONFIG} Parameters Computational complexity MLP mlp mlp_pf.py pf_features.yaml 739k 0.001 GMac DeepAK8 (1D CNN) deepak8 deepak8_pf.py pf_features.yaml 349k 0.012 GMac ParticleNet (DGCNN) particlenet particlenet_pf.py pf_points_features.yaml 577k 0.441 GMac","title":"1. Build models in Weaver"},{"location":"inference/particlenet.html#2-start-training","text":"Now we train the three neural networks based on the provided model and data configurations. Here we present three ways of training. For readers who have a local machine with CUDA GPUs, please try out training on the local GPUs. Readers who would like to try on CPUs can also refer to the local GPU instruction. It is also possible to borrow the GPU resources from the lxplus HTCondor or CMS Connect. Please find in the following that meets your situation. Train on local GPUs The three networks can be trained with a universal script. Enter the weaver base folder and run the following command. Note that ${DATA_CONFIG} , ${MODEL_CONFIG} , and ${PREFIX} refers to the value in the above table for each example, and the fake path should be replaced with the correct one. PREFIX = '<prefix-from-table>' MODEL_CONFIG = '<model-config-from-table>' DATA_CONFIG = '<data-config-from-table>' PATH_TO_SAMPLES = '<your-path-to-samples>' python train . py \\ -- data - train $ { PATH_TO_SAMPLES } '/prep/top_train_*.root' \\ -- data - val $ { PATH_TO_SAMPLES } '/prep/top_val_*.root' \\ -- fetch - by - file -- fetch - step 1 -- num - workers 3 \\ -- data - config top_tagging / data / $ { DATA_CONFIG } \\ -- network - config top_tagging / networks / $ { MODEL_CONFIG } \\ -- model - prefix output / $ { PREFIX } \\ -- gpus 0 , 1 -- batch - size 1024 -- start - lr 5e-3 -- num - epochs 20 -- optimizer ranger \\ -- log output / $ { PREFIX } . train . log Here --gpus 0,1 specifies the GPUs to run with the device ID 1 and 2. For training on CPUs, please use --gpu '' . A detailed description of the training command can be found in Weaver README . Below we will note a few more caveats about the data loading options, though the specific settings will depend on the specifics of the input data. Caveats on the data loading options Our goal in data loading is to guarantee that the data loaded in every mini-batch is evenly distributed with different labels, though they are not necessarily stored evenly in the file. Besides, we also need to ensure that the on-the-fly loading and preprocessing of data should be smooth and not be a bottleneck of the data delivering pipeline. The total amount of loaded data also needs to be controlled so as not to explode the entire memory. The following guidelines should be used to choose the best options for your use case: in the default case, data are loaded from every input file with a small proportion per fetch-step, provided by --fetch-step (default is 0.01). This adapts to the case when we have multiple classes of input, each class having multiple files (e.g., it adapts to the real CMS application because we may have multiple nano_i.root files for different input classes). The strategy gathered all pieces per fetch-step from all input files, shuffle them, and present the data we need in each regular mini-batch. One can also append --num-workers n with n being the number of paralleled workers to load the data. --fetch-step 1 --num-workers 1 . This strategy helps in the case we have few input files with data in different labels not evenly distributed. In the extreme case, we only have 1 file, with all data at the top being one class (signal) and data at the bottom being another class (background), or we have 2 or multiple files, each containing a specific class. In this option, --fetch-step 1 guarantees the entire data in the file is loaded and participate in the shuffle. Therefore all classes are safely mixed before sending to the mini-batch. --num-workers 1 means we only use one worker that takes care of all files to avoid inconsistent loading speeds of multiple workers (depending on CPUs). This strategy can further cooperate with --in-memory so that all data are put permanently in memory and will not be reloaded every epoch. --fetch-by-file is the option we can use when all input files have a similar structure. See Weaver README : An alternative approach is the \"file-based\" strategy, which can be enabled with --fetch-by-files . This approach will instead read all events from every file for each step, and it will read m input files ( m is set by --fetch-step ) before mixing and shuffling the loaded events. This strategy is more suitable when each input file is already a mixture of all types of events (e.g., pre-processed with NNTools), otherwise it may lead to suboptimal training performance. However, a higher data loading speed can generally be achieved with this approach. Please note that you can test if all data classes are well mixed by printing the truth label in each mini-batch. Also, remember to test if data are loaded just-in-time by monitoring the GPU performance \u2014 if switching the data loading strategy helps improve the GPU efficiency, it means the previous data loader is the bottleneck in the pipeline to deliver and use the data. After training, we predict the score on the test datasets using the best model: PREFIX = '<prefix-from-table>' MODEL_CONFIG = '<model-config-from-table>' DATA_CONFIG = '<data-config-from-table>' PATH_TO_SAMPLES = '<your-path-to-samples>' python train . py -- predict \\ -- data - test $ { PATH_TO_SAMPLES } '/prep/top_test_*.root' \\ -- num - workers 3 \\ -- data - config top_tagging / data / $ { DATA_CONFIG } \\ -- network - config top_tagging / networks / $ { MODEL_CONFIG } \\ -- model - prefix output / $ { PREFIX } _best_epoch_state . pt \\ -- gpus 0 , 1 -- batch - size 1024 \\ -- predict - output output / $ { PREFIX } _predict . root Use GPUs on lxplus HTCondor On lxplus HTCondor, the GPU(s) can be booked via the arguments request_gpus . To get familiar with the GPU service, please refer to the documentation here . While it is not possible to test the script locally, you can try out the condor_ssh_to_job command to connect to the remote condor machine that runs the jobs. This interesting feature will help you with debugging or monitoring the condor job. Here we provide the example executed script and the condor submitted file for the training and predicting task. Create the following two files: The executable: run.sh Still, please remember to specify ${DATA_CONFIG} , ${MODEL_CONFIG} , and ${PREFIX} as shown in the above table, and replace the fake path with the correct one. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 #!/bin/bash PREFIX = $1 MODEL_CONFIG = $2 DATA_CONFIG = $3 PATH_TO_SAMPLES = $4 WORKDIR = ` pwd ` # Download miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda_install.sh bash miniconda_install.sh -b -p ${ WORKDIR } /miniconda export PATH = $WORKDIR /miniconda/bin: $PATH pip install numpy pandas scikit-learn scipy matplotlib tqdm PyYAML pip install uproot3 awkward0 lz4 xxhash pip install tables pip install onnxruntime-gpu pip install tensorboard pip install torch # CUDA environment setup export PATH = $PATH :/usr/local/cuda-10.2/bin export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/usr/local/cuda-10.2/lib64 export LIBRARY_PATH = $LIBRARY_PATH :/usr/local/cuda-10.2/lib64 # Clone weaver-benchmark git clone --recursive https://github.com/colizz/weaver-benchmark.git ln -s ../top_tagging weaver-benchmark/weaver/top_tagging cd weaver-benchmark/weaver/ mkdir output # Training, using 1 GPU python train.py \\ --data-train ${ PATH_TO_SAMPLES } '/prep/top_train_*.root' \\ --data-val ${ PATH_TO_SAMPLES } '/prep/top_val_*.root' \\ --fetch-by-file --fetch-step 1 --num-workers 3 \\ --data-config top_tagging/data/ ${ DATA_CONFIG } \\ --network-config top_tagging/networks/ ${ MODEL_CONFIG } \\ --model-prefix output/ ${ PREFIX } \\ --gpus 0 --batch-size 1024 --start-lr 5e-3 --num-epochs 20 --optimizer ranger \\ --log output/ ${ PREFIX } .train.log # Predicting score, using 1 GPU python train.py --predict \\ --data-test ${ PATH_TO_SAMPLES } '/prep/top_test_*.root' \\ --num-workers 3 \\ --data-config top_tagging/data/ ${ DATA_CONFIG } \\ --network-config top_tagging/networks/ ${ MODEL_CONFIG } \\ --model-prefix output/ ${ PREFIX } _best_epoch_state.pt \\ --gpus 0 --batch-size 1024 \\ --predict-output output/ ${ PREFIX } _predict.root [ -d \"runs/\" ] && tar -caf output.tar output/ runs/ || tar -caf output.tar output/ HTCondor submitted file: submit.sub Modify the argument line. These are the bash variable PREFIX , MODEL_CONFIG , DATA_CONFIG , PATH_TO_SAMPLES used in the Weaver command. Since the EOS directory is accessable accross all condor nodes on lxplus, one may directly specify <your-path-to-samples> as the EOS path provided above. An example is shown in the commented line. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Universe = vanilla executable = run.sh arguments = <prefix> <model-config> <data-config> <your-path-to-samples> #arguments = mlp mlp_pf.py pf_features.yaml /eos/user/c/coli/public/weaver-benchmark/top_tagging/samples output = job. $( ClusterId ) . $( ProcId ) .out error = job. $( ClusterId ) . $( ProcId ) .err log = job. $( ClusterId ) .log should_transfer_files = YES when_to_transfer_output = ON_EXIT_OR_EVICT transfer_output_files = weaver-benchmark/weaver/output.tar transfer_output_remaps = \"output.tar = output. $( ClusterId ) . $( ProcId ) .tar\" request_GPUs = 1 request_CPUs = 4 +MaxRuntime = 604800 queue Make the run.sh script an executable, then submit the job. chmod +x run.sh condor_submit submit.sub A tarball will be transfered back with the weaver/output directory where the trained models and the predicted ROOT file are stored. Use GPUs on CMS Connect CMS Connect provides several GPU nodes. One can request to run GPU condor jobs in a similar way as on lxplus, please refer to the link: https://ci-connect.atlassian.net/wiki/spaces/CMS/pages/80117822/Requesting+GPUs As the EOS user space may not be accessed from the remote node launched by CMS Connect, one may consider either (1) migrating the input files by condor, or (2) using XRootD to transfer the input file from EOS space to the condor node, before running the Weaver train command.","title":"2. Start training!"},{"location":"inference/particlenet.html#3-evaluation-of-models","text":"In the output folder, we find the trained PyTorch models after every epoch and the log file that records the loss and accuracy in the runtime. The predict step also produces a predicted root file in the output folder, including the truth label, the predicted store, and several observer variables we provided in the data card. With the predicted root file, we make the ROC curve comparing the performance of the three trained models. Here is the result from my training: Model AUC Accuracy 1/ e B (@ e S =0.3) MLP 0.961 0.898 186 DeepAK8 (1D CNN) 0.979 0.927 585 ParticleNet (DGCNN) 0.984 0.936 1030 We see that the ParticleNet model shows an outstanding performance in this classification task. Besides, the DeepAK8 and ParticleNet results are similar to the benchmark values found in the gDoc . We address that the performance can be further improved by some following tricks: Train an ensemble of models with different initial parametrization. For each event/jet, take the final predicted score as the mean/median of the score ensembles predicted by each model. This is a widely used ML technique to pursue an extra few percent of improvements. Use more input variables for training. We note that in the above training example, only four input variables are used instead of a full suite of input features as done in the ParticleNet paper [ arXiv:1902.08570 ]. Additional variables (e.g. \u0394R or log( p T / p T (jet))) can be designed based on the given 4-momenta, and, although providing redundant information in principle, can still help the network fully exploit the point cloud structure and thus do a better discrimination job. The fine-tuning of the model will also bring some performance gain. See details in the next section.","title":"3. Evaluation of models"},{"location":"inference/particlenet.html#tuning-the-particlenet-model","text":"When it comes to the real application of any DNN model, tunning the hyperparameters is an important path towards a better performance. In this section, we provide some tips on the ParticleNet model tunning. For a more detailed discussion on this topic, see more in the \"validation\" chapter in the documentation.","title":"Tuning the ParticleNet model"},{"location":"inference/particlenet.html#1-choices-on-the-optimizer-and-the-learning-rate","text":"The optimizer decides how our neural network update all its parameters, and the learning rate means how fast the parameters changes in one training iteration. Learning rate is the most important hyperparameter to choose from before concrete training is done. Here we quote from a suggested strategy: if you only have the opportunity to optimize one hyperparameter, choose the learning rate. The optimizer is also important because a wiser strategy usually means avoid the zig-zagging updating route, avoid falling into the local minima and even adapting different strategies for the fast-changing parameters and the slow ones. Adam (and its several variations) is a widely used optimizer. Another recently developed advanced optimizer is Ranger that combines RAdam and LookAhead. However, one should note that the few percent level improvement by using different optimizers is likely to be smeared by an unoptimized learning rate. The above training scheme uses a start learning rate of 5e-3, and Ranger as the optimizer. It uses a flat+decay schedular, in a way that the LR starts to decay after processing 70% of epochs, and gradually reduce to 0.01 of its original value when nearing the completion of all epochs. First, we note that the current case is already well optimized. Therefore, by simply reuse the current choice, the training will converge to a stable result in general. But it is always good in practice to test several choices of the optimizer and reoptimize the learning rate. Weaver integrates multiple optimizers. In the above training command, we use --optimizer ranger to adopt the Ranger optimizer. It is also possible to switch to --optimizer adam or --optimizer adamW . Weaver also provides the interface to optimize the learning rate before real training is performed. In the ParticleNet model training, we append --lr-finder 5e-6,5e0,200 in the command, then a specific learning-rate finder program will be launched. This setup scans over the LR from 5e-6 to 5e0 by applying 200 mini-batches of training. It outputs a plot showing the training loss for different starting learning rates. In general, a lower training loss means a better choice of the learning rate parameter. Below shows the results from LR finder by specifying --lr-finder 5e-6,5e0,200 , for the --optimizer adamW (left) and the --optimizer ranger (right) case. The training loss forms a basin shape which indicates that the optimal learning rate falls somewhere in the middle. We extract two aspects from the plots. First, the basin covers a wide range, meaning that the LR finder only provides a rough estimation. But it is a good attempt to first run the LR finder to have an overall feeling. For the Ranger case (right figure), one can choose the range 1e-3 to 1e-2 and further determine the optminal learning rate by delivering the full training. Second, we should be aware that different optimizer takes different optimal LR values. As can be seen here, the AdamW in general requires a small LR than Ranger.","title":"1. Choices on the optimizer and the learning rate"},{"location":"inference/particlenet.html#2-visualize-the-training-with-tensorboard","text":"To monitor the full training/evaluation accuracy and the loss for each mini-batch, we can draw support from a nicely integrated utility, TensorBoard, to employ real-time monitoring. See the introduction page from PyTorch: https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html To activate TensorBoard, append (note that replace ${PREFIX} according to the above table) --tensorboard ${ PREFIX } to the training command. The runs/ subfolder containing the TensorBoard monitoring log will appear in the Weaver directory (if you are launching condor jobs, the runs/ folder will be transferred back in the tarball). Then, one can run tensorboard --logdir = runs to start the TensorBoard service and go to URL https://localhost:6006 to view the TensorBoard dashboard. The below plots show the training and evaluation loss, in our standard choice with LR being 5e-3, and in the case of a small LR 2e-3 and a large LR 1e-2. Note that all tested LR values are within the basin in the LR finder plots. We see that in the evaluated loss plot, the standard LR outperforms two variational choices. The reason may be that a larger LR finds difficulty in converging to the global minima, while a smaller LR may not be adequate to reach the minima point in a journey of 20 epochs. Overall, we see 5e-3 as a good choice as the starting LR for the Ranger optimizer.","title":"2. Visualize the training with TensorBoard"},{"location":"inference/particlenet.html#3-optimize-the-model","text":"In practice, tuning the model size is also an important task. By concept, a smaller model tends to have unsatisfactory performance due to the limited ability to learn many local features. As the model size goes up, the performance will climb to some extent, but may further decrease due to the network \"degradation\" (deeper models have difficulty learning features). Besides, a heavier model may also cause the overfitting issue. In practice, it also leads to larger inference time which is the main concern when coming to real applications. For the ParticleNet model case, we also test between a smaller and larger variation of the model size. Recall that the original model is defined by the following layer parameters. conv_params = [ ( 16 , ( 64 , 64 , 64 )), ( 16 , ( 128 , 128 , 128 )), ( 16 , ( 256 , 256 , 256 )), ] fc_params = [( 256 , 0.1 )] We can replace the code block with ec_k = kwargs . get ( 'ec_k' , 16 ) ec_c1 = kwargs . get ( 'ec_c1' , 64 ) ec_c2 = kwargs . get ( 'ec_c2' , 128 ) ec_c3 = kwargs . get ( 'ec_c3' , 256 ) fc_c , fc_p = kwargs . get ( 'fc_c' , 256 ), kwargs . get ( 'fc_p' , 0.1 ) conv_params = [ ( ec_k , ( ec_c1 , ec_c1 , ec_c1 )), ( ec_k , ( ec_c2 , ec_c2 , ec_c2 )), ( ec_k , ( ec_c3 , ec_c3 , ec_c3 )), ] fc_params = [( fc_c , fc_p )] Then we have the ability to tune the model parameters from the command line. Append the extra arguments in the training command --network-option ec_k 32 --network-option ec_c1 128 --network-option ec_c2 192 --network-option ec_c3 256 and the model parameters will take the new values as specified. We test over two cases, one with the above setting to enlarge the model, and another by using --network-option ec_c1 64 --network-option ec_c2 64 --network-option ec_c3 96 to adopt a lite version. The Tensorboard monitoring plots in the training/evaluation loss is shown as follows. We see that the \"heavy\" model reaches even smaller training loss, meaning that the model does not meet the degradation issue yet. However, the evaluation loss is not catching up with the training loss, showing some degree of overtraining in this scheme. From the evaluation result, we see no improvement by moving to a heavy model.","title":"3. Optimize the model"},{"location":"inference/particlenet.html#4-apply-preselection-and-class-weights","text":"In HEP applications, it is sometimes required to train a multi-class classifier. While it is simple to specify the input classes in the label section of the Weaver data config, it is sometimes ignored to set up the preselection and assign the suitable class weights for training. Using an unoptimized configuration, the trained model will not reach the best performance although no error message will result. Since our top tagging example is a binary classification problem, there is no specific need to configure the preselection and class weights. Below we summarize some experiences that may be applicable in reader's custom multi-class training task. The preselection should be chosen in a way that all remaining events passing the selection should fall into one and only one category. In other words, events with no labels attached should not be kept since it will confuse the training process. Class weights (the class_weights option under weights in the data config) control the relative importance of input sample categories for training. Implementation-wise, it changes the event probability in a specific category chosen as training input events. The class weight comes into effect when one trains a multi-class classifier. Take 3-class case (denoted as [A, B, C]) as an example, the class_weights: [1, 1, 1] gives equal weights to all categories. Retraining the input with class_weights: [10, 1, 1] may result in a better discriminating power for class A vs. B or A vs. C; while the power of B separating with C will be weakened. As a trade-off between separating A vs. C and B vs. C, the class weights need to be intentionally tuned to achieve reasonable performance. After the class weights are tuned, one can use another method to further factor out the interplay across categories, i.e., to define a \"binarized\" score between two classes only. Suppose the raw score for the three classes are P (A), P (B), and P (C) (their sum should be 1), then one can define the discriminant P (BvsC) = P (B) / ( P (B)+ P (C)) to separate B vs. C. In this way, the saparating power of B vs. C will remain unchanged for class_weights configured as either [1, 1, 1] or [10, 1, 1] . This strategy has been widely used in CMS to define composite tagger discrimant which are applied analysis-wise. Above, we discuss in a very detailed manner on various attempts we can make to optimize the model. We hope the practical experiences presented here will help readers develop and deploy the complex ML model.","title":"4. Apply preselection and class weights"},{"location":"inference/performance.html","text":"Performance of inference tools \u00b6","title":"Performance"},{"location":"inference/performance.html#performance-of-inference-tools","text":"","title":"Performance of inference tools"},{"location":"inference/pyg.html","text":"PyTorch Geometric \u00b6 Geometric deep learning (GDL) is an emerging field focused on applying machine learning (ML) techniques to non-Euclidean domains such as graphs, point clouds, and manifolds. The PyTorch Geometric (PyG) library extends PyTorch to include GDL functionality, for example classes necessary to handle data with irregular structure. PyG is introduced at a high level in Fast Graph Representation Learning with PyTorch Geometric and in detail in the PyG docs . GDL with PyG \u00b6 A complete reveiw of GDL is available in the following recently-published (and freely-available) textbook: Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges . The authors specify several key GDL architectures including convolutional neural networks (CNNs) operating on grids, Deep Sets architectures operating on sets, and graph neural networks (GNNs) operating on graphs, collections of nodes connected by edges. PyG is focused in particular on graph-structured data, which naturally encompases set-structured data. In fact, many state-of-the-art GNN architectures are implemented in PyG (see the docs )! A review of the landscape of GNN architectures is available in Graph Neural Networks: A Review of Methods and Applications . The Data Class: PyG Graphs \u00b6 Graphs are data structures designed to encode data structured as a set of objects and relations. Objects are embedded as graph nodes \\(u\\in\\mathcal{V}\\) , where \\(\\mathcal{V}\\) is the node set. Relations are represented by edges \\((i,j)\\in\\mathcal{E}\\) between nodes, where \\(\\mathcal{E}\\) is the edge set. Denote the sizes of the node and edge sets as \\(|\\mathcal{V}|=n_\\mathrm{nodes}\\) and \\(|\\mathcal{E}|=n_\\mathrm{edges}\\) respectively. The choice of edge connectivity determines the local structure of a graph, which has important downstream effects on graph-based learning algorithms. Graph construction is the process of embedding input data onto a graph structure. Graph-based learning algorithms are correspondingly imbued with a relational inductive bias based on the choice of graph representation; a graph's edge connectivity defines its local structure. The simplest graph construction routine is to construct no edges, yielding a permutation invariant set of objects. On the other hand, fully-connected graphs connect every node-node pair with an edge, yielding \\(n_\\mathrm{edges}=n_\\mathrm{nodes}(n_\\mathrm{nodes}-1)/2\\) edges. This representation may be feasible for small inputs like particle clouds corresponding to a jet, but is intractible for large-scale applications such as high-pileup tracking datasets. Notably, dynamic graph construction techniques operate on input point clouds, constructing edges on them dynamically during inference. For example, EdgeConv and GravNet GNN layers dynamically construct edges between nodes projected into a latent space; multiple such layers may be applied in sequence, yielding many intermediate graph representations on an input point cloud. In general, nodes can have positions \\(\\{p_i\\}_{i=1}^{n_\\mathrm{nodes}}\\) , \\(p_i\\in\\mathbb{R}^{n_\\mathrm{space\\_dim}}\\) , and features (attributes) \\(\\{x_i\\}_{i=1}^{n_\\mathrm{nodes}}\\) , \\(x_i\\in\\mathbb{R}^{n_\\mathrm{node\\_dim}}\\) . In some applications like GNN-based particle tracking, node positions are taken to be the features. In others, e.g. jet identification, positional information may be used to seed dynamic graph consturction while kinematic features are propagated as edge features. Edges, too, can have features \\(\\{e_{ij}\\}_{(i,j)\\in\\mathcal{E}}\\) , \\(e_{ij}\\in\\mathbb{R}^{n_\\mathrm{edge\\_dim}}\\) , but do not have positions; instead, edges are defined by the nodes they connect, and may therefore be represented by, for example, the distance between the respective node-node pair. In PyG, graphs are stored as instances of the data class, whose fields fully specify the graph: data.x : node feature matrix, \\(X\\in\\mathbb{R}^{n_\\mathrm{nodes}\\times n_\\mathrm{node\\_dim}}\\) data.edge_index : node indices at each end of each edge, \\(I\\in\\mathbb{R}^{2\\times n_\\mathrm{edges}}\\) data.edge_attr : edge feature matrix, \\(E\\in\\mathbb{R}^{n_\\mathrm{edges}\\times n_\\mathrm{edge\\_dim}}\\) data.y : training target with arbitary shape ( \\(y\\in\\mathbb{R}^{n_\\mathrm{nodes}\\times n_\\mathrm{out}}\\) for node-level targets, \\(y\\in\\mathbb{R}^{n_\\mathrm{edges}\\times n_\\mathrm{out}}\\) for edge-level targets or \\(y\\in\\mathbb{R}^{1\\times n_\\mathrm{out}}\\) for node-level targets). data.pos : Node position matrix, \\(P\\in\\mathbb{R}^{n_\\mathrm{nodes}\\times n_\\mathrm{space\\_dim}}\\) The PyG Introduction By Example tutorial covers the basics of graph creation, batching, transformation, and inference using this data class. As an example, consider the ZINC chemical compounds dataset , which available as a built-in dataset in PyG: from torch_geometric.datasets import ZINC train_dataset = ZINC ( root = '/tmp/ZINC' , subset = True , split = 'train' ) test_dataset = ZINC ( root = '/tmp/ZINC' , subset = True , split = 'test' ) len ( train_dataset ) >>> 10000 len ( test_dataset ) >>> 1000 Each graph in the dataset is a chemical compound; nodes are atoms and edges are chemical bonds. The node features x are categorical atom labels and the edge features edge_attr are categorical bond labels. The edge_index matrix lists all bonds present in the compound in COO format. The truth labels y indicate a synthetic computed property called constrained solubility; given a set of molecules represented as graphs, the task is to regress the constrained solubility. Therefore, this dataset is suitable for graph-level regression. Let's take a look at one molecule: data = train_dataset [ 27 ] data . x # node features >>> tensor ([[ 0 ], [ 0 ], [ 1 ], [ 2 ], [ 0 ], [ 0 ], [ 2 ], [ 0 ], [ 1 ], [ 2 ], [ 4 ], [ 0 ], [ 0 ], [ 0 ], [ 0 ], [ 4 ], [ 0 ], [ 0 ], [ 0 ], [ 0 ]]) data . pos # node positions >>> None data . edge_index # COO edge indices >>> tensor ([[ 0 , 1 , 1 , 1 , 2 , 3 , 3 , 4 , 4 , 5 , 5 , 6 , 6 , 7 , 7 , 7 , 8 , 9 , 9 , 10 , 10 , 10 , 11 , 11 , 12 , 12 , 13 , 13 , 14 , 14 , 15 , 15 , 15 , 16 , 16 , 16 , 16 , 17 , 18 , 19 ], # node indices w/ outgoing edges [ 1 , 0 , 2 , 3 , 1 , 1 , 4 , 3 , 5 , 4 , 6 , 5 , 7 , 6 , 8 , 9 , 7 , 7 , 10 , 9 , 11 , 15 , 10 , 12 , 11 , 13 , 12 , 14 , 13 , 15 , 10 , 14 , 16 , 15 , 17 , 18 , 19 , 16 , 16 , 16 ]]) # node indices w/ incoming edges data . edge_attr # edge features >>> tensor ([ 1 , 1 , 2 , 1 , 2 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 2 , 1 , 2 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]) data . y # truth labels >>> tensor ([ - 0.0972 ]) data . num_nodes >>> 20 data . num_edges >>> 40 data . num_node_features >>> 1 We can load the full set of graphs onto an available GPU and create PyG dataloaders as follows: import torch from torch_geometric.data import DataLoader device = 'cuda:0' if torch . cuda . is_available () else 'cpu' test_dataset = [ d . to ( device ) for d in test_dataset ] train_dataset = [ d . to ( device ) for d in train_dataset ] test_loader = DataLoader ( test_dataset , batch_size = 1 , shuffle = False ) train_loader = DataLoader ( train_dataset , batch_size = 1 , shuffle = True ) The Message Passing Base Class: PyG GNNs \u00b6 The 2017 paper Neural Message Passing for Quantum Chemistry presents a unified framework for a swath of GNN architectures known as message passing neural networks (MPNNs). MPNNs are GNNs whose feature updates are given by: \\[x_i^{(k)} = \\gamma^{(k)} \\left(x_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(x_i^{(k-1)}, x_j^{(k-1)},e_{ij}\\right) \\right)\\] Here, \\(\\gamma\\) and \\(\\phi\\) are learnable functions (which we can approximate as multilayer perceptrons), \\(\\square\\) is a permutation-invariant function (e.g. mean, max, add), and \\(\\mathcal{N}(i)\\) is the neighborhood of node \\(i\\) . In PyG, you'd write your own MPNN by using the MessagePassing base class, implementing each of the above mathematical objects as an explicit function. MessagePassing.message() : define an explicit NN for \\(\\phi\\) , use it to calculate \"messages\" between a node \\(x_i^{(k-1)}\\) and its neighbors \\(x_j^{(k-1)}\\) , \\(j\\in\\mathcal{N}(i)\\) , leveraging edge features \\(e_{ij}\\) if applicable MessagePassing.propagate() : in this step, messages are calculated via the message function and aggregated across each receiving node; the keyword aggr (which can be 'add' , 'max' , or 'mean' ) is used to specify the specific permutation invariant function \\(\\square_{j\\in\\mathcal{N}(i)}\\) used for message aggregation. MessagePassing.update() : the results of message passing are used to update the node features \\(x_i^{(k)}\\) through the \\(\\gamma\\) MLP The specific implementations of message() , propagate() , and update() are up to the user. A specific example is available in the PyG Creating Message Passing Networks tutorial Message-Passing with ZINC Data \u00b6 Returning to the ZINC molecular compound dataset, we can design a message-passing layer to aggregate messages across molecular graphs. Here, we'll define a multi-layer perceptron (MLP) class and use it to build a message passing layer (MPL) the following equation: \\[x_i' = \\gamma \\left(x_i, \\frac{1}{|\\mathcal{N}(i)|}\\sum_{j \\in \\mathcal{N}(i)} \\, \\phi\\left([x_i, x_j, e_{j,i}\\right]) \\right)\\] Here, the MLP dimensions are constrained. Since \\(x_i, e_{i,j}\\in\\mathbb{R}\\) , the \\(\\phi\\) MLP must map \\(\\mathbb{R}^3\\) to \\(\\mathbb{R}^\\mathrm{message\\_size}\\) . Similarly, \\(\\gamma\\) must map \\(\\mathbb{R}^{1+\\mathrm{\\mathrm{message\\_size}}}\\) to \\(\\mathbb{R}^\\mathrm{out}\\) . from torch_geometric.nn import MessagePassing import torch.nn as nn from torch.nn import Sequential as Seq , Linear , ReLU class MLP ( nn . Module ): def __init__ ( self , input_size , output_size ): super ( MLP , self ) . __init__ () self . layers = nn . Sequential ( nn . Linear ( input_size , 16 ), nn . ReLU (), nn . Linear ( 16 , 16 ), nn . ReLU (), nn . Linear ( 16 , output_size ), ) def forward ( self , x ): return self . layers ( x ) class MPLayer ( MessagePassing ): def __init__ ( self , n_node_feats , n_edge_feats , message_size , output_size ): super ( MPLayer , self ) . __init__ ( aggr = 'mean' , flow = 'source_to_target' ) self . phi = MLP ( 2 * n_node_feats + n_edge_feats , message_size ) self . gamma = MLP ( message_size + n_node_feats , output_size ) def forward ( self , x , edge_index , edge_attr ): return self . propagate ( edge_index , x = x , edge_attr = edge_attr ) def message ( self , x_i , x_j , edge_attr ): return self . phi ( torch . cat ([ x_i , x_j , edge_attr ], dim = 1 )) def update ( self , aggr_out , x ): return self . gamma ( torch . cat ([ x , aggr_out ], dim = 1 )) Let's apply this layer to one of the ZINC molecules: molecule = train_dataset [ 0 ] torch . Size ([ 29 , 1 ]) # 29 atoms and 1 feature (atom label) mpl = MPLayer ( 1 , 1 , 16 , 8 ) . to ( device ) # message_size = 16, output_size = 8 xprime = mpl ( graph . x . float (), graph . edge_index , graph . edge_attr . unsqueeze ( 1 )) xprime . shape >>> torch . Size ([ 29 , 8 ]) # 29 atoms and 8 features There we have it - the message passing layer has produced 8 new features for each atom.","title":"PyTorch Geometric"},{"location":"inference/pyg.html#pytorch-geometric","text":"Geometric deep learning (GDL) is an emerging field focused on applying machine learning (ML) techniques to non-Euclidean domains such as graphs, point clouds, and manifolds. The PyTorch Geometric (PyG) library extends PyTorch to include GDL functionality, for example classes necessary to handle data with irregular structure. PyG is introduced at a high level in Fast Graph Representation Learning with PyTorch Geometric and in detail in the PyG docs .","title":"PyTorch Geometric"},{"location":"inference/pyg.html#gdl-with-pyg","text":"A complete reveiw of GDL is available in the following recently-published (and freely-available) textbook: Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges . The authors specify several key GDL architectures including convolutional neural networks (CNNs) operating on grids, Deep Sets architectures operating on sets, and graph neural networks (GNNs) operating on graphs, collections of nodes connected by edges. PyG is focused in particular on graph-structured data, which naturally encompases set-structured data. In fact, many state-of-the-art GNN architectures are implemented in PyG (see the docs )! A review of the landscape of GNN architectures is available in Graph Neural Networks: A Review of Methods and Applications .","title":"GDL with PyG"},{"location":"inference/pyg.html#the-data-class-pyg-graphs","text":"Graphs are data structures designed to encode data structured as a set of objects and relations. Objects are embedded as graph nodes \\(u\\in\\mathcal{V}\\) , where \\(\\mathcal{V}\\) is the node set. Relations are represented by edges \\((i,j)\\in\\mathcal{E}\\) between nodes, where \\(\\mathcal{E}\\) is the edge set. Denote the sizes of the node and edge sets as \\(|\\mathcal{V}|=n_\\mathrm{nodes}\\) and \\(|\\mathcal{E}|=n_\\mathrm{edges}\\) respectively. The choice of edge connectivity determines the local structure of a graph, which has important downstream effects on graph-based learning algorithms. Graph construction is the process of embedding input data onto a graph structure. Graph-based learning algorithms are correspondingly imbued with a relational inductive bias based on the choice of graph representation; a graph's edge connectivity defines its local structure. The simplest graph construction routine is to construct no edges, yielding a permutation invariant set of objects. On the other hand, fully-connected graphs connect every node-node pair with an edge, yielding \\(n_\\mathrm{edges}=n_\\mathrm{nodes}(n_\\mathrm{nodes}-1)/2\\) edges. This representation may be feasible for small inputs like particle clouds corresponding to a jet, but is intractible for large-scale applications such as high-pileup tracking datasets. Notably, dynamic graph construction techniques operate on input point clouds, constructing edges on them dynamically during inference. For example, EdgeConv and GravNet GNN layers dynamically construct edges between nodes projected into a latent space; multiple such layers may be applied in sequence, yielding many intermediate graph representations on an input point cloud. In general, nodes can have positions \\(\\{p_i\\}_{i=1}^{n_\\mathrm{nodes}}\\) , \\(p_i\\in\\mathbb{R}^{n_\\mathrm{space\\_dim}}\\) , and features (attributes) \\(\\{x_i\\}_{i=1}^{n_\\mathrm{nodes}}\\) , \\(x_i\\in\\mathbb{R}^{n_\\mathrm{node\\_dim}}\\) . In some applications like GNN-based particle tracking, node positions are taken to be the features. In others, e.g. jet identification, positional information may be used to seed dynamic graph consturction while kinematic features are propagated as edge features. Edges, too, can have features \\(\\{e_{ij}\\}_{(i,j)\\in\\mathcal{E}}\\) , \\(e_{ij}\\in\\mathbb{R}^{n_\\mathrm{edge\\_dim}}\\) , but do not have positions; instead, edges are defined by the nodes they connect, and may therefore be represented by, for example, the distance between the respective node-node pair. In PyG, graphs are stored as instances of the data class, whose fields fully specify the graph: data.x : node feature matrix, \\(X\\in\\mathbb{R}^{n_\\mathrm{nodes}\\times n_\\mathrm{node\\_dim}}\\) data.edge_index : node indices at each end of each edge, \\(I\\in\\mathbb{R}^{2\\times n_\\mathrm{edges}}\\) data.edge_attr : edge feature matrix, \\(E\\in\\mathbb{R}^{n_\\mathrm{edges}\\times n_\\mathrm{edge\\_dim}}\\) data.y : training target with arbitary shape ( \\(y\\in\\mathbb{R}^{n_\\mathrm{nodes}\\times n_\\mathrm{out}}\\) for node-level targets, \\(y\\in\\mathbb{R}^{n_\\mathrm{edges}\\times n_\\mathrm{out}}\\) for edge-level targets or \\(y\\in\\mathbb{R}^{1\\times n_\\mathrm{out}}\\) for node-level targets). data.pos : Node position matrix, \\(P\\in\\mathbb{R}^{n_\\mathrm{nodes}\\times n_\\mathrm{space\\_dim}}\\) The PyG Introduction By Example tutorial covers the basics of graph creation, batching, transformation, and inference using this data class. As an example, consider the ZINC chemical compounds dataset , which available as a built-in dataset in PyG: from torch_geometric.datasets import ZINC train_dataset = ZINC ( root = '/tmp/ZINC' , subset = True , split = 'train' ) test_dataset = ZINC ( root = '/tmp/ZINC' , subset = True , split = 'test' ) len ( train_dataset ) >>> 10000 len ( test_dataset ) >>> 1000 Each graph in the dataset is a chemical compound; nodes are atoms and edges are chemical bonds. The node features x are categorical atom labels and the edge features edge_attr are categorical bond labels. The edge_index matrix lists all bonds present in the compound in COO format. The truth labels y indicate a synthetic computed property called constrained solubility; given a set of molecules represented as graphs, the task is to regress the constrained solubility. Therefore, this dataset is suitable for graph-level regression. Let's take a look at one molecule: data = train_dataset [ 27 ] data . x # node features >>> tensor ([[ 0 ], [ 0 ], [ 1 ], [ 2 ], [ 0 ], [ 0 ], [ 2 ], [ 0 ], [ 1 ], [ 2 ], [ 4 ], [ 0 ], [ 0 ], [ 0 ], [ 0 ], [ 4 ], [ 0 ], [ 0 ], [ 0 ], [ 0 ]]) data . pos # node positions >>> None data . edge_index # COO edge indices >>> tensor ([[ 0 , 1 , 1 , 1 , 2 , 3 , 3 , 4 , 4 , 5 , 5 , 6 , 6 , 7 , 7 , 7 , 8 , 9 , 9 , 10 , 10 , 10 , 11 , 11 , 12 , 12 , 13 , 13 , 14 , 14 , 15 , 15 , 15 , 16 , 16 , 16 , 16 , 17 , 18 , 19 ], # node indices w/ outgoing edges [ 1 , 0 , 2 , 3 , 1 , 1 , 4 , 3 , 5 , 4 , 6 , 5 , 7 , 6 , 8 , 9 , 7 , 7 , 10 , 9 , 11 , 15 , 10 , 12 , 11 , 13 , 12 , 14 , 13 , 15 , 10 , 14 , 16 , 15 , 17 , 18 , 19 , 16 , 16 , 16 ]]) # node indices w/ incoming edges data . edge_attr # edge features >>> tensor ([ 1 , 1 , 2 , 1 , 2 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 2 , 1 , 2 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]) data . y # truth labels >>> tensor ([ - 0.0972 ]) data . num_nodes >>> 20 data . num_edges >>> 40 data . num_node_features >>> 1 We can load the full set of graphs onto an available GPU and create PyG dataloaders as follows: import torch from torch_geometric.data import DataLoader device = 'cuda:0' if torch . cuda . is_available () else 'cpu' test_dataset = [ d . to ( device ) for d in test_dataset ] train_dataset = [ d . to ( device ) for d in train_dataset ] test_loader = DataLoader ( test_dataset , batch_size = 1 , shuffle = False ) train_loader = DataLoader ( train_dataset , batch_size = 1 , shuffle = True )","title":"The Data Class: PyG Graphs"},{"location":"inference/pyg.html#the-message-passing-base-class-pyg-gnns","text":"The 2017 paper Neural Message Passing for Quantum Chemistry presents a unified framework for a swath of GNN architectures known as message passing neural networks (MPNNs). MPNNs are GNNs whose feature updates are given by: \\[x_i^{(k)} = \\gamma^{(k)} \\left(x_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(x_i^{(k-1)}, x_j^{(k-1)},e_{ij}\\right) \\right)\\] Here, \\(\\gamma\\) and \\(\\phi\\) are learnable functions (which we can approximate as multilayer perceptrons), \\(\\square\\) is a permutation-invariant function (e.g. mean, max, add), and \\(\\mathcal{N}(i)\\) is the neighborhood of node \\(i\\) . In PyG, you'd write your own MPNN by using the MessagePassing base class, implementing each of the above mathematical objects as an explicit function. MessagePassing.message() : define an explicit NN for \\(\\phi\\) , use it to calculate \"messages\" between a node \\(x_i^{(k-1)}\\) and its neighbors \\(x_j^{(k-1)}\\) , \\(j\\in\\mathcal{N}(i)\\) , leveraging edge features \\(e_{ij}\\) if applicable MessagePassing.propagate() : in this step, messages are calculated via the message function and aggregated across each receiving node; the keyword aggr (which can be 'add' , 'max' , or 'mean' ) is used to specify the specific permutation invariant function \\(\\square_{j\\in\\mathcal{N}(i)}\\) used for message aggregation. MessagePassing.update() : the results of message passing are used to update the node features \\(x_i^{(k)}\\) through the \\(\\gamma\\) MLP The specific implementations of message() , propagate() , and update() are up to the user. A specific example is available in the PyG Creating Message Passing Networks tutorial","title":"The Message Passing Base Class: PyG GNNs"},{"location":"inference/pyg.html#message-passing-with-zinc-data","text":"Returning to the ZINC molecular compound dataset, we can design a message-passing layer to aggregate messages across molecular graphs. Here, we'll define a multi-layer perceptron (MLP) class and use it to build a message passing layer (MPL) the following equation: \\[x_i' = \\gamma \\left(x_i, \\frac{1}{|\\mathcal{N}(i)|}\\sum_{j \\in \\mathcal{N}(i)} \\, \\phi\\left([x_i, x_j, e_{j,i}\\right]) \\right)\\] Here, the MLP dimensions are constrained. Since \\(x_i, e_{i,j}\\in\\mathbb{R}\\) , the \\(\\phi\\) MLP must map \\(\\mathbb{R}^3\\) to \\(\\mathbb{R}^\\mathrm{message\\_size}\\) . Similarly, \\(\\gamma\\) must map \\(\\mathbb{R}^{1+\\mathrm{\\mathrm{message\\_size}}}\\) to \\(\\mathbb{R}^\\mathrm{out}\\) . from torch_geometric.nn import MessagePassing import torch.nn as nn from torch.nn import Sequential as Seq , Linear , ReLU class MLP ( nn . Module ): def __init__ ( self , input_size , output_size ): super ( MLP , self ) . __init__ () self . layers = nn . Sequential ( nn . Linear ( input_size , 16 ), nn . ReLU (), nn . Linear ( 16 , 16 ), nn . ReLU (), nn . Linear ( 16 , output_size ), ) def forward ( self , x ): return self . layers ( x ) class MPLayer ( MessagePassing ): def __init__ ( self , n_node_feats , n_edge_feats , message_size , output_size ): super ( MPLayer , self ) . __init__ ( aggr = 'mean' , flow = 'source_to_target' ) self . phi = MLP ( 2 * n_node_feats + n_edge_feats , message_size ) self . gamma = MLP ( message_size + n_node_feats , output_size ) def forward ( self , x , edge_index , edge_attr ): return self . propagate ( edge_index , x = x , edge_attr = edge_attr ) def message ( self , x_i , x_j , edge_attr ): return self . phi ( torch . cat ([ x_i , x_j , edge_attr ], dim = 1 )) def update ( self , aggr_out , x ): return self . gamma ( torch . cat ([ x , aggr_out ], dim = 1 )) Let's apply this layer to one of the ZINC molecules: molecule = train_dataset [ 0 ] torch . Size ([ 29 , 1 ]) # 29 atoms and 1 feature (atom label) mpl = MPLayer ( 1 , 1 , 16 , 8 ) . to ( device ) # message_size = 16, output_size = 8 xprime = mpl ( graph . x . float (), graph . edge_index , graph . edge_attr . unsqueeze ( 1 )) xprime . shape >>> torch . Size ([ 29 , 8 ]) # 29 atoms and 8 features There we have it - the message passing layer has produced 8 new features for each atom.","title":"Message-Passing with ZINC Data"},{"location":"inference/pytorch.html","text":"PyTorch Inference \u00b6 PyTorch is an open source ML library developed by Facebook's AI Research lab. Initially released in late-2016, PyTorch is a relatively new tool, but has become increasingly popular among ML researchers (in fact, some analyses suggest it's becoming more popular than TensorFlow in academic communities!). PyTorch is written in idiomatic Python, so its syntax is easy to parse for experienced Python programmers. Additionally, it is highly compatible with graphics processing units (GPUs), which can substantially accelerate many deep learning workflows. To date PyTorch has not been integrated into CMSSW. Trained PyTorch models may be evaluated in CMSSW via ONNX Runtime, but model construction and training workflows must currently exist outside of CMSSW. Given the considerable interest in PyTorch within the HEP/ML community, we have reason to believe it will soon be available, so stay tuned! Introductory References \u00b6 PyTorch Install Guide PyTorch Tutorials LPC HATs: PyTorch Deep Learning w/ PyTorch Course Repo CODAS-HEP The Basics \u00b6 The following documentation surrounds a set of code snippets designed to highlight some important ML features made available in PyTorch. In the following sections, we'll break down snippets from this script, highlighting specifically the PyTorch objects in it. Tensors \u00b6 The fundamental PyTorch object is the tensor. At a glance, tensors behave similarly to NumPy arrays. For example, they are broadcasted, concatenated, and sliced in exactly the same way. The following examples highlight some common numpy-like tensor transformations: a = torch . randn ( size = ( 2 , 2 )) >>> tensor ([[ 1.3552 , - 0.0204 ], [ 1.2677 , - 0.8926 ]]) a . view ( - 1 , 1 ) >>> tensor ([[ 1.3552 ], [ - 0.0204 ], [ 1.2677 ], [ - 0.8926 ]]) a . transpose ( 0 , 1 ) >>> tensor ([[ 1.3552 , 1.2677 ], [ - 0.0204 , - 0.8926 ]]) a . unsqueeze ( dim = 0 ) >>> tensor ([[[ 1.3552 , - 0.0204 ], [ 1.2677 , - 0.8926 ]]]) a . squeeze ( dim = 0 ) >>> tensor ([[ 1.3552 , - 0.0204 ], [ 1.2677 , - 0.8926 ]]) Additionally, torch supports familiar matrix operations with various syntax options: m1 = torch . randn ( size = ( 2 , 3 )) m2 = torch . randn ( size = ( 3 , 2 )) x = torch . randn ( 3 ) m1 @ m2 == m1 . mm ( m2 ) # matrix multiplication >>> tensor ([[ True , True ], [ True , True ]]) m1 @ x == m1 . mv ( x ) # matrix-vector multiplication >>> tensor ([ True , True ]) m1 . t () == m1 . transpose ( 0 , 1 ) # matrix transpose >>> tensor ([[ True , True ], [ True , True ], [ True , True ]]) Note that tensor.transpose(dim0, dim1) is a more general operation than tensor.t() . It is important to note that tensors have been ''upgraded'' from Numpy arrays in two key ways: 1) Tensors have native GPU support. If a GPU is available at runtime, tensors can be transferred from CPU to GPU, where computations such as matrix operations are substantially faster. Note that tensor operations must be performed on objects on the same device. PyTorch supports CUDA tensor types for GPU computation (see the PyTorch Cuda Semantics guide). 2) Tensors support automatic gradient (audograd) calculations, such that operations on tensors flagged with requires_grad=True are automatically tracked. The flow of tracked tensor operations defines a computation graph in which nodes are tensors and edges are functions mapping input tensors to output tensors. Gradients are calculated numerically via autograd by walking through this computation graph. GPU Support \u00b6 Tensors are created on the host CPU by default: b = torch . zeros ([ 2 , 3 ], dtype = torch . int32 ) b . device >>> cpu You can also create tensors on any available GPUs: torch . cuda . is_available () # check that a GPU is available >>> True cuda0 = torch . device ( 'cuda:0' ) c = torch . ones ([ 2 , 3 ], dtype = torch . int32 , device = cuda0 ) c . device >>> cuda : 0 You can also move tensors between devices: b = b . to ( cuda0 ) b . device >>> cuda : 0 There are trade-offs between computations on the CPU and GPU. GPUs have limited memory and there is a cost associated with transfering data from CPUs to GPUs. However, GPUs perform heavy matrix operations much faster than CPUs, and are therefore often used to speed up training routines. N = 1000 # for i , N in enumerate ([ 10 , 100 , 500 , 1000 , 5000 ]): print ( \"( {} , {} ) Matrices:\" . format ( N , N )) M1_cpu = torch . randn ( size = ( N , N ), device = 'cpu' ) M2_cpu = torch . randn ( size = ( N , N ), device = 'cpu' ) M1_gpu = torch . randn ( size = ( N , N ), device = cuda0 ) M2_gpu = torch . randn ( size = ( N , N ), device = cuda0 ) if ( i == 0 ): print ( 'Check devices for each tensor:' ) print ( 'M1_cpu, M2_cpu devices:' , M1_cpu . device , M2_cpu . device ) print ( 'M1_gpu, M2_gpu devices:' , M1_gpu . device , M2_gpu . device ) def large_matrix_multiply ( M1 , M2 ): return M1 * M2 . transpose ( 0 , 1 ) n_iter = 1000 t_cpu = Timer ( lambda : large_matrix_multiply ( M1_cpu , M2_cpu )) cpu_time = t_cpu . timeit ( number = n_iter ) / n_iter print ( 'cpu time per call: {:.6f} s' . format ( cpu_time )) t_gpu = Timer ( lambda : large_matrix_multiply ( M1_gpu , M2_gpu )) gpu_time = t_gpu . timeit ( number = n_iter ) / n_iter print ( 'gpu time per call: {:.6f} s' . format ( gpu_time )) print ( 'gpu_time/cpu_time: {:.6f} \\n ' . format ( gpu_time / cpu_time )) >>> ( 10 , 10 ) Matrices : Check devices for each tensor : M1_cpu , M2_cpu devices : cpu cpu M1_gpu , M2_gpu devices : cuda : 0 cuda : 0 cpu time per call : 0.000008 s gpu time per call : 0.000015 s gpu_time / cpu_time : 1.904711 ( 100 , 100 ) Matrices : cpu time per call : 0.000015 s gpu time per call : 0.000015 s gpu_time / cpu_time : 0.993163 ( 500 , 500 ) Matrices : cpu time per call : 0.000058 s gpu time per call : 0.000016 s gpu_time / cpu_time : 0.267371 ( 1000 , 1000 ) Matrices : cpu time per call : 0.000170 s gpu time per call : 0.000015 s gpu_time / cpu_time : 0.089784 ( 5000 , 5000 ) Matrices : cpu time per call : 0.025083 s gpu time per call : 0.000011 s gpu_time / cpu_time : 0.000419 The complete list of Torch Tensor operations is available in the docs . Autograd \u00b6 Backpropagation occurs automatically through autograd. For example, consider the following function and its derivatives: \\[\\begin{aligned} f(\\textbf{a}, \\textbf{b}) &= \\textbf{a}^T \\textbf{X} \\textbf{b} \\\\ \\frac{\\partial f}{\\partial \\textbf{a}} &= \\textbf{b}^T \\textbf{X}^T\\\\ \\frac{\\partial f}{\\partial \\textbf{b}} &= \\textbf{a}^T \\textbf{X} \\end{aligned}\\] Given specific choices of \\(\\textbf{X}\\) , \\(\\textbf{a}\\) , and \\(\\textbf{b}\\) , we can calculate the corresponding derivatives via autograd by requiring a gradient to be stored in each relevant tensor: X = torch . ones (( 2 , 2 ), requires_grad = True ) a = torch . tensor ([ 0.5 , 1 ], requires_grad = True ) b = torch . tensor ([ 0.5 , - 2 ], requires_grad = True ) f = a . T @ X @ b f >>> tensor ( - 2.2500 , grad_fn =< DotBackward > ) f . backward () # backprop a . grad >>> tensor ([ - 1.5000 , - 1.5000 ]) b . T @ X . T >>> tensor ([ - 1.5000 , - 1.5000 ], grad_fn =< SqueezeBackward3 > ) b . grad >>> tensor ([ 1.5000 , 1.5000 ]) a . T @ X >>> tensor ([ 1.5000 , 1.5000 ], grad_fn =< SqueezeBackward3 > ) The tensor.backward() call initiates backpropagation, accumulating the gradient backward through a series of grad_fn labels tied to each tensor (e.g. <DotBackward> , indicating the dot product \\((\\textbf{a}^T\\textbf{X})\\textbf{b}\\) ). Data Utils \u00b6 PyTorch is equipped with many useful data-handling utilities. For example, the torch.utils.data package implements datasets ( torch.utils.data.Dataset ) and iterable data loaders ( torch.utils.data.DataLoader ). Additionally, various batching and sampling schemes are available. You can create custom iterable datasets via torch.utils.data.Dataset , for example a dataset collecting the results of XOR on two binary inputs: from torch.utils.data import Dataset class Data ( Dataset ): def __init__ ( self , device ): self . samples = torch . tensor ([[ 0 , 0 ], [ 0 , 1 ], [ 1 , 0 ], [ 1 , 1 ]]) . float () . to ( device ) self . targets = np . logical_xor ( self . samples [:, 0 ], self . samples [:, 1 ]) . float () . to ( device ) def __len__ ( self ): return len ( self . targets ) def __getitem__ ( self , idx ): return ({ 'x' : self . samples [ idx ], 'y' : self . targets [ idx ]}) Dataloaders, from torch.utils.data.DataLoader , can generate shuffled batches of data via multiple workers. Here, we load our datasets onto the GPU: from torch.utils.data import DataLoader device = 'cpu' train_data = Data ( device ) test_data = Data ( device ) train_loader = DataLoader ( train_data , batch_size = 1 , shuffle = True , num_workers = 2 ) test_loader = DataLoader ( test_data , batch_size = 1 , shuffle = False , num_workers = 2 ) for i , batch in enumerate ( train_loader ): print ( i , batch ) >>> 0 { 'x' : tensor ([[ 0. , 0. ]]), 'y' : tensor ([ 0. ])} 1 { 'x' : tensor ([[ 1. , 0. ]]), 'y' : tensor ([ 1. ])} 2 { 'x' : tensor ([[ 1. , 1. ]]), 'y' : tensor ([ 0. ])} 3 { 'x' : tensor ([[ 0. , 1. ]]), 'y' : tensor ([ 1. ])} The full set of data utils is available in the docs . Neural Networks \u00b6 The PyTorch nn package specifies a set of modules that correspond to different neural network (NN) components and operations. For example, the torch.nn.Linear module defines a linear transform with learnable parameters and the torch.nn.Flatten module flattens two contiguous tensor dimensions. The torch.nn.Sequential module contains a set of modules such as torch.nn.Linear and torch.nn.Sequential , chaining them together to form the forward pass of a forward network. Furthermore, one may specify various pre-implemented loss functions, for example torch.nn.BCELoss and torch.nn.KLDivLoss . The full set of PyTorch NN building blocks is available in the docs . As an example, we can design a simple neural network designed to reproduce the output of the XOR operation on binary inputs. To do so, we can compute a simple NN of the form: \\[\\begin{aligned} x_{in}&\\in\\{0,1\\}^{2}\\\\ l_1 &= \\sigma(W_1^Tx_{in} + b_1); \\ W_1\\in\\mathbb{R}^{2\\times2},\\ b_1\\in\\mathbb{R}^{2}\\\\ l_2 &= \\sigma(W_2^Tx + b_2); \\ W_2\\in\\mathbb{R}^{2},\\ b_1\\in\\mathbb{R}\\\\ \\end{aligned}\\] import torch.nn as nn class Network ( nn . Module ): def __init__ ( self ): super () . __init__ () self . l1 = nn . Linear ( 2 , 2 ) self . l2 = nn . Linear ( 2 , 1 ) def forward ( self , x ): x = torch . sigmoid ( self . l1 ( x )) x = torch . sigmoid ( self . l2 ( x )) return x model = Network () . to ( device ) model ( train_data [ 'x' ]) >>> tensor ([[ 0.5000 ], [ 0.4814 ], [ 0.5148 ], [ 0.4957 ]], grad_fn =< SigmoidBackward > ) Optimizers \u00b6 Training a neural network involves minimizing a loss function; classes in the torch.optim package implement various optimization strategies for example stochastic gradient descent and Adam through torch.optim.SGD and torch.optim.Adam respectively. Optimizers are configurable through parameters such as the learning rate (configuring the optimizer's step size). The full set of optimizers and accompanying tutorials are available in the docs . To demonstrate the use of an optimizer, let's train the NN above to produce the results of the XOR operation on binary inputs. Here we'll use the Adam optimizer : from torch import optim from torch.optim.lr_scheduler import StepLR from matplotlib import pyplot as plt # helpful references: # Learning XOR: exploring the space of a classic problem # https://towardsdatascience.com/how-neural-networks-solve-the-xor-problem-59763136bdd7 # https://courses.cs.washington.edu/courses/cse446/18wi/sections/section8/XOR-Pytorch.html # the training function initiates backprop and # steps the optimizer towards the weights that # optimize the loss function def train ( model , train_loader , optimizer , epoch ): model . train () losses = [] for i , batch in enumerate ( train_loader ): optimizer . zero_grad () output = model ( batch [ 'x' ]) y , output = batch [ 'y' ], output . squeeze ( 1 ) # optimize binary cross entropy: # https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html loss = F . binary_cross_entropy ( output , y , reduction = 'mean' ) loss . backward () optimizer . step () losses . append ( loss . item ()) return np . mean ( losses ) # the test function does not adjust the model's weights def test ( model , test_loader ): model . eval () losses , n_correct , n_incorrect = [], 0 , 0 with torch . no_grad (): for i , batch in enumerate ( test_loader ): output = model ( batch [ 'x' ]) y , output = batch [ 'y' ], output . squeeze ( 1 ) loss = F . binary_cross_entropy ( output , y , reduction = 'mean' ) . item () losses . append ( loss ) # determine accuracy by thresholding model output at 0.5 batch_correct = torch . sum ((( output > 0.5 ) & ( y == 1 )) | (( output < 0.5 ) & ( y == 0 ))) batch_incorrect = len ( y ) - batch_correct n_correct += batch_correct n_incorrect += batch_incorrect return np . mean ( losses ), n_correct / ( n_correct + n_incorrect ) # randomly initialize the model's weights for module in model . modules (): if isinstance ( module , nn . Linear ): module . weight . data . normal_ ( 0 , 1 ) # send weights to optimizer lr = 2.5e-2 optimizer = optim . Adam ( model . parameters (), lr = lr ) epochs = 500 for epoch in range ( 1 , epochs + 1 ): train_loss = train ( model , train_loader , optimizer , epoch ) test_loss , test_acc = test ( model , test_loader ) if epoch % 25 == 0 : print ( 'epoch= {} : train_loss= {:.3f} , test_loss= {:.3f} , test_acc= {:.3f} ' . format ( epoch , train_loss , test_loss , test_acc )) >>> epoch = 25 : train_loss = 0.683 , test_loss = 0.681 , test_acc = 0.500 epoch = 50 : train_loss = 0.665 , test_loss = 0.664 , test_acc = 0.750 epoch = 75 : train_loss = 0.640 , test_loss = 0.635 , test_acc = 0.750 epoch = 100 : train_loss = 0.598 , test_loss = 0.595 , test_acc = 0.750 epoch = 125 : train_loss = 0.554 , test_loss = 0.550 , test_acc = 0.750 epoch = 150 : train_loss = 0.502 , test_loss = 0.498 , test_acc = 0.750 epoch = 175 : train_loss = 0.435 , test_loss = 0.432 , test_acc = 0.750 epoch = 200 : train_loss = 0.360 , test_loss = 0.358 , test_acc = 0.750 epoch = 225 : train_loss = 0.290 , test_loss = 0.287 , test_acc = 1.000 epoch = 250 : train_loss = 0.230 , test_loss = 0.228 , test_acc = 1.000 epoch = 275 : train_loss = 0.184 , test_loss = 0.183 , test_acc = 1.000 epoch = 300 : train_loss = 0.149 , test_loss = 0.148 , test_acc = 1.000 epoch = 325 : train_loss = 0.122 , test_loss = 0.122 , test_acc = 1.000 epoch = 350 : train_loss = 0.102 , test_loss = 0.101 , test_acc = 1.000 epoch = 375 : train_loss = 0.086 , test_loss = 0.086 , test_acc = 1.000 epoch = 400 : train_loss = 0.074 , test_loss = 0.073 , test_acc = 1.000 epoch = 425 : train_loss = 0.064 , test_loss = 0.063 , test_acc = 1.000 epoch = 450 : train_loss = 0.056 , test_loss = 0.055 , test_acc = 1.000 epoch = 475 : train_loss = 0.049 , test_loss = 0.049 , test_acc = 1.000 epoch = 500 : train_loss = 0.043 , test_loss = 0.043 , test_acc = 1.000 Here, the model has converged to 100% test accuracy, indicating that it has learned to reproduce the XOR outputs perfectly. Note that even though the test accuracy is 100%, the test loss (BCE) decreases steadily; this is because the BCE loss is nonzero when \\(y_{output}\\) is not exactly 0 or 1, while accuracy is determined by thresholding the model outputs such that each prediction is the boolean \\((y_{output} > 0.5)\\) . This highlights that it is important to choose the correct performance metric for an ML problem. In the case of XOR, perfect test accuracy is sufficient. Let's check that we've recovered the XOR output by extracting the model's weights and using them to build a custom XOR function: for name , param in model . named_parameters (): if param . requires_grad : print ( name , param . data ) >>> l1 . weight tensor ([[ 7.2888 , - 6.4168 ], [ 7.2824 , - 8.1637 ]]) l1 . bias tensor ([ 2.6895 , - 3.9633 ]) l2 . weight tensor ([[ - 6.3500 , 8.0990 ]]) l2 . bias tensor ([ 2.5058 ]) Because our model was built with nn.Linear modules, we have weight matrices and bias terms. Next, we'll hard-code the matrix operations into a custom XOR function based on the architecture of the NN: def XOR ( x ): w1 = torch . tensor ([[ 7.2888 , - 6.4168 ], [ 7.2824 , - 8.1637 ]]) . t () b1 = torch . tensor ([ 2.6895 , - 3.9633 ]) layer1_out = torch . tensor ([ x [ 0 ] * w1 [ 0 , 0 ] + x [ 1 ] * w1 [ 1 , 0 ] + b1 [ 0 ], x [ 0 ] * w1 [ 0 , 1 ] + x [ 1 ] * w1 [ 1 , 1 ] + b1 [ 1 ]]) layer1_out = torch . sigmoid ( layer1_out ) w2 = torch . tensor ([ - 6.3500 , 8.0990 ]) b2 = 2.5058 layer2_out = layer1_out [ 0 ] * w2 [ 0 ] + layer1_out [ 1 ] * w2 [ 1 ] + b2 layer2_out = torch . sigmoid ( layer2_out ) return layer2_out , ( layer2_out > 0.5 ) XOR ([ 0. , 0. ]) >>> ( tensor ( 0.0359 ), tensor ( False )) XOR ([ 0. , 1. ]) >>> ( tensor ( 0.9135 ), tensor ( True )) XOR ([ 1. , 0. ]) >>> ( tensor ( 0.9815 ), tensor ( True )) XOR ([ 1. , 1. ]) >>> ( tensor ( 0.0265 ), tensor ( False )) There we have it - the NN learned XOR! PyTorch in CMSSW \u00b6 Via ONNX \u00b6 One way to incorporate your PyTorch models into CMSSW is through the Open Neural Network Exchange (ONNX) Runtime tool. In brief, ONNX supports training and inference for a variety of ML frameworks, and is currently integrated into CMSSW (see the CMS ML tutorial). PyTorch hosts an excellent tutorial on exporting a model from PyTorch to ONNX . ONNX is available in CMSSW (see a relevant discussion in the CMSSW git repo). Example Use Cases \u00b6 The \\(ZZ\\rightarrow 4b\\) analysis utilizes trained PyTorch models via ONNX in CMSSW (see the corresponding repo ). Briefly, they run ONNX in CMSSW_11_X via the CMSSW package PhysicsTools/ONNXRuntime , using it to define a multiClassifierONNX class. This multiclassifier is capable of loading pre-trained PyTorch models specified by a modelFile string as follows: #include \"PhysicsTools/ONNXRuntime/interface/ONNXRuntime.h\" std :: unique_ptr < cms :: Ort :: ONNXRuntime > model ; Ort :: SessionOptions * session_options = new Ort :: SessionOptions (); session_options -> SetIntraOpNumThreads ( 1 ); model = std :: make_unique < cms :: Ort :: ONNXRuntime > ( modelFile , session_options ); Via Triton \u00b6 Coprocessors (GPUs, FPGAs, etc.) are frequently used to accelerate ML operations such as inference and training. In the 'as-a-service' paradigm, users can access cloud-based applications through lightweight client inferfaces. The Services for Optimized Network Inference on Coprocessors ( SONIC ) framework implements this paradigm in CMSSW, allowing the optimal integration of GPUs into event processing workflows. One powerful implementation of SONIC is the the NVIDIA Triton Inference Server, which is flexible with respect to ML framework, storage source, and hardware infrastructure. For more details, see the corresponding NVIDIA developer blog entry . A Graph Attention Network (GAN) is available via Triton in CMSSW, and can be accessed here: https://github.com/cms-sw/cmssw/tree/master/HeterogeneousCore/SonicTriton/test Training Tips \u00b6 When instantiating a DataLoader , shuffle=True should be enabled for training data but not for validation and testing data. At each training epoch, this will vary the order of data objects in each batch; accordingly, it is not efficient to load the full dataset (in its original ordering) into GPU memory before training. Instead, enable num_workers>1 ; this allows the DataLoader to load batches to the GPU as they're prepared. Note that this launches muliple threads on the CPU. For more information, see a corresponding discussion in the PyTorch forum.","title":"PyTorch"},{"location":"inference/pytorch.html#pytorch-inference","text":"PyTorch is an open source ML library developed by Facebook's AI Research lab. Initially released in late-2016, PyTorch is a relatively new tool, but has become increasingly popular among ML researchers (in fact, some analyses suggest it's becoming more popular than TensorFlow in academic communities!). PyTorch is written in idiomatic Python, so its syntax is easy to parse for experienced Python programmers. Additionally, it is highly compatible with graphics processing units (GPUs), which can substantially accelerate many deep learning workflows. To date PyTorch has not been integrated into CMSSW. Trained PyTorch models may be evaluated in CMSSW via ONNX Runtime, but model construction and training workflows must currently exist outside of CMSSW. Given the considerable interest in PyTorch within the HEP/ML community, we have reason to believe it will soon be available, so stay tuned!","title":"PyTorch Inference"},{"location":"inference/pytorch.html#introductory-references","text":"PyTorch Install Guide PyTorch Tutorials LPC HATs: PyTorch Deep Learning w/ PyTorch Course Repo CODAS-HEP","title":"Introductory References"},{"location":"inference/pytorch.html#the-basics","text":"The following documentation surrounds a set of code snippets designed to highlight some important ML features made available in PyTorch. In the following sections, we'll break down snippets from this script, highlighting specifically the PyTorch objects in it.","title":"The Basics"},{"location":"inference/pytorch.html#tensors","text":"The fundamental PyTorch object is the tensor. At a glance, tensors behave similarly to NumPy arrays. For example, they are broadcasted, concatenated, and sliced in exactly the same way. The following examples highlight some common numpy-like tensor transformations: a = torch . randn ( size = ( 2 , 2 )) >>> tensor ([[ 1.3552 , - 0.0204 ], [ 1.2677 , - 0.8926 ]]) a . view ( - 1 , 1 ) >>> tensor ([[ 1.3552 ], [ - 0.0204 ], [ 1.2677 ], [ - 0.8926 ]]) a . transpose ( 0 , 1 ) >>> tensor ([[ 1.3552 , 1.2677 ], [ - 0.0204 , - 0.8926 ]]) a . unsqueeze ( dim = 0 ) >>> tensor ([[[ 1.3552 , - 0.0204 ], [ 1.2677 , - 0.8926 ]]]) a . squeeze ( dim = 0 ) >>> tensor ([[ 1.3552 , - 0.0204 ], [ 1.2677 , - 0.8926 ]]) Additionally, torch supports familiar matrix operations with various syntax options: m1 = torch . randn ( size = ( 2 , 3 )) m2 = torch . randn ( size = ( 3 , 2 )) x = torch . randn ( 3 ) m1 @ m2 == m1 . mm ( m2 ) # matrix multiplication >>> tensor ([[ True , True ], [ True , True ]]) m1 @ x == m1 . mv ( x ) # matrix-vector multiplication >>> tensor ([ True , True ]) m1 . t () == m1 . transpose ( 0 , 1 ) # matrix transpose >>> tensor ([[ True , True ], [ True , True ], [ True , True ]]) Note that tensor.transpose(dim0, dim1) is a more general operation than tensor.t() . It is important to note that tensors have been ''upgraded'' from Numpy arrays in two key ways: 1) Tensors have native GPU support. If a GPU is available at runtime, tensors can be transferred from CPU to GPU, where computations such as matrix operations are substantially faster. Note that tensor operations must be performed on objects on the same device. PyTorch supports CUDA tensor types for GPU computation (see the PyTorch Cuda Semantics guide). 2) Tensors support automatic gradient (audograd) calculations, such that operations on tensors flagged with requires_grad=True are automatically tracked. The flow of tracked tensor operations defines a computation graph in which nodes are tensors and edges are functions mapping input tensors to output tensors. Gradients are calculated numerically via autograd by walking through this computation graph.","title":"Tensors"},{"location":"inference/pytorch.html#gpu-support","text":"Tensors are created on the host CPU by default: b = torch . zeros ([ 2 , 3 ], dtype = torch . int32 ) b . device >>> cpu You can also create tensors on any available GPUs: torch . cuda . is_available () # check that a GPU is available >>> True cuda0 = torch . device ( 'cuda:0' ) c = torch . ones ([ 2 , 3 ], dtype = torch . int32 , device = cuda0 ) c . device >>> cuda : 0 You can also move tensors between devices: b = b . to ( cuda0 ) b . device >>> cuda : 0 There are trade-offs between computations on the CPU and GPU. GPUs have limited memory and there is a cost associated with transfering data from CPUs to GPUs. However, GPUs perform heavy matrix operations much faster than CPUs, and are therefore often used to speed up training routines. N = 1000 # for i , N in enumerate ([ 10 , 100 , 500 , 1000 , 5000 ]): print ( \"( {} , {} ) Matrices:\" . format ( N , N )) M1_cpu = torch . randn ( size = ( N , N ), device = 'cpu' ) M2_cpu = torch . randn ( size = ( N , N ), device = 'cpu' ) M1_gpu = torch . randn ( size = ( N , N ), device = cuda0 ) M2_gpu = torch . randn ( size = ( N , N ), device = cuda0 ) if ( i == 0 ): print ( 'Check devices for each tensor:' ) print ( 'M1_cpu, M2_cpu devices:' , M1_cpu . device , M2_cpu . device ) print ( 'M1_gpu, M2_gpu devices:' , M1_gpu . device , M2_gpu . device ) def large_matrix_multiply ( M1 , M2 ): return M1 * M2 . transpose ( 0 , 1 ) n_iter = 1000 t_cpu = Timer ( lambda : large_matrix_multiply ( M1_cpu , M2_cpu )) cpu_time = t_cpu . timeit ( number = n_iter ) / n_iter print ( 'cpu time per call: {:.6f} s' . format ( cpu_time )) t_gpu = Timer ( lambda : large_matrix_multiply ( M1_gpu , M2_gpu )) gpu_time = t_gpu . timeit ( number = n_iter ) / n_iter print ( 'gpu time per call: {:.6f} s' . format ( gpu_time )) print ( 'gpu_time/cpu_time: {:.6f} \\n ' . format ( gpu_time / cpu_time )) >>> ( 10 , 10 ) Matrices : Check devices for each tensor : M1_cpu , M2_cpu devices : cpu cpu M1_gpu , M2_gpu devices : cuda : 0 cuda : 0 cpu time per call : 0.000008 s gpu time per call : 0.000015 s gpu_time / cpu_time : 1.904711 ( 100 , 100 ) Matrices : cpu time per call : 0.000015 s gpu time per call : 0.000015 s gpu_time / cpu_time : 0.993163 ( 500 , 500 ) Matrices : cpu time per call : 0.000058 s gpu time per call : 0.000016 s gpu_time / cpu_time : 0.267371 ( 1000 , 1000 ) Matrices : cpu time per call : 0.000170 s gpu time per call : 0.000015 s gpu_time / cpu_time : 0.089784 ( 5000 , 5000 ) Matrices : cpu time per call : 0.025083 s gpu time per call : 0.000011 s gpu_time / cpu_time : 0.000419 The complete list of Torch Tensor operations is available in the docs .","title":"GPU Support"},{"location":"inference/pytorch.html#autograd","text":"Backpropagation occurs automatically through autograd. For example, consider the following function and its derivatives: \\[\\begin{aligned} f(\\textbf{a}, \\textbf{b}) &= \\textbf{a}^T \\textbf{X} \\textbf{b} \\\\ \\frac{\\partial f}{\\partial \\textbf{a}} &= \\textbf{b}^T \\textbf{X}^T\\\\ \\frac{\\partial f}{\\partial \\textbf{b}} &= \\textbf{a}^T \\textbf{X} \\end{aligned}\\] Given specific choices of \\(\\textbf{X}\\) , \\(\\textbf{a}\\) , and \\(\\textbf{b}\\) , we can calculate the corresponding derivatives via autograd by requiring a gradient to be stored in each relevant tensor: X = torch . ones (( 2 , 2 ), requires_grad = True ) a = torch . tensor ([ 0.5 , 1 ], requires_grad = True ) b = torch . tensor ([ 0.5 , - 2 ], requires_grad = True ) f = a . T @ X @ b f >>> tensor ( - 2.2500 , grad_fn =< DotBackward > ) f . backward () # backprop a . grad >>> tensor ([ - 1.5000 , - 1.5000 ]) b . T @ X . T >>> tensor ([ - 1.5000 , - 1.5000 ], grad_fn =< SqueezeBackward3 > ) b . grad >>> tensor ([ 1.5000 , 1.5000 ]) a . T @ X >>> tensor ([ 1.5000 , 1.5000 ], grad_fn =< SqueezeBackward3 > ) The tensor.backward() call initiates backpropagation, accumulating the gradient backward through a series of grad_fn labels tied to each tensor (e.g. <DotBackward> , indicating the dot product \\((\\textbf{a}^T\\textbf{X})\\textbf{b}\\) ).","title":"Autograd"},{"location":"inference/pytorch.html#data-utils","text":"PyTorch is equipped with many useful data-handling utilities. For example, the torch.utils.data package implements datasets ( torch.utils.data.Dataset ) and iterable data loaders ( torch.utils.data.DataLoader ). Additionally, various batching and sampling schemes are available. You can create custom iterable datasets via torch.utils.data.Dataset , for example a dataset collecting the results of XOR on two binary inputs: from torch.utils.data import Dataset class Data ( Dataset ): def __init__ ( self , device ): self . samples = torch . tensor ([[ 0 , 0 ], [ 0 , 1 ], [ 1 , 0 ], [ 1 , 1 ]]) . float () . to ( device ) self . targets = np . logical_xor ( self . samples [:, 0 ], self . samples [:, 1 ]) . float () . to ( device ) def __len__ ( self ): return len ( self . targets ) def __getitem__ ( self , idx ): return ({ 'x' : self . samples [ idx ], 'y' : self . targets [ idx ]}) Dataloaders, from torch.utils.data.DataLoader , can generate shuffled batches of data via multiple workers. Here, we load our datasets onto the GPU: from torch.utils.data import DataLoader device = 'cpu' train_data = Data ( device ) test_data = Data ( device ) train_loader = DataLoader ( train_data , batch_size = 1 , shuffle = True , num_workers = 2 ) test_loader = DataLoader ( test_data , batch_size = 1 , shuffle = False , num_workers = 2 ) for i , batch in enumerate ( train_loader ): print ( i , batch ) >>> 0 { 'x' : tensor ([[ 0. , 0. ]]), 'y' : tensor ([ 0. ])} 1 { 'x' : tensor ([[ 1. , 0. ]]), 'y' : tensor ([ 1. ])} 2 { 'x' : tensor ([[ 1. , 1. ]]), 'y' : tensor ([ 0. ])} 3 { 'x' : tensor ([[ 0. , 1. ]]), 'y' : tensor ([ 1. ])} The full set of data utils is available in the docs .","title":"Data Utils"},{"location":"inference/pytorch.html#neural-networks","text":"The PyTorch nn package specifies a set of modules that correspond to different neural network (NN) components and operations. For example, the torch.nn.Linear module defines a linear transform with learnable parameters and the torch.nn.Flatten module flattens two contiguous tensor dimensions. The torch.nn.Sequential module contains a set of modules such as torch.nn.Linear and torch.nn.Sequential , chaining them together to form the forward pass of a forward network. Furthermore, one may specify various pre-implemented loss functions, for example torch.nn.BCELoss and torch.nn.KLDivLoss . The full set of PyTorch NN building blocks is available in the docs . As an example, we can design a simple neural network designed to reproduce the output of the XOR operation on binary inputs. To do so, we can compute a simple NN of the form: \\[\\begin{aligned} x_{in}&\\in\\{0,1\\}^{2}\\\\ l_1 &= \\sigma(W_1^Tx_{in} + b_1); \\ W_1\\in\\mathbb{R}^{2\\times2},\\ b_1\\in\\mathbb{R}^{2}\\\\ l_2 &= \\sigma(W_2^Tx + b_2); \\ W_2\\in\\mathbb{R}^{2},\\ b_1\\in\\mathbb{R}\\\\ \\end{aligned}\\] import torch.nn as nn class Network ( nn . Module ): def __init__ ( self ): super () . __init__ () self . l1 = nn . Linear ( 2 , 2 ) self . l2 = nn . Linear ( 2 , 1 ) def forward ( self , x ): x = torch . sigmoid ( self . l1 ( x )) x = torch . sigmoid ( self . l2 ( x )) return x model = Network () . to ( device ) model ( train_data [ 'x' ]) >>> tensor ([[ 0.5000 ], [ 0.4814 ], [ 0.5148 ], [ 0.4957 ]], grad_fn =< SigmoidBackward > )","title":"Neural Networks"},{"location":"inference/pytorch.html#optimizers","text":"Training a neural network involves minimizing a loss function; classes in the torch.optim package implement various optimization strategies for example stochastic gradient descent and Adam through torch.optim.SGD and torch.optim.Adam respectively. Optimizers are configurable through parameters such as the learning rate (configuring the optimizer's step size). The full set of optimizers and accompanying tutorials are available in the docs . To demonstrate the use of an optimizer, let's train the NN above to produce the results of the XOR operation on binary inputs. Here we'll use the Adam optimizer : from torch import optim from torch.optim.lr_scheduler import StepLR from matplotlib import pyplot as plt # helpful references: # Learning XOR: exploring the space of a classic problem # https://towardsdatascience.com/how-neural-networks-solve-the-xor-problem-59763136bdd7 # https://courses.cs.washington.edu/courses/cse446/18wi/sections/section8/XOR-Pytorch.html # the training function initiates backprop and # steps the optimizer towards the weights that # optimize the loss function def train ( model , train_loader , optimizer , epoch ): model . train () losses = [] for i , batch in enumerate ( train_loader ): optimizer . zero_grad () output = model ( batch [ 'x' ]) y , output = batch [ 'y' ], output . squeeze ( 1 ) # optimize binary cross entropy: # https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html loss = F . binary_cross_entropy ( output , y , reduction = 'mean' ) loss . backward () optimizer . step () losses . append ( loss . item ()) return np . mean ( losses ) # the test function does not adjust the model's weights def test ( model , test_loader ): model . eval () losses , n_correct , n_incorrect = [], 0 , 0 with torch . no_grad (): for i , batch in enumerate ( test_loader ): output = model ( batch [ 'x' ]) y , output = batch [ 'y' ], output . squeeze ( 1 ) loss = F . binary_cross_entropy ( output , y , reduction = 'mean' ) . item () losses . append ( loss ) # determine accuracy by thresholding model output at 0.5 batch_correct = torch . sum ((( output > 0.5 ) & ( y == 1 )) | (( output < 0.5 ) & ( y == 0 ))) batch_incorrect = len ( y ) - batch_correct n_correct += batch_correct n_incorrect += batch_incorrect return np . mean ( losses ), n_correct / ( n_correct + n_incorrect ) # randomly initialize the model's weights for module in model . modules (): if isinstance ( module , nn . Linear ): module . weight . data . normal_ ( 0 , 1 ) # send weights to optimizer lr = 2.5e-2 optimizer = optim . Adam ( model . parameters (), lr = lr ) epochs = 500 for epoch in range ( 1 , epochs + 1 ): train_loss = train ( model , train_loader , optimizer , epoch ) test_loss , test_acc = test ( model , test_loader ) if epoch % 25 == 0 : print ( 'epoch= {} : train_loss= {:.3f} , test_loss= {:.3f} , test_acc= {:.3f} ' . format ( epoch , train_loss , test_loss , test_acc )) >>> epoch = 25 : train_loss = 0.683 , test_loss = 0.681 , test_acc = 0.500 epoch = 50 : train_loss = 0.665 , test_loss = 0.664 , test_acc = 0.750 epoch = 75 : train_loss = 0.640 , test_loss = 0.635 , test_acc = 0.750 epoch = 100 : train_loss = 0.598 , test_loss = 0.595 , test_acc = 0.750 epoch = 125 : train_loss = 0.554 , test_loss = 0.550 , test_acc = 0.750 epoch = 150 : train_loss = 0.502 , test_loss = 0.498 , test_acc = 0.750 epoch = 175 : train_loss = 0.435 , test_loss = 0.432 , test_acc = 0.750 epoch = 200 : train_loss = 0.360 , test_loss = 0.358 , test_acc = 0.750 epoch = 225 : train_loss = 0.290 , test_loss = 0.287 , test_acc = 1.000 epoch = 250 : train_loss = 0.230 , test_loss = 0.228 , test_acc = 1.000 epoch = 275 : train_loss = 0.184 , test_loss = 0.183 , test_acc = 1.000 epoch = 300 : train_loss = 0.149 , test_loss = 0.148 , test_acc = 1.000 epoch = 325 : train_loss = 0.122 , test_loss = 0.122 , test_acc = 1.000 epoch = 350 : train_loss = 0.102 , test_loss = 0.101 , test_acc = 1.000 epoch = 375 : train_loss = 0.086 , test_loss = 0.086 , test_acc = 1.000 epoch = 400 : train_loss = 0.074 , test_loss = 0.073 , test_acc = 1.000 epoch = 425 : train_loss = 0.064 , test_loss = 0.063 , test_acc = 1.000 epoch = 450 : train_loss = 0.056 , test_loss = 0.055 , test_acc = 1.000 epoch = 475 : train_loss = 0.049 , test_loss = 0.049 , test_acc = 1.000 epoch = 500 : train_loss = 0.043 , test_loss = 0.043 , test_acc = 1.000 Here, the model has converged to 100% test accuracy, indicating that it has learned to reproduce the XOR outputs perfectly. Note that even though the test accuracy is 100%, the test loss (BCE) decreases steadily; this is because the BCE loss is nonzero when \\(y_{output}\\) is not exactly 0 or 1, while accuracy is determined by thresholding the model outputs such that each prediction is the boolean \\((y_{output} > 0.5)\\) . This highlights that it is important to choose the correct performance metric for an ML problem. In the case of XOR, perfect test accuracy is sufficient. Let's check that we've recovered the XOR output by extracting the model's weights and using them to build a custom XOR function: for name , param in model . named_parameters (): if param . requires_grad : print ( name , param . data ) >>> l1 . weight tensor ([[ 7.2888 , - 6.4168 ], [ 7.2824 , - 8.1637 ]]) l1 . bias tensor ([ 2.6895 , - 3.9633 ]) l2 . weight tensor ([[ - 6.3500 , 8.0990 ]]) l2 . bias tensor ([ 2.5058 ]) Because our model was built with nn.Linear modules, we have weight matrices and bias terms. Next, we'll hard-code the matrix operations into a custom XOR function based on the architecture of the NN: def XOR ( x ): w1 = torch . tensor ([[ 7.2888 , - 6.4168 ], [ 7.2824 , - 8.1637 ]]) . t () b1 = torch . tensor ([ 2.6895 , - 3.9633 ]) layer1_out = torch . tensor ([ x [ 0 ] * w1 [ 0 , 0 ] + x [ 1 ] * w1 [ 1 , 0 ] + b1 [ 0 ], x [ 0 ] * w1 [ 0 , 1 ] + x [ 1 ] * w1 [ 1 , 1 ] + b1 [ 1 ]]) layer1_out = torch . sigmoid ( layer1_out ) w2 = torch . tensor ([ - 6.3500 , 8.0990 ]) b2 = 2.5058 layer2_out = layer1_out [ 0 ] * w2 [ 0 ] + layer1_out [ 1 ] * w2 [ 1 ] + b2 layer2_out = torch . sigmoid ( layer2_out ) return layer2_out , ( layer2_out > 0.5 ) XOR ([ 0. , 0. ]) >>> ( tensor ( 0.0359 ), tensor ( False )) XOR ([ 0. , 1. ]) >>> ( tensor ( 0.9135 ), tensor ( True )) XOR ([ 1. , 0. ]) >>> ( tensor ( 0.9815 ), tensor ( True )) XOR ([ 1. , 1. ]) >>> ( tensor ( 0.0265 ), tensor ( False )) There we have it - the NN learned XOR!","title":"Optimizers"},{"location":"inference/pytorch.html#pytorch-in-cmssw","text":"","title":"PyTorch in CMSSW"},{"location":"inference/pytorch.html#via-onnx","text":"One way to incorporate your PyTorch models into CMSSW is through the Open Neural Network Exchange (ONNX) Runtime tool. In brief, ONNX supports training and inference for a variety of ML frameworks, and is currently integrated into CMSSW (see the CMS ML tutorial). PyTorch hosts an excellent tutorial on exporting a model from PyTorch to ONNX . ONNX is available in CMSSW (see a relevant discussion in the CMSSW git repo).","title":"Via ONNX"},{"location":"inference/pytorch.html#example-use-cases","text":"The \\(ZZ\\rightarrow 4b\\) analysis utilizes trained PyTorch models via ONNX in CMSSW (see the corresponding repo ). Briefly, they run ONNX in CMSSW_11_X via the CMSSW package PhysicsTools/ONNXRuntime , using it to define a multiClassifierONNX class. This multiclassifier is capable of loading pre-trained PyTorch models specified by a modelFile string as follows: #include \"PhysicsTools/ONNXRuntime/interface/ONNXRuntime.h\" std :: unique_ptr < cms :: Ort :: ONNXRuntime > model ; Ort :: SessionOptions * session_options = new Ort :: SessionOptions (); session_options -> SetIntraOpNumThreads ( 1 ); model = std :: make_unique < cms :: Ort :: ONNXRuntime > ( modelFile , session_options );","title":"Example Use Cases"},{"location":"inference/pytorch.html#via-triton","text":"Coprocessors (GPUs, FPGAs, etc.) are frequently used to accelerate ML operations such as inference and training. In the 'as-a-service' paradigm, users can access cloud-based applications through lightweight client inferfaces. The Services for Optimized Network Inference on Coprocessors ( SONIC ) framework implements this paradigm in CMSSW, allowing the optimal integration of GPUs into event processing workflows. One powerful implementation of SONIC is the the NVIDIA Triton Inference Server, which is flexible with respect to ML framework, storage source, and hardware infrastructure. For more details, see the corresponding NVIDIA developer blog entry . A Graph Attention Network (GAN) is available via Triton in CMSSW, and can be accessed here: https://github.com/cms-sw/cmssw/tree/master/HeterogeneousCore/SonicTriton/test","title":"Via Triton"},{"location":"inference/pytorch.html#training-tips","text":"When instantiating a DataLoader , shuffle=True should be enabled for training data but not for validation and testing data. At each training epoch, this will vary the order of data objects in each batch; accordingly, it is not efficient to load the full dataset (in its original ordering) into GPU memory before training. Instead, enable num_workers>1 ; this allows the DataLoader to load batches to the GPU as they're prepared. Note that this launches muliple threads on the CPU. For more information, see a corresponding discussion in the PyTorch forum.","title":"Training Tips"},{"location":"inference/sonic_triton.html","text":"Service-based inference with Triton/Sonic \u00b6 This page is still under construction. For the moment, please see the Sonic+Triton tutorial given as part of the Machine Learning HATS@LPC 2021. Link to Indico agenda Slides Exercise twiki","title":"Sonic/Triton"},{"location":"inference/sonic_triton.html#service-based-inference-with-tritonsonic","text":"This page is still under construction. For the moment, please see the Sonic+Triton tutorial given as part of the Machine Learning HATS@LPC 2021. Link to Indico agenda Slides Exercise twiki","title":"Service-based inference with Triton/Sonic"},{"location":"inference/standalone.html","text":"Todo. Idea: Working w/ TF+ROOT standalone (outside of CMSSW)","title":"Standalone framework"},{"location":"inference/swan_aws.html","text":"Todo. Ideas: best practices cost model instance priving need to log out monitoring madatory","title":"SWAN + AWS"},{"location":"inference/tensorflow1.html","text":"Direct inference with TensorFlow 1 \u00b6 While it is technically still possible to use TensorFlow 1, this version of TensorFlow is quite old and is no longer supported by CMSSW. We highly recommend that you update your model to TensorFlow 2 and follow the integration guide in the Inference/Direct inference/TensorFlow 2 documentation.","title":"TensorFlow 1"},{"location":"inference/tensorflow1.html#direct-inference-with-tensorflow-1","text":"While it is technically still possible to use TensorFlow 1, this version of TensorFlow is quite old and is no longer supported by CMSSW. We highly recommend that you update your model to TensorFlow 2 and follow the integration guide in the Inference/Direct inference/TensorFlow 2 documentation.","title":"Direct inference with TensorFlow 1"},{"location":"inference/tensorflow2.html","text":"Direct inference with TensorFlow 2 \u00b6 TensorFlow 2 is available since CMSSW_11_1_X ( cmssw#28711 , cmsdist#5525 ). The integration into the software stack can be found in cmsdist/tensorflow.spec and the interface is located in cmssw/PhysicsTools/TensorFlow . The current version is 2.1.0 and, at the moment, only supports inference on CPU. GPU support is planned for the integration of version 2.3. See the guide on inference with TensorFlow 1 for earlier versions. Software setup \u00b6 To run the examples shown below, create a mininmal inference setup with the following snippet. 1 2 3 4 5 6 7 8 9 10 export SCRAM_ARCH = \"slc7_amd64_gcc820\" export CMSSW_VERSION = \"CMSSW_11_1_2\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b Below, the cmsml Python package is used to convert models from TensorFlow objects ( tf.function 's or Keras models) to protobuf graph files ( documentation ). It should be available after executing the commands above. You can check its version via python -c \"import cmsml; print(cmsml.__version__)\" and compare to the released tags . If you want to install a newer version from either the master branch of the cmsml repository or the Python package index (PyPI) , you can simply do that via pip. master # into your user directory (usually ~/.local) pip install --upgrade --user git+https://github.com/cms-ml/cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" git+https://github.com/cms-ml/cmsml PyPI # into your user directory (usually ~/.local) pip install --upgrade --user cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" cmsml Saving your model \u00b6 After successfully training, you should save your model in a protobuf graph file which can be read by the interface in CMSSW. Naturally, you only want to save that part of your model is required to run the network prediction, i.e., it should not contain operations related to model training or loss functions (unless explicitely required). Also, to reduce the memory footprint and to accelerate the inference, variables should be converted to constant tensors. Both of these model transformations are provided by the cmsml package. Instructions on how to transform and save your model are shown below, depending on whether you use Keras or plain TensorFlow with tf.function 's. Keras The code below saves a Keras Model instance as a protobuf graph file using cmsml.tensorflow.save_graph . In order for Keras to built the internal graph representation before saving, make sure to either compile the model, or pass an input_shape to the first layer: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # coding: utf-8 import tensorflow as tf import tf.keras.layers as layers import cmsml # define your model model = tf . keras . Sequential () model . add ( layers . InputLayer ( input_shape = ( 10 ,), name = \"input\" )) model . add ( layers . Dense ( 100 , activation = \"tanh\" )) model . add ( layers . Dense ( 3 , activation = \"softmax\" , name = \"output\" )) # train it ... # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , model , variables_to_constants = True ) Following the Keras naming conventions for certain layers, the input will be named \"input\" while the output is named \"sequential/output/Softmax\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . tf.function Let's consider you write your network model in a single tf.function . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # coding: utf-8 import tensorflow as tf import cmsml # define the model @tf . function def model ( x ): # lift variable initialization to the lowest context so they are # not re-initialized on every call (eager calls or signature tracing) with tf . init_scope (): W = tf . Variable ( tf . ones ([ 10 , 1 ])) b = tf . Variable ( tf . ones ([ 1 ])) # define your \"complex\" model here h = tf . add ( tf . matmul ( x , W ), b ) y = tf . tanh ( h , name = \"y\" ) return y In TensorFlow terms, the model function is polymorphic - it accepts different types of the input tensor x ( tf.float32 , tf.float64 , ...). For each type, TensorFlow will create a concrete function with an associated tf.Graph object. This mechanism is referred to as signature tracing . For deeper insights into tf.function , the concepts of signature tracing, polymorphic and concrete functions, see the guide on Better performance with tf.function . To save the model as a protobuf graph file, you explicitely need to create a concrete function. However, this is fairly easy once you know the exact type and shape of all input arguments. 20 21 22 23 24 25 26 # create a concrete function cmodel = model . get_concrete_function ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 )) # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , cmodel , variables_to_constants = True ) The input will be named \"x\" while the output is named \"y\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . Different method: Frozen signatures Instead of creating a polymorphic tf.function and extracting a concrete one in a second step, you can directly define an input signature upon definition. @tf . function ( input_signature = ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 ),)) def model ( x ): ... This disables signature tracing since the input signature is frozen. However, you can directly pass it to cmsml.tensorflow.save_graph . Inference in CMSSW \u00b6 The inference can be implemented to run in a single thread . In general, this does not mean that the module cannot be executed with multiple threads ( cmsRun --numThreads <N> <CFG_FILE> ), but rather that its performance in terms of evaluation time and especially memory consumption is likely to be suboptimal. Therefore, for modules to be integrated into CMSSW, the multi-threaded implementation is strongly recommended . CMSSW module setup \u00b6 If you aim to use the TensorFlow interface in a CMSSW plugin , make sure to include 1 2 3 <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml file. If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): 1 2 3 4 5 <use name= \"PhysicsTools/TensorFlow\" /> <export> <lib name= \"1\" /> </export> Single-threaded inference \u00b6 Despite tf.Session being removed in the Python interface as of TensorFlow 2, the concepts of Graph 's, containing the constant computational structure and trained variables of your model, Session 's, handling execution, data exchange and device placement, and the separation between them lives on in the C++ interface. Thus, the overall inference approach is 1) include the interface, 2) initialize Graph and session , 3) per event create input tensors and run the inference, and 4) cleanup. 1. Includes \u00b6 1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" // further framework includes ... 2. Initialize objects \u00b6 1 2 3 4 5 6 7 8 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // load the graph definition tensorflow :: GraphDef * graphDef = tensorflow :: loadGraphDef ( \"/path/to/constantgraph.pb\" ); // create a session tensorflow :: Session * session = tensorflow :: createSession ( graphDef ); 3. Inference \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { \"input\" , input } }, { \"output\" }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float 4. Cleanup \u00b6 1 2 tensorflow :: closeSession ( session ); delete graphDef ; Full example \u00b6 Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 /* * Example plugin to demonstrate the direct single-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" class MyPlugin : public edm :: one :: EDAnalyzer <> { public : explicit MyPlugin ( const edm :: ParameterSet & ); ~ MyPlugin (){}; static void fillDescriptions ( edm :: ConfigurationDescriptions & ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string graphPath_ ; std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: GraphDef * graphDef_ ; tensorflow :: Session * session_ ; }; void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config ) : graphPath_ ( config . getParameter < std :: string > ( \"graphPath\" )), inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), graphDef_ ( nullptr ), session_ ( nullptr ) { // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); } void MyPlugin :: beginJob () { // load the graph graphDef_ = tensorflow :: loadGraphDef ( graphPath_ ); // create a new session and add the graphDef session_ = tensorflow :: createSession ( graphDef_ ); } void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); // delete the graph delete graphDef_ ; graphDef_ = nullptr ; } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin ) Multi-threaded inference \u00b6 Compared to the single-threaded implementation above , the multi-threaded version has one major difference: the Graph is no longer a member of a particular module instance, but rather shared between all instances in all threads . This is possible since the Graph is actually a constant object that does not change over the course of the inference process. All volatile, device dependent information is kept in a Session which we keep instantiating per module instance. The Graph on the other hand is stored in a edm :: GlobalCache < T > . See the documentation on the C++ interface of stream modules for details. Thus, the overall inference approach is 1) include the interface, 2) define the edm :: GlobalCache < T > holding the Graph , 3) initialize the Session with the cached Graph , 4) per event create input tensors and run the inference, and 5) cleanup. 1. Includes \u00b6 1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" // further framework includes ... Note that stream/EDAnalyzer.h is included rather than one/EDAnalyzer.h . 2. Define and use the cache \u00b6 The cache definition is done by declaring a simle struct. 1 2 3 4 struct MyCache { MyCache () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; Use it in the edm :: GlobalCache template argument and adjust the plugin accordingly. 1 2 3 4 5 6 7 8 9 class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit GraphLoadingMT ( const edm :: ParameterSet & , const CacheData * ); ~ GraphLoadingMT (); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); ... Implement initializeGlobalCache and globalEndJob to control the behavior of how the cache object is created and destroyed. See the full example below for details. 3. Initialize objects \u00b6 1 2 3 4 5 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // create a session using the graphDef stored in the cache tensorflow :: Session * session = tensorflow :: createSession ( cacheData -> graphDef ); 4. Inference \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float 5. Cleanup \u00b6 1 2 3 4 5 // per module instance tensorflow :: closeSession ( session_ ); // in globalEndJob delete cacheData -> graphDef ; Full example \u00b6 Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 /* * Example plugin to demonstrate the direct multi-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" // define the cache object // it could handle graph loading and destruction on its own, // but in this example, we define it as a logicless container struct CacheData { CacheData () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit MyPlugin ( const edm :: ParameterSet & , const CacheData * ); ~ MyPlugin (){}; static void fillDescriptions ( edm :: ConfigurationDescriptions & ); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: Session * session_ ; }; std :: unique_ptr < CacheData > MyPlugin :: initializeGlobalCache ( const edm :: ParameterSet & config ) { // this method is supposed to create, initialize and return a CacheData instance CacheData * cacheData = new CacheData (); // load the graph def and save it std :: string graphPath = config . getParameter < std :: string > ( \"graphPath\" ); cacheData -> graphDef = tensorflow :: loadGraphDef ( graphPath ); // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); return std :: unique_ptr < CacheData > ( cacheData ); } void MyPlugin :: globalEndJob ( const CacheData * cacheData ) { // reset the graphDef if ( cacheData -> graphDef != nullptr ) { delete cacheData -> graphDef ; } } void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config , const CacheData * cacheData ) : inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), session_ ( tensorflow :: createSession ( cacheData -> graphDef )) {} void MyPlugin :: beginJob () {} void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin ) Optimization \u00b6 Depending on the use case, the following approaches can optimize the inference performance. It could be worth checking them out in your algorithm. Further optimization approaches can be found in the integration checklist . Reusing tensors \u00b6 In some cases, instead of creating new input tensors for each inference call, you might want to store input tensors as members of your plugin. This is of course possible if you know its exact shape a-prioro and comes with the cost of keeping the tensor in memory for the lifetime of your module instance. You can use tensor . flat < float > (). setZero (); to reset the values of your tensor prior to each call. Tensor data access via pointers \u00b6 As shown in the examples above, tensor data can be accessed through methods such as flat<type>() or matrix<type>() which return objects that represent the underlying data in the requested structure ( tensorflow::Tensor C++ API ). To read and manipulate particular elements, you can directly call this object with the coordinates of an element. // matrix returns a 2D representation // set element (b,i) to f tensor . matrix < float > ()( b , i ) = float ( f ); However, doing this for a large input tensor might entail some overhead. Since the data is actually contiguous in memory (C-style \"row-major\" memory ordering), a faster (though less explicit) way of interacting with tensor data is using a pointer. // get the pointer to the first tensor element float * d = tensor . flat < float > (). data (); Now, the tensor data can be filled using simple and fast pointer arithmetic. // fill tensor data using pointer arithmethic // memory ordering is row-major, so the most outer loop corresponds dimension 0 for ( size_t b = 0 ; b < batchSize ; b ++ ) { for ( size_t i = 0 ; i < nFeatures ; i ++ , d ++ ) { // note the d++ * d = float ( i ); } } Inter- and intra-operation parallelism \u00b6 Debugging and local processing only Parallelism between (inter) and within (intra) operations can greatly improve the inference performance. However, this allows TensorFlow to manage and schedule threads on its own, possibly interfering with the thread model inherent to CMSSW. For inference code that is to be officially integrated, you should avoid inter- and intra-op parallelism and rather adhere to the examples shown above. You can configure the amount of inter- and infra-op threads via the second argument of the tensorflow :: createSession method. Simple 1 tensorflow :: Session * session = tensorflow :: createSession ( graphDef , nThreads ); Verbose 1 2 3 4 5 tensorflow :: SessionOptions sessionOptions ; sessionOptions . config . set_intra_op_parallelism_threads ( nThreads ); sessionOptions . config . set_inter_op_parallelism_threads ( nThreads ); tensorflow :: Session * session = tensorflow :: createSession ( graphDef , sessionOptions ); Then, when calling tensorflow :: run , pass the internal name of the TensorFlow threadpool, i.e. \"tensorflow\" , as the last argument. 1 2 3 4 5 6 7 8 std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs , \"tensorflow\" ); Miscellaneous \u00b6 Logging \u00b6 By default, TensorFlow logging is quite verbose. This can be changed by either setting the TF_CPP_MIN_LOG_LEVEL environment varibale before calling cmsRun , or within your code through tensorflow :: setLogging ( level ) . Verbosity level TF_CPP_MIN_LOG_LEVEL debug \"0\" info \"1\" (default) warning \"2\" error \"3\" none \"4\" Forwarding logs to the MessageLogger service is not possible yet. Links and further reading \u00b6 cmsml package CMSSW TensorFlow interface documentation TensorFlow interface header CMSSW process options C++ interface of stream modules TensorFlow TensorFlow 2 tutorial tf.function C++ API tensorflow::Tensor tensorflow::Operation tensorflow::ClientSession Keras API Authors: Marcel Rieger","title":"TensorFlow 2"},{"location":"inference/tensorflow2.html#direct-inference-with-tensorflow-2","text":"TensorFlow 2 is available since CMSSW_11_1_X ( cmssw#28711 , cmsdist#5525 ). The integration into the software stack can be found in cmsdist/tensorflow.spec and the interface is located in cmssw/PhysicsTools/TensorFlow . The current version is 2.1.0 and, at the moment, only supports inference on CPU. GPU support is planned for the integration of version 2.3. See the guide on inference with TensorFlow 1 for earlier versions.","title":"Direct inference with TensorFlow 2"},{"location":"inference/tensorflow2.html#software-setup","text":"To run the examples shown below, create a mininmal inference setup with the following snippet. 1 2 3 4 5 6 7 8 9 10 export SCRAM_ARCH = \"slc7_amd64_gcc820\" export CMSSW_VERSION = \"CMSSW_11_1_2\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b Below, the cmsml Python package is used to convert models from TensorFlow objects ( tf.function 's or Keras models) to protobuf graph files ( documentation ). It should be available after executing the commands above. You can check its version via python -c \"import cmsml; print(cmsml.__version__)\" and compare to the released tags . If you want to install a newer version from either the master branch of the cmsml repository or the Python package index (PyPI) , you can simply do that via pip. master # into your user directory (usually ~/.local) pip install --upgrade --user git+https://github.com/cms-ml/cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" git+https://github.com/cms-ml/cmsml PyPI # into your user directory (usually ~/.local) pip install --upgrade --user cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" cmsml","title":"Software setup"},{"location":"inference/tensorflow2.html#saving-your-model","text":"After successfully training, you should save your model in a protobuf graph file which can be read by the interface in CMSSW. Naturally, you only want to save that part of your model is required to run the network prediction, i.e., it should not contain operations related to model training or loss functions (unless explicitely required). Also, to reduce the memory footprint and to accelerate the inference, variables should be converted to constant tensors. Both of these model transformations are provided by the cmsml package. Instructions on how to transform and save your model are shown below, depending on whether you use Keras or plain TensorFlow with tf.function 's. Keras The code below saves a Keras Model instance as a protobuf graph file using cmsml.tensorflow.save_graph . In order for Keras to built the internal graph representation before saving, make sure to either compile the model, or pass an input_shape to the first layer: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # coding: utf-8 import tensorflow as tf import tf.keras.layers as layers import cmsml # define your model model = tf . keras . Sequential () model . add ( layers . InputLayer ( input_shape = ( 10 ,), name = \"input\" )) model . add ( layers . Dense ( 100 , activation = \"tanh\" )) model . add ( layers . Dense ( 3 , activation = \"softmax\" , name = \"output\" )) # train it ... # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , model , variables_to_constants = True ) Following the Keras naming conventions for certain layers, the input will be named \"input\" while the output is named \"sequential/output/Softmax\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . tf.function Let's consider you write your network model in a single tf.function . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # coding: utf-8 import tensorflow as tf import cmsml # define the model @tf . function def model ( x ): # lift variable initialization to the lowest context so they are # not re-initialized on every call (eager calls or signature tracing) with tf . init_scope (): W = tf . Variable ( tf . ones ([ 10 , 1 ])) b = tf . Variable ( tf . ones ([ 1 ])) # define your \"complex\" model here h = tf . add ( tf . matmul ( x , W ), b ) y = tf . tanh ( h , name = \"y\" ) return y In TensorFlow terms, the model function is polymorphic - it accepts different types of the input tensor x ( tf.float32 , tf.float64 , ...). For each type, TensorFlow will create a concrete function with an associated tf.Graph object. This mechanism is referred to as signature tracing . For deeper insights into tf.function , the concepts of signature tracing, polymorphic and concrete functions, see the guide on Better performance with tf.function . To save the model as a protobuf graph file, you explicitely need to create a concrete function. However, this is fairly easy once you know the exact type and shape of all input arguments. 20 21 22 23 24 25 26 # create a concrete function cmodel = model . get_concrete_function ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 )) # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , cmodel , variables_to_constants = True ) The input will be named \"x\" while the output is named \"y\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . Different method: Frozen signatures Instead of creating a polymorphic tf.function and extracting a concrete one in a second step, you can directly define an input signature upon definition. @tf . function ( input_signature = ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 ),)) def model ( x ): ... This disables signature tracing since the input signature is frozen. However, you can directly pass it to cmsml.tensorflow.save_graph .","title":"Saving your model"},{"location":"inference/tensorflow2.html#inference-in-cmssw","text":"The inference can be implemented to run in a single thread . In general, this does not mean that the module cannot be executed with multiple threads ( cmsRun --numThreads <N> <CFG_FILE> ), but rather that its performance in terms of evaluation time and especially memory consumption is likely to be suboptimal. Therefore, for modules to be integrated into CMSSW, the multi-threaded implementation is strongly recommended .","title":"Inference in CMSSW"},{"location":"inference/tensorflow2.html#cmssw-module-setup","text":"If you aim to use the TensorFlow interface in a CMSSW plugin , make sure to include 1 2 3 <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml file. If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): 1 2 3 4 5 <use name= \"PhysicsTools/TensorFlow\" /> <export> <lib name= \"1\" /> </export>","title":"CMSSW module setup"},{"location":"inference/tensorflow2.html#single-threaded-inference","text":"Despite tf.Session being removed in the Python interface as of TensorFlow 2, the concepts of Graph 's, containing the constant computational structure and trained variables of your model, Session 's, handling execution, data exchange and device placement, and the separation between them lives on in the C++ interface. Thus, the overall inference approach is 1) include the interface, 2) initialize Graph and session , 3) per event create input tensors and run the inference, and 4) cleanup.","title":"Single-threaded inference"},{"location":"inference/tensorflow2.html#1-includes","text":"1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" // further framework includes ...","title":"1. Includes"},{"location":"inference/tensorflow2.html#2-initialize-objects","text":"1 2 3 4 5 6 7 8 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // load the graph definition tensorflow :: GraphDef * graphDef = tensorflow :: loadGraphDef ( \"/path/to/constantgraph.pb\" ); // create a session tensorflow :: Session * session = tensorflow :: createSession ( graphDef );","title":"2. Initialize objects"},{"location":"inference/tensorflow2.html#3-inference","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { \"input\" , input } }, { \"output\" }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float","title":"3. Inference"},{"location":"inference/tensorflow2.html#4-cleanup","text":"1 2 tensorflow :: closeSession ( session ); delete graphDef ;","title":"4. Cleanup"},{"location":"inference/tensorflow2.html#full-example","text":"Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 /* * Example plugin to demonstrate the direct single-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" class MyPlugin : public edm :: one :: EDAnalyzer <> { public : explicit MyPlugin ( const edm :: ParameterSet & ); ~ MyPlugin (){}; static void fillDescriptions ( edm :: ConfigurationDescriptions & ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string graphPath_ ; std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: GraphDef * graphDef_ ; tensorflow :: Session * session_ ; }; void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config ) : graphPath_ ( config . getParameter < std :: string > ( \"graphPath\" )), inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), graphDef_ ( nullptr ), session_ ( nullptr ) { // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); } void MyPlugin :: beginJob () { // load the graph graphDef_ = tensorflow :: loadGraphDef ( graphPath_ ); // create a new session and add the graphDef session_ = tensorflow :: createSession ( graphDef_ ); } void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); // delete the graph delete graphDef_ ; graphDef_ = nullptr ; } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin )","title":"Full example"},{"location":"inference/tensorflow2.html#multi-threaded-inference","text":"Compared to the single-threaded implementation above , the multi-threaded version has one major difference: the Graph is no longer a member of a particular module instance, but rather shared between all instances in all threads . This is possible since the Graph is actually a constant object that does not change over the course of the inference process. All volatile, device dependent information is kept in a Session which we keep instantiating per module instance. The Graph on the other hand is stored in a edm :: GlobalCache < T > . See the documentation on the C++ interface of stream modules for details. Thus, the overall inference approach is 1) include the interface, 2) define the edm :: GlobalCache < T > holding the Graph , 3) initialize the Session with the cached Graph , 4) per event create input tensors and run the inference, and 5) cleanup.","title":"Multi-threaded inference"},{"location":"inference/tensorflow2.html#1-includes_1","text":"1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" // further framework includes ... Note that stream/EDAnalyzer.h is included rather than one/EDAnalyzer.h .","title":"1. Includes"},{"location":"inference/tensorflow2.html#2-define-and-use-the-cache","text":"The cache definition is done by declaring a simle struct. 1 2 3 4 struct MyCache { MyCache () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; Use it in the edm :: GlobalCache template argument and adjust the plugin accordingly. 1 2 3 4 5 6 7 8 9 class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit GraphLoadingMT ( const edm :: ParameterSet & , const CacheData * ); ~ GraphLoadingMT (); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); ... Implement initializeGlobalCache and globalEndJob to control the behavior of how the cache object is created and destroyed. See the full example below for details.","title":"2. Define and use the cache"},{"location":"inference/tensorflow2.html#3-initialize-objects","text":"1 2 3 4 5 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // create a session using the graphDef stored in the cache tensorflow :: Session * session = tensorflow :: createSession ( cacheData -> graphDef );","title":"3. Initialize objects"},{"location":"inference/tensorflow2.html#4-inference","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float","title":"4. Inference"},{"location":"inference/tensorflow2.html#5-cleanup","text":"1 2 3 4 5 // per module instance tensorflow :: closeSession ( session_ ); // in globalEndJob delete cacheData -> graphDef ;","title":"5. Cleanup"},{"location":"inference/tensorflow2.html#full-example_1","text":"Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 /* * Example plugin to demonstrate the direct multi-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" // define the cache object // it could handle graph loading and destruction on its own, // but in this example, we define it as a logicless container struct CacheData { CacheData () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit MyPlugin ( const edm :: ParameterSet & , const CacheData * ); ~ MyPlugin (){}; static void fillDescriptions ( edm :: ConfigurationDescriptions & ); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: Session * session_ ; }; std :: unique_ptr < CacheData > MyPlugin :: initializeGlobalCache ( const edm :: ParameterSet & config ) { // this method is supposed to create, initialize and return a CacheData instance CacheData * cacheData = new CacheData (); // load the graph def and save it std :: string graphPath = config . getParameter < std :: string > ( \"graphPath\" ); cacheData -> graphDef = tensorflow :: loadGraphDef ( graphPath ); // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); return std :: unique_ptr < CacheData > ( cacheData ); } void MyPlugin :: globalEndJob ( const CacheData * cacheData ) { // reset the graphDef if ( cacheData -> graphDef != nullptr ) { delete cacheData -> graphDef ; } } void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config , const CacheData * cacheData ) : inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), session_ ( tensorflow :: createSession ( cacheData -> graphDef )) {} void MyPlugin :: beginJob () {} void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin )","title":"Full example"},{"location":"inference/tensorflow2.html#optimization","text":"Depending on the use case, the following approaches can optimize the inference performance. It could be worth checking them out in your algorithm. Further optimization approaches can be found in the integration checklist .","title":"Optimization"},{"location":"inference/tensorflow2.html#reusing-tensors","text":"In some cases, instead of creating new input tensors for each inference call, you might want to store input tensors as members of your plugin. This is of course possible if you know its exact shape a-prioro and comes with the cost of keeping the tensor in memory for the lifetime of your module instance. You can use tensor . flat < float > (). setZero (); to reset the values of your tensor prior to each call.","title":"Reusing tensors"},{"location":"inference/tensorflow2.html#tensor-data-access-via-pointers","text":"As shown in the examples above, tensor data can be accessed through methods such as flat<type>() or matrix<type>() which return objects that represent the underlying data in the requested structure ( tensorflow::Tensor C++ API ). To read and manipulate particular elements, you can directly call this object with the coordinates of an element. // matrix returns a 2D representation // set element (b,i) to f tensor . matrix < float > ()( b , i ) = float ( f ); However, doing this for a large input tensor might entail some overhead. Since the data is actually contiguous in memory (C-style \"row-major\" memory ordering), a faster (though less explicit) way of interacting with tensor data is using a pointer. // get the pointer to the first tensor element float * d = tensor . flat < float > (). data (); Now, the tensor data can be filled using simple and fast pointer arithmetic. // fill tensor data using pointer arithmethic // memory ordering is row-major, so the most outer loop corresponds dimension 0 for ( size_t b = 0 ; b < batchSize ; b ++ ) { for ( size_t i = 0 ; i < nFeatures ; i ++ , d ++ ) { // note the d++ * d = float ( i ); } }","title":"Tensor data access via pointers"},{"location":"inference/tensorflow2.html#inter-and-intra-operation-parallelism","text":"Debugging and local processing only Parallelism between (inter) and within (intra) operations can greatly improve the inference performance. However, this allows TensorFlow to manage and schedule threads on its own, possibly interfering with the thread model inherent to CMSSW. For inference code that is to be officially integrated, you should avoid inter- and intra-op parallelism and rather adhere to the examples shown above. You can configure the amount of inter- and infra-op threads via the second argument of the tensorflow :: createSession method. Simple 1 tensorflow :: Session * session = tensorflow :: createSession ( graphDef , nThreads ); Verbose 1 2 3 4 5 tensorflow :: SessionOptions sessionOptions ; sessionOptions . config . set_intra_op_parallelism_threads ( nThreads ); sessionOptions . config . set_inter_op_parallelism_threads ( nThreads ); tensorflow :: Session * session = tensorflow :: createSession ( graphDef , sessionOptions ); Then, when calling tensorflow :: run , pass the internal name of the TensorFlow threadpool, i.e. \"tensorflow\" , as the last argument. 1 2 3 4 5 6 7 8 std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs , \"tensorflow\" );","title":"Inter- and intra-operation parallelism"},{"location":"inference/tensorflow2.html#miscellaneous","text":"","title":"Miscellaneous"},{"location":"inference/tensorflow2.html#logging","text":"By default, TensorFlow logging is quite verbose. This can be changed by either setting the TF_CPP_MIN_LOG_LEVEL environment varibale before calling cmsRun , or within your code through tensorflow :: setLogging ( level ) . Verbosity level TF_CPP_MIN_LOG_LEVEL debug \"0\" info \"1\" (default) warning \"2\" error \"3\" none \"4\" Forwarding logs to the MessageLogger service is not possible yet.","title":"Logging"},{"location":"inference/tensorflow2.html#links-and-further-reading","text":"cmsml package CMSSW TensorFlow interface documentation TensorFlow interface header CMSSW process options C++ interface of stream modules TensorFlow TensorFlow 2 tutorial tf.function C++ API tensorflow::Tensor tensorflow::Operation tensorflow::ClientSession Keras API Authors: Marcel Rieger","title":"Links and further reading"},{"location":"inference/tfaas.html","text":"TensorFlow as a Service \u00b6 TensorFlow as a Service (TFaas) was developed as a general purpose service which can be deployed on any infrastruction from personal laptop, VM, to cloud infrastructure, inculding kubernetes/docker based ones. The main repository contains all details about the service, including install , end-to-end example , and demo . For CERN users we already deploy TFaaS on the following URL: https://cms-tfaas.cern.ch It can be used by CMS members using any HTTP based client. For example, here is a basic access from curl client: curl -k https://cms-tfaas.cern.ch/models [ { \"name\": \"luca\", \"model\": \"prova.pb\", \"labels\": \"labels.csv\", \"options\": null, \"inputNode\": \"dense_1_input\", \"outputNode\": \"output_node0\", \"description\": \"\", \"timestamp\": \"2021-10-22 14:04:52.890554036 +0000 UTC m=+600537.976386186\" }, { \"name\": \"test_luca_1024\", \"model\": \"saved_model.pb\", \"labels\": \"labels.txt\", \"options\": null, \"inputNode\": \"dense_input_1:0\", \"outputNode\": \"dense_3/Sigmoid:0\", \"description\": \"\", \"timestamp\": \"2021-10-22 14:04:52.890776518 +0000 UTC m=+600537.976608672\" }, { \"name\": \"vk\", \"model\": \"model.pb\", \"labels\": \"labels.txt\", \"options\": null, \"inputNode\": \"dense_1_input\", \"outputNode\": \"output_node0\", \"description\": \"\", \"timestamp\": \"2021-10-22 14:04:52.890903234 +0000 UTC m=+600537.976735378\" } ] The following APIs are available: - /upload to push your favorite TF model to TFaaS server either for Form or as tar-ball bundle, see examples below - /delete to delete your TF model from TFaaS server - /models to view existing TF models on TFaaS server - /predict/json to serve TF model predictions in JSON data-format - /predict/proto to serve TF model predictions in ProtoBuffer data-format - /predict/image to serve TF model predictions forimages in JPG/PNG formats \u2780 look-up your favorite model \u00b6 You may easily look-up your ML model from TFaaS server, e.g. curl https://cms-tfaas.cern.ch/models # possible output may looks like this [ { \"name\": \"luca\", \"model\": \"prova.pb\", \"labels\": \"labels.csv\", \"options\": null, \"inputNode\": \"dense_1_input\", \"outputNode\": \"output_node0\", \"description\": \"\", \"timestamp\": \"2021-11-08 20:07:18.397487027 +0000 UTC m=+2091094.457327022\" } ... ] The provided /models API will list the name of the model, its file name, labels file, possible options, input and output nodes, description and proper timestamp when it was added to TFaaS repository \u2781 upload your TF model to TFaaS server \u00b6 If your model is not in TFaaS server you may easily add it as following: # example of image based model upload curl -X POST https://cms-tfaas.cern.ch/upload -F 'name=ImageModel' -F 'params=@/path/params.json' -F 'model=@/path/tf_model.pb' -F 'labels=@/path/labels.txt' # example of TF pb file upload curl -s -X POST https://cms-tfaas.cern.ch/upload \\ -F 'name=vk' -F 'params=@/path/params.json' \\ -F 'model=@/path/model.pb' -F 'labels=@/path/labels.txt' # example of bundle upload produce with Keras TF # here is our saved model area ls model assets saved_model.pb variables # we can create tarball and upload it to TFaaS via bundle end-point tar cfz model.tar.gz model curl -X POST -H \"Content-Encoding: gzip\" \\ -H \"content-type: application/octet-stream\" \\ --data-binary @/path/models.tar.gz https://cms-tfaas.cern.ch/upload \u2782 get your predictions \u00b6 Finally, you may obtain predictions from your favorite model by using proper API, e.g. # obtain predictions from your ImageModel curl https://cms-tfaas.cern.ch/image -F 'image=@/path/file.png' -F 'model=ImageModel' # obtain predictions from your TF based model cat input.json {\"keys\": [...], \"values\": [...], \"model\":\"model\"} # call to get predictions from /json end-point using input.json curl -s -X POST -H \"Content-type: application/json\" \\ -d@/path/input.json https://cms-tfaas.cern.ch/json Fore more information please visit curl client page. TFaaS interface \u00b6 Clients communicate with TFaaS via HTTP protocol. See examples for Curl , Python and C++ clients. TFaaS benchmarks \u00b6 Benchmark results on CentOS, 24 cores, 32GB of RAM serving DL NN with 42x128x128x128x64x64x1x1 architecture (JSON and ProtoBuffer formats show similar performance): - 400 req/sec for 100 concurrent clients, 1000 requests in total - 480 req/sec for 200 concurrent clients, 5000 requests in total For more information please visit bencmarks page.","title":"TFaaS"},{"location":"inference/tfaas.html#tensorflow-as-a-service","text":"TensorFlow as a Service (TFaas) was developed as a general purpose service which can be deployed on any infrastruction from personal laptop, VM, to cloud infrastructure, inculding kubernetes/docker based ones. The main repository contains all details about the service, including install , end-to-end example , and demo . For CERN users we already deploy TFaaS on the following URL: https://cms-tfaas.cern.ch It can be used by CMS members using any HTTP based client. For example, here is a basic access from curl client: curl -k https://cms-tfaas.cern.ch/models [ { \"name\": \"luca\", \"model\": \"prova.pb\", \"labels\": \"labels.csv\", \"options\": null, \"inputNode\": \"dense_1_input\", \"outputNode\": \"output_node0\", \"description\": \"\", \"timestamp\": \"2021-10-22 14:04:52.890554036 +0000 UTC m=+600537.976386186\" }, { \"name\": \"test_luca_1024\", \"model\": \"saved_model.pb\", \"labels\": \"labels.txt\", \"options\": null, \"inputNode\": \"dense_input_1:0\", \"outputNode\": \"dense_3/Sigmoid:0\", \"description\": \"\", \"timestamp\": \"2021-10-22 14:04:52.890776518 +0000 UTC m=+600537.976608672\" }, { \"name\": \"vk\", \"model\": \"model.pb\", \"labels\": \"labels.txt\", \"options\": null, \"inputNode\": \"dense_1_input\", \"outputNode\": \"output_node0\", \"description\": \"\", \"timestamp\": \"2021-10-22 14:04:52.890903234 +0000 UTC m=+600537.976735378\" } ] The following APIs are available: - /upload to push your favorite TF model to TFaaS server either for Form or as tar-ball bundle, see examples below - /delete to delete your TF model from TFaaS server - /models to view existing TF models on TFaaS server - /predict/json to serve TF model predictions in JSON data-format - /predict/proto to serve TF model predictions in ProtoBuffer data-format - /predict/image to serve TF model predictions forimages in JPG/PNG formats","title":"TensorFlow as a Service"},{"location":"inference/tfaas.html#look-up-your-favorite-model","text":"You may easily look-up your ML model from TFaaS server, e.g. curl https://cms-tfaas.cern.ch/models # possible output may looks like this [ { \"name\": \"luca\", \"model\": \"prova.pb\", \"labels\": \"labels.csv\", \"options\": null, \"inputNode\": \"dense_1_input\", \"outputNode\": \"output_node0\", \"description\": \"\", \"timestamp\": \"2021-11-08 20:07:18.397487027 +0000 UTC m=+2091094.457327022\" } ... ] The provided /models API will list the name of the model, its file name, labels file, possible options, input and output nodes, description and proper timestamp when it was added to TFaaS repository","title":"&#10112; look-up your favorite model"},{"location":"inference/tfaas.html#upload-your-tf-model-to-tfaas-server","text":"If your model is not in TFaaS server you may easily add it as following: # example of image based model upload curl -X POST https://cms-tfaas.cern.ch/upload -F 'name=ImageModel' -F 'params=@/path/params.json' -F 'model=@/path/tf_model.pb' -F 'labels=@/path/labels.txt' # example of TF pb file upload curl -s -X POST https://cms-tfaas.cern.ch/upload \\ -F 'name=vk' -F 'params=@/path/params.json' \\ -F 'model=@/path/model.pb' -F 'labels=@/path/labels.txt' # example of bundle upload produce with Keras TF # here is our saved model area ls model assets saved_model.pb variables # we can create tarball and upload it to TFaaS via bundle end-point tar cfz model.tar.gz model curl -X POST -H \"Content-Encoding: gzip\" \\ -H \"content-type: application/octet-stream\" \\ --data-binary @/path/models.tar.gz https://cms-tfaas.cern.ch/upload","title":"&#10113; upload your TF model to TFaaS server"},{"location":"inference/tfaas.html#get-your-predictions","text":"Finally, you may obtain predictions from your favorite model by using proper API, e.g. # obtain predictions from your ImageModel curl https://cms-tfaas.cern.ch/image -F 'image=@/path/file.png' -F 'model=ImageModel' # obtain predictions from your TF based model cat input.json {\"keys\": [...], \"values\": [...], \"model\":\"model\"} # call to get predictions from /json end-point using input.json curl -s -X POST -H \"Content-type: application/json\" \\ -d@/path/input.json https://cms-tfaas.cern.ch/json Fore more information please visit curl client page.","title":"&#10114; get your predictions"},{"location":"inference/tfaas.html#tfaas-interface","text":"Clients communicate with TFaaS via HTTP protocol. See examples for Curl , Python and C++ clients.","title":"TFaaS interface"},{"location":"inference/tfaas.html#tfaas-benchmarks","text":"Benchmark results on CentOS, 24 cores, 32GB of RAM serving DL NN with 42x128x128x128x64x64x1x1 architecture (JSON and ProtoBuffer formats show similar performance): - 400 req/sec for 100 concurrent clients, 1000 requests in total - 480 req/sec for 200 concurrent clients, 5000 requests in total For more information please visit bencmarks page.","title":"TFaaS benchmarks"},{"location":"inference/xgboost.html","text":"Direct inference with XGBoost \u00b6 General \u00b6 XGBoost is avaliable (at least) since CMSSW_9_2_4 cmssw#19377 . In CMSSW environment, XGBoost can be used via its Python API . For UL era, there are different verisons available for different SCRAM_ARCH : For slc7_amd64_gcc700 and above, ver.0.80 is available. For slc7_amd64_gcc900 and above, ver.1.3.3 is available. Please note that different major versions have different behavior( See Caveat Session). Existing Examples \u00b6 There are some existing good examples of using XGBoost under CMSSW, as listed below: Offical sample for testing the integration of XGBoost library with CMSSW. Useful codes created by Dr. Huilin Qu for inference with existing trained model. C/C++ Interface for inference with existing trained model. We will provide examples for both C/C++ interface and python interface of XGBoost under CMSSW environment. Example: Classification of points from joint-Gaussian distribution. \u00b6 In this specific example, you will use XGBoost to classify data points generated from two 8-dimension joint-Gaussian distribution. Feature Index 0 1 2 3 4 5 6 7 \u03bc 1 1 2 3 4 5 6 7 8 \u03bc 2 0 1.9 3.2 4.5 4.8 6.1 8.1 11 \u03c3 \u00bd = \u03c3 1 1 1 1 1 1 1 1 |\u03bc 1 - \u03bc 2 | / \u03c3 1 0.1 0.2 0.5 0.2 0.1 1.1 3 All generated data points for train(1:10000,2:10000) and test(1:1000,2:1000) are stored as Train_data.csv / Test_data.csv . Preparing Model \u00b6 The training process of a XGBoost model can be done outside of CMSSW. We provide a python script for illustration. # importing necessary models import numpy as np import pandas as pd from xgboost import XGBClassifier # Or XGBRegressor for Logistic Regression import matplotlib.pyplot as plt import pandas as pd # specify parameters via map param = { 'n_estimators' : 50 } xgb = XGBClassifier ( param ) # using Pandas.DataFrame data-format, other available format are XGBoost's DMatrix and numpy.ndarray train_data = pd . read_csv ( \"path/to/the/data\" ) # The training dataset is code/XGBoost/Train_data.csv train_Variable = train_data [ '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' ] train_Score = train_data [ 'Type' ] # Score should be integer, 0, 1, (2 and larger for multiclass) test_data = pd . read_csv ( \"path/to/the/data\" ) # The testing dataset is code/XGBoost/Test_data.csv test_Variable = test_data [ '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' ] test_Score = test_data [ 'Type' ] # Now the data are well prepared and named as train_Variable, train_Score and test_Variable, test_Score. xgb . fit ( train_Variable , train_Score ) # Training xgb . predict ( test_Variable ) # Outputs are integers xgb . predict_proba ( test_Variable ) # Output scores , output structre: [prob for 0, prob for 1,...] xgb . save_model ( \"\\Path\\To\\Where\\You\\Want\\ModelName.model\" ) # Saving model The saved model ModelName.model is thus available for python and C/C++ api to load. Please use the XGBoost major version consistently (see Caveat ). While training with data from different datasets, proper treatment of weights are necessary for better model performance. Please refer to Official Recommendation for more details. C/C++ Usage with CMSSW \u00b6 To use a saved XGBoost model with C/C++ code, it is convenient to use the XGBoost's offical C api . Here we provide a simple example as following. Module setup \u00b6 There is no official CMSSW interface for XGBoost while its library are placed in cvmfs of CMSSW. Thus we have to use the raw c_api as well as setting up the library manually. To run XGBoost's c_api within CMSSW framework, in addition to the following standard setup. export SCRAM_ARCH = \"slc7_amd64_gcc700\" # To use higher version, please switch to slc7_amd64_900 export CMSSW_VERSION = \"CMSSW_X_Y_Z\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b The addtional effort is to add corresponding xml file(s) to $CMSSW_BASE/toolbox$CMSSW_BASE/config/toolbox/$SCRAM_ARCH/tools/selected/ for setting up XGBoost. For lower version (<1), add two xml files as below. xgboost.xml <tool name= \"xgboost\" version= \"0.80\" > <lib name= \"xgboost\" /> <client> <environment name= \"LIBDIR\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/lib\" /> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> <use name= \"rabit\" /> </tool> rabit.xml <tool name= \"rabit\" version= \"0.80\" > <client> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/rabit/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> </tool> Please note that the path in cvmfs is not fixed, one can list all available versions in the py2-xgboost directory and choose one to use. For higher version (>=1), and one xml file xgboost.xml <tool name= \"xgboost\" version= \"0.80\" > <lib name= \"xgboost\" /> <client> <environment name= \"LIBDIR\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/xgboost/1.3.3/lib64\" /> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/xgboost/1.3.3/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> </tool> Also one has the freedom to choose the available xgboost version inside xgboost directory. After adding xml file(s), the following commands should be executed for setting up. For lower version (<1), use scram setup rabit scram setup xgboost For higher version (>=1), use scram setup xgboost For using XGBoost as a plugin of CMSSW, it is necessary to add <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml . If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): <use name= \"xgboost\" /> <export> <lib name= \"1\" /> </export> The libxgboost.so would be too large to load for cmsRun job, please using the following commands for pre-loading: export LD_PRELOAD = $CMSSW_BASE /external/ $SCRAM_ARCH /lib/libxgboost.so Basic Usage of C API \u00b6 In order to use c_api of XGBoost to load model and operate inference, one should construct necessaries objects: Files to include #include <xgboost/c_api.h> BoosterHandle : worker of XGBoost // Declare Object BoosterHandle booster_ ; // Allocate memory in C style XGBoosterCreate ( NULL , 0 , & booster_ ); // Load Model XGBoosterLoadModel ( booster_ , model_path . c_str ()); // second argument should be a const char *. DMatrixHandle : handle to dmatrix, the data format of XGBoost float TestData [ 2000 ][ 8 ] // Suppose 2000 data points, each data point has 8 dimension // Assign data to the \"TestData\" 2d array ... // Declare object DMatrixHandle data_ ; // Allocate memory and use external float array to initialize XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); // The first argument takes in float * namely 1d float array only, 2nd & 3rd: shape of input, 4th: value to replace missing ones XGBoosterPredict : function for inference bst_ulong outlen ; // bst_ulong is a typedef of unsigned long const float * f ; // array to store predictions XGBoosterPredict ( booster_ , data_ , 0 , 0 , & out_len , & f ); // lower version API // XGBoosterPredict(booster_,data_,0,0,0,&out_len,&f);// higher version API /* lower version (ver.<1) API XGB_DLL int XGBoosterPredict( BoosterHandle handle, DMatrixHandle dmat, int option_mask, // 0 for normal output, namely reporting scores int training, // 0 for prediction bst_ulong * out_len, const float ** out_result ) higher version (ver.>=1) API XGB_DLL int XGBoosterPredict( BoosterHandle handle, DMatrixHandle dmat, int option_mask, // 0 for normal output, namely reporting scores int ntree_limit, // how many trees for prediction, set to 0 means no limit int training, // 0 for prediction bst_ulong * out_len, const float ** out_result ) */ Full Example \u00b6 Click to expand full example The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 XGBoostExample.cc \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 python/ \u2502 \u2514\u2500\u2500 xgboost_cfg.py \u2502 \u251c\u2500\u2500 toolbox/ (storing necessary xml(s) to be copied to toolbox/ of $CMSSW_BASE) \u2502 \u2514\u2500\u2500 xgboost.xml \u2502 \u2514\u2500\u2500 rabit.xml (lower version only) \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 Test_data.csv \u2514\u2500\u2500 lowVer.model / highVer.model Please also note that in order to operate inference in an event-by-event way, please put XGBoosterPredict in analyze rather than beginJob . plugins/XGBoostExample.cc for lower version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 // -*- C++ -*- // // Package: XGB_Example/XGBoostExample // Class: XGBoostExample // /**\\class XGBoostExample XGBoostExample.cc XGB_Example/XGBoostExample/plugins/XGBoostExample.cc Description: [one line class summary] Implementation: [Notes on implementation] */ // // Original Author: Qian Sitian // Created: Sat, 19 Jun 2021 08:38:51 GMT // // // system include files #include <memory> // user include files #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"FWCore/Utilities/interface/InputTag.h\" #include \"DataFormats/TrackReco/interface/Track.h\" #include \"DataFormats/TrackReco/interface/TrackFwd.h\" #include <xgboost/c_api.h> #include <vector> #include <tuple> #include <string> #include <iostream> #include <fstream> #include <sstream> using namespace std ; vector < vector < double >> readinCSV ( const char * name ){ auto fin = ifstream ( name ); vector < vector < double >> floatVec ; string strFloat ; float fNum ; int counter = 0 ; getline ( fin , strFloat ); while ( getline ( fin , strFloat )) { std :: stringstream linestream ( strFloat ); floatVec . push_back ( std :: vector < double > ()); while ( linestream >> fNum ) { floatVec [ counter ]. push_back ( fNum ); if ( linestream . peek () == ',' ) linestream . ignore (); } ++ counter ; } return floatVec ; } // // class declaration // // If the analyzer does not use TFileService, please remove // the template argument to the base class so the class inherits // from edm::one::EDAnalyzer<> // This will improve performance in multithreaded jobs. class XGBoostExample : public edm :: one :: EDAnalyzer <> { public : explicit XGBoostExample ( const edm :: ParameterSet & ); ~ XGBoostExample (); static void fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ); private : virtual void beginJob () ; virtual void analyze ( const edm :: Event & , const edm :: EventSetup & ) ; virtual void endJob () ; // ----------member data --------------------------- std :: string test_data_path ; std :: string model_path ; }; // // constants, enums and typedefs // // // static data member definitions // // // constructors and destructor // XGBoostExample :: XGBoostExample ( const edm :: ParameterSet & config ) : test_data_path ( config . getParameter < std :: string > ( \"test_data_path\" )), model_path ( config . getParameter < std :: string > ( \"model_path\" )) { } XGBoostExample ::~ XGBoostExample () { // do anything here that needs to be done at desctruction time // (e.g. close files, deallocate resources etc.) } // // member functions // void XGBoostExample :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { } void XGBoostExample :: beginJob () { BoosterHandle booster_ ; XGBoosterCreate ( NULL , 0 , & booster_ ); cout << \"Hello World No.2\" << endl ; XGBoosterLoadModel ( booster_ , model_path . c_str ()); unsigned long numFeature = 0 ; cout << \"Hello World No.3\" << endl ; vector < vector < double >> TestDataVector = readinCSV ( test_data_path . c_str ()); cout << \"Hello World No.4\" << endl ; float TestData [ 2000 ][ 8 ]; cout << \"Hello World No.5\" << endl ; for ( unsigned i = 0 ; ( i < 2000 ); i ++ ) { for ( unsigned j = 0 ; ( j < 8 ); j ++ ) { TestData [ i ][ j ] = TestDataVector [ i ][ j ]; // cout<<TestData[i][j]<<\"\\t\"; } //cout<<endl; } cout << \"Hello World No.6\" << endl ; DMatrixHandle data_ ; XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); cout << \"Hello World No.7\" << endl ; bst_ulong out_len = 0 ; const float * f ; cout << out_len << endl ; auto ret = XGBoosterPredict ( booster_ , data_ , 0 , 0 , & out_len , & f ); cout << ret << endl ; for ( unsigned int i = 0 ; i < 2 ; i ++ ) std :: cout << i << \" \\t \" << f [ i ] << std :: endl ; cout << \"Hello World No.8\" << endl ; } void XGBoostExample :: endJob () { } void XGBoostExample :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { //The following says we do not know what parameters are allowed so do no validation // Please change this to state exactly what you do use, even if it is no parameters edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"test_data_path\" ); desc . add < std :: string > ( \"model_path\" ); descriptions . addWithDefaultLabel ( desc ); //Specify that only 'tracks' is allowed //To use, remove the default given above and uncomment below //ParameterSetDescription desc; //desc.addUntracked<edm::InputTag>(\"tracks\",\"ctfWithMaterialTracks\"); //descriptions.addDefault(desc); } //define this as a plug-in DEFINE_FWK_MODULE ( XGBoostExample ); plugins/BuildFile.xml for lower version XGBoost 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"DataFormats/TrackReco\" /> <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> python/xgboost_cfg.py for lower version XGBoost plugins/XGBoostExample.cc for higher version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 // -*- C++ -*- // // Package: XGB_Example/XGBoostExample // Class: XGBoostExample // /**\\class XGBoostExample XGBoostExample.cc XGB_Example/XGBoostExample/plugins/XGBoostExample.cc Description: [one line class summary] Implementation: [Notes on implementation] */ // // Original Author: Qian Sitian // Created: Sat, 19 Jun 2021 08:38:51 GMT // // // system include files #include <memory> // user include files #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"FWCore/Utilities/interface/InputTag.h\" #include \"DataFormats/TrackReco/interface/Track.h\" #include \"DataFormats/TrackReco/interface/TrackFwd.h\" #include <xgboost/c_api.h> #include <vector> #include <tuple> #include <string> #include <iostream> #include <fstream> #include <sstream> using namespace std ; vector < vector < double >> readinCSV ( const char * name ){ auto fin = ifstream ( name ); vector < vector < double >> floatVec ; string strFloat ; float fNum ; int counter = 0 ; getline ( fin , strFloat ); while ( getline ( fin , strFloat )) { std :: stringstream linestream ( strFloat ); floatVec . push_back ( std :: vector < double > ()); while ( linestream >> fNum ) { floatVec [ counter ]. push_back ( fNum ); if ( linestream . peek () == ',' ) linestream . ignore (); } ++ counter ; } return floatVec ; } // // class declaration // // If the analyzer does not use TFileService, please remove // the template argument to the base class so the class inherits // from edm::one::EDAnalyzer<> // This will improve performance in multithreaded jobs. class XGBoostExample : public edm :: one :: EDAnalyzer <> { public : explicit XGBoostExample ( const edm :: ParameterSet & ); ~ XGBoostExample (); static void fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ); private : virtual void beginJob () ; virtual void analyze ( const edm :: Event & , const edm :: EventSetup & ) ; virtual void endJob () ; // ----------member data --------------------------- std :: string test_data_path ; std :: string model_path ; }; // // constants, enums and typedefs // // // static data member definitions // // // constructors and destructor // XGBoostExample :: XGBoostExample ( const edm :: ParameterSet & config ) : test_data_path ( config . getParameter < std :: string > ( \"test_data_path\" )), model_path ( config . getParameter < std :: string > ( \"model_path\" )) { } XGBoostExample ::~ XGBoostExample () { // do anything here that needs to be done at desctruction time // (e.g. close files, deallocate resources etc.) } // // member functions // void XGBoostExample :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { } void XGBoostExample :: beginJob () { BoosterHandle booster_ ; XGBoosterCreate ( NULL , 0 , & booster_ ); XGBoosterLoadModel ( booster_ , model_path . c_str ()); unsigned long numFeature = 0 ; vector < vector < double >> TestDataVector = readinCSV ( test_data_path . c_str ()); float TestData [ 2000 ][ 8 ]; for ( unsigned i = 0 ; ( i < 2000 ); i ++ ) { for ( unsigned j = 0 ; ( j < 8 ); j ++ ) { TestData [ i ][ j ] = TestDataVector [ i ][ j ]; // cout<<TestData[i][j]<<\"\\t\"; } //cout<<endl; } DMatrixHandle data_ ; XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); bst_ulong out_len = 0 ; const float * f ; auto ret = XGBoosterPredict ( booster_ , data_ , 0 , 0 , 0 , & out_len , & f ); for ( unsigned int i = 0 ; i < out_len ; i ++ ) std :: cout << i << \" \\t \" << f [ i ] << std :: endl ; } void XGBoostExample :: endJob () { } void XGBoostExample :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { //The following says we do not know what parameters are allowed so do no validation // Please change this to state exactly what you do use, even if it is no parameters edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"test_data_path\" ); desc . add < std :: string > ( \"model_path\" ); descriptions . addWithDefaultLabel ( desc ); //Specify that only 'tracks' is allowed //To use, remove the default given above and uncomment below //ParameterSetDescription desc; //desc.addUntracked<edm::InputTag>(\"tracks\",\"ctfWithMaterialTracks\"); //descriptions.addDefault(desc); } //define this as a plug-in DEFINE_FWK_MODULE ( XGBoostExample ); plugins/BuildFile.xml for higher version XGBoost 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"DataFormats/TrackReco\" /> <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> python/xgboost_cfg.py for higher version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # setup minimal options #options = VarParsing(\"python\") #options.setDefault(\"inputFiles\", \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\") # noqa #options.parseArguments() # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) #process.source = cms.Source(\"PoolSource\", # fileNames=cms.untracked.vstring('file:/afs/cern.ch/cms/Tutorials/TWIKI_DATA/TTJets_8TeV_53X.root')) process . source = cms . Source ( \"EmptySource\" ) #process.source = cms.Source(\"PoolSource\", # fileNames=cms.untracked.vstring(options.inputFiles)) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) process . XGBoostExample = cms . EDAnalyzer ( \"XGBoostExample\" ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) #process.load(\"XGB_Example.XGBoostExample.XGBoostExample_cfi\") process . XGBoostExample . model_path = cms . string ( \"/Your/Path/data/highVer.model\" ) process . XGBoostExample . test_data_path = cms . string ( \"/Your/Path/data/Test_data.csv\" ) # define what to run in the path process . p = cms . Path ( process . XGBoostExample ) Python Usage \u00b6 To use XGBoost's python interface, using the snippet below under CMSSW environment # importing necessary models import numpy as np import pandas as pd from xgboost import XGBClassifier import matplotlib.pyplot as plt import pandas as pd xgb = XGBClassifier () xgb . load_model ( 'ModelName.model' ) # After loading model, usage is the same as discussed in the model preparation section. Caveat \u00b6 It is worth mentioning that both behavior and APIs of different XGBoost version can have difference. When using c_api for C/C++ inference, for ver.<1 , the API is XGB_DLL int XGBoosterPredict(BoosterHandle handle, DMatrixHandle dmat,int option_mask, int training, bst_ulong * out_len,const float ** out_result) , while for ver.>=1 the API changes to XGB_DLL int XGBoosterPredict(BoosterHandle handle, DMatrixHandle dmat,int option_mask, unsigned int ntree_limit, int training, bst_ulong * out_len,const float ** out_result) . Model from ver.>=1 cannot be used for ver.<1 . Other important issue for C/C++ user is that DMatrix only takes in single precision floats ( float ), not double precision floats ( double ). Appendix: Tips for XGBoost users \u00b6 Importance Plot \u00b6 XGBoost uses F-score to describe feature importance quantatitively. XGBoost's python API provides a nice tool, plot_importance , to plot the feature importance conveniently after finishing train . # Once the training is done, the plot_importance function can thus be used to plot the feature importance. from xgboost import plot_importance # Import the function plot_importance ( xgb ) # suppose the xgboost object is named \"xgb\" plt . savefig ( \"importance_plot.pdf\" ) # plot_importance is based on matplotlib, so the plot can be saved use plt.savefig() The importance plot is consistent with our expectation, as in our toy-model, the data points differ by most on the feature \"7\". (see toy model setup ). ROC Curve and AUC \u00b6 The receiver operating characteristic (ROC) and auccrency (AUC) are key quantities to describe the model performance. For XGBoost, ROC curve and auc score can be easily obtained with the help of sci-kit learn (sklearn) functionals, which is also in CMSSW software. from sklearn.metrics import roc_auc_score , roc_curve , auc # ROC and AUC should be obtained on test set # Suppose the ground truth is 'y_test', and the output score is named as 'y_score' fpr , tpr , _ = roc_curve ( y_test , y_score ) roc_auc = auc ( fpr , tpr ) plt . figure () lw = 2 plt . plot ( fpr , tpr , color = 'darkorange' , lw = lw , label = 'ROC curve (area = %0.2f )' % roc_auc ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], color = 'navy' , lw = lw , linestyle = '--' ) plt . xlim ([ 0.0 , 1.0 ]) plt . ylim ([ 0.0 , 1.05 ]) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . title ( 'Receiver operating characteristic example' ) plt . legend ( loc = \"lower right\" ) # plt.show() # display the figure when not using jupyter display plt . savefig ( \"roc.png\" ) # resulting plot is shown below Reference of XGBoost \u00b6 XGBoost Wiki: https://en.wikipedia.org/wiki/XGBoost XGBoost Github Repo.: https://github.com/dmlc/xgboost XGBoost offical api tutorial Latest, Python: https://xgboost.readthedocs.io/en/latest/python/index.html Latest, C/C++: https://xgboost.readthedocs.io/en/latest/tutorials/c_api_tutorial.html Older (0.80), Python: https://xgboost.readthedocs.io/en/release_0.80/python/index.html No Tutorial for older version C/C++ api, source code: https://github.com/dmlc/xgboost/blob/release_0.80/src/c_api/c_api.cc","title":"XGBoost"},{"location":"inference/xgboost.html#direct-inference-with-xgboost","text":"","title":"Direct inference with XGBoost"},{"location":"inference/xgboost.html#general","text":"XGBoost is avaliable (at least) since CMSSW_9_2_4 cmssw#19377 . In CMSSW environment, XGBoost can be used via its Python API . For UL era, there are different verisons available for different SCRAM_ARCH : For slc7_amd64_gcc700 and above, ver.0.80 is available. For slc7_amd64_gcc900 and above, ver.1.3.3 is available. Please note that different major versions have different behavior( See Caveat Session).","title":"General"},{"location":"inference/xgboost.html#existing-examples","text":"There are some existing good examples of using XGBoost under CMSSW, as listed below: Offical sample for testing the integration of XGBoost library with CMSSW. Useful codes created by Dr. Huilin Qu for inference with existing trained model. C/C++ Interface for inference with existing trained model. We will provide examples for both C/C++ interface and python interface of XGBoost under CMSSW environment.","title":"Existing Examples"},{"location":"inference/xgboost.html#example-classification-of-points-from-joint-gaussian-distribution","text":"In this specific example, you will use XGBoost to classify data points generated from two 8-dimension joint-Gaussian distribution. Feature Index 0 1 2 3 4 5 6 7 \u03bc 1 1 2 3 4 5 6 7 8 \u03bc 2 0 1.9 3.2 4.5 4.8 6.1 8.1 11 \u03c3 \u00bd = \u03c3 1 1 1 1 1 1 1 1 |\u03bc 1 - \u03bc 2 | / \u03c3 1 0.1 0.2 0.5 0.2 0.1 1.1 3 All generated data points for train(1:10000,2:10000) and test(1:1000,2:1000) are stored as Train_data.csv / Test_data.csv .","title":"Example: Classification of points from joint-Gaussian distribution."},{"location":"inference/xgboost.html#preparing-model","text":"The training process of a XGBoost model can be done outside of CMSSW. We provide a python script for illustration. # importing necessary models import numpy as np import pandas as pd from xgboost import XGBClassifier # Or XGBRegressor for Logistic Regression import matplotlib.pyplot as plt import pandas as pd # specify parameters via map param = { 'n_estimators' : 50 } xgb = XGBClassifier ( param ) # using Pandas.DataFrame data-format, other available format are XGBoost's DMatrix and numpy.ndarray train_data = pd . read_csv ( \"path/to/the/data\" ) # The training dataset is code/XGBoost/Train_data.csv train_Variable = train_data [ '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' ] train_Score = train_data [ 'Type' ] # Score should be integer, 0, 1, (2 and larger for multiclass) test_data = pd . read_csv ( \"path/to/the/data\" ) # The testing dataset is code/XGBoost/Test_data.csv test_Variable = test_data [ '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' ] test_Score = test_data [ 'Type' ] # Now the data are well prepared and named as train_Variable, train_Score and test_Variable, test_Score. xgb . fit ( train_Variable , train_Score ) # Training xgb . predict ( test_Variable ) # Outputs are integers xgb . predict_proba ( test_Variable ) # Output scores , output structre: [prob for 0, prob for 1,...] xgb . save_model ( \"\\Path\\To\\Where\\You\\Want\\ModelName.model\" ) # Saving model The saved model ModelName.model is thus available for python and C/C++ api to load. Please use the XGBoost major version consistently (see Caveat ). While training with data from different datasets, proper treatment of weights are necessary for better model performance. Please refer to Official Recommendation for more details.","title":"Preparing Model"},{"location":"inference/xgboost.html#cc-usage-with-cmssw","text":"To use a saved XGBoost model with C/C++ code, it is convenient to use the XGBoost's offical C api . Here we provide a simple example as following.","title":"C/C++ Usage with CMSSW"},{"location":"inference/xgboost.html#module-setup","text":"There is no official CMSSW interface for XGBoost while its library are placed in cvmfs of CMSSW. Thus we have to use the raw c_api as well as setting up the library manually. To run XGBoost's c_api within CMSSW framework, in addition to the following standard setup. export SCRAM_ARCH = \"slc7_amd64_gcc700\" # To use higher version, please switch to slc7_amd64_900 export CMSSW_VERSION = \"CMSSW_X_Y_Z\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b The addtional effort is to add corresponding xml file(s) to $CMSSW_BASE/toolbox$CMSSW_BASE/config/toolbox/$SCRAM_ARCH/tools/selected/ for setting up XGBoost. For lower version (<1), add two xml files as below. xgboost.xml <tool name= \"xgboost\" version= \"0.80\" > <lib name= \"xgboost\" /> <client> <environment name= \"LIBDIR\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/lib\" /> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> <use name= \"rabit\" /> </tool> rabit.xml <tool name= \"rabit\" version= \"0.80\" > <client> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/rabit/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> </tool> Please note that the path in cvmfs is not fixed, one can list all available versions in the py2-xgboost directory and choose one to use. For higher version (>=1), and one xml file xgboost.xml <tool name= \"xgboost\" version= \"0.80\" > <lib name= \"xgboost\" /> <client> <environment name= \"LIBDIR\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/xgboost/1.3.3/lib64\" /> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/xgboost/1.3.3/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> </tool> Also one has the freedom to choose the available xgboost version inside xgboost directory. After adding xml file(s), the following commands should be executed for setting up. For lower version (<1), use scram setup rabit scram setup xgboost For higher version (>=1), use scram setup xgboost For using XGBoost as a plugin of CMSSW, it is necessary to add <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml . If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): <use name= \"xgboost\" /> <export> <lib name= \"1\" /> </export> The libxgboost.so would be too large to load for cmsRun job, please using the following commands for pre-loading: export LD_PRELOAD = $CMSSW_BASE /external/ $SCRAM_ARCH /lib/libxgboost.so","title":"Module setup"},{"location":"inference/xgboost.html#basic-usage-of-c-api","text":"In order to use c_api of XGBoost to load model and operate inference, one should construct necessaries objects: Files to include #include <xgboost/c_api.h> BoosterHandle : worker of XGBoost // Declare Object BoosterHandle booster_ ; // Allocate memory in C style XGBoosterCreate ( NULL , 0 , & booster_ ); // Load Model XGBoosterLoadModel ( booster_ , model_path . c_str ()); // second argument should be a const char *. DMatrixHandle : handle to dmatrix, the data format of XGBoost float TestData [ 2000 ][ 8 ] // Suppose 2000 data points, each data point has 8 dimension // Assign data to the \"TestData\" 2d array ... // Declare object DMatrixHandle data_ ; // Allocate memory and use external float array to initialize XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); // The first argument takes in float * namely 1d float array only, 2nd & 3rd: shape of input, 4th: value to replace missing ones XGBoosterPredict : function for inference bst_ulong outlen ; // bst_ulong is a typedef of unsigned long const float * f ; // array to store predictions XGBoosterPredict ( booster_ , data_ , 0 , 0 , & out_len , & f ); // lower version API // XGBoosterPredict(booster_,data_,0,0,0,&out_len,&f);// higher version API /* lower version (ver.<1) API XGB_DLL int XGBoosterPredict( BoosterHandle handle, DMatrixHandle dmat, int option_mask, // 0 for normal output, namely reporting scores int training, // 0 for prediction bst_ulong * out_len, const float ** out_result ) higher version (ver.>=1) API XGB_DLL int XGBoosterPredict( BoosterHandle handle, DMatrixHandle dmat, int option_mask, // 0 for normal output, namely reporting scores int ntree_limit, // how many trees for prediction, set to 0 means no limit int training, // 0 for prediction bst_ulong * out_len, const float ** out_result ) */","title":"Basic Usage of C API"},{"location":"inference/xgboost.html#full-example","text":"Click to expand full example The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 XGBoostExample.cc \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 python/ \u2502 \u2514\u2500\u2500 xgboost_cfg.py \u2502 \u251c\u2500\u2500 toolbox/ (storing necessary xml(s) to be copied to toolbox/ of $CMSSW_BASE) \u2502 \u2514\u2500\u2500 xgboost.xml \u2502 \u2514\u2500\u2500 rabit.xml (lower version only) \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 Test_data.csv \u2514\u2500\u2500 lowVer.model / highVer.model Please also note that in order to operate inference in an event-by-event way, please put XGBoosterPredict in analyze rather than beginJob . plugins/XGBoostExample.cc for lower version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 // -*- C++ -*- // // Package: XGB_Example/XGBoostExample // Class: XGBoostExample // /**\\class XGBoostExample XGBoostExample.cc XGB_Example/XGBoostExample/plugins/XGBoostExample.cc Description: [one line class summary] Implementation: [Notes on implementation] */ // // Original Author: Qian Sitian // Created: Sat, 19 Jun 2021 08:38:51 GMT // // // system include files #include <memory> // user include files #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"FWCore/Utilities/interface/InputTag.h\" #include \"DataFormats/TrackReco/interface/Track.h\" #include \"DataFormats/TrackReco/interface/TrackFwd.h\" #include <xgboost/c_api.h> #include <vector> #include <tuple> #include <string> #include <iostream> #include <fstream> #include <sstream> using namespace std ; vector < vector < double >> readinCSV ( const char * name ){ auto fin = ifstream ( name ); vector < vector < double >> floatVec ; string strFloat ; float fNum ; int counter = 0 ; getline ( fin , strFloat ); while ( getline ( fin , strFloat )) { std :: stringstream linestream ( strFloat ); floatVec . push_back ( std :: vector < double > ()); while ( linestream >> fNum ) { floatVec [ counter ]. push_back ( fNum ); if ( linestream . peek () == ',' ) linestream . ignore (); } ++ counter ; } return floatVec ; } // // class declaration // // If the analyzer does not use TFileService, please remove // the template argument to the base class so the class inherits // from edm::one::EDAnalyzer<> // This will improve performance in multithreaded jobs. class XGBoostExample : public edm :: one :: EDAnalyzer <> { public : explicit XGBoostExample ( const edm :: ParameterSet & ); ~ XGBoostExample (); static void fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ); private : virtual void beginJob () ; virtual void analyze ( const edm :: Event & , const edm :: EventSetup & ) ; virtual void endJob () ; // ----------member data --------------------------- std :: string test_data_path ; std :: string model_path ; }; // // constants, enums and typedefs // // // static data member definitions // // // constructors and destructor // XGBoostExample :: XGBoostExample ( const edm :: ParameterSet & config ) : test_data_path ( config . getParameter < std :: string > ( \"test_data_path\" )), model_path ( config . getParameter < std :: string > ( \"model_path\" )) { } XGBoostExample ::~ XGBoostExample () { // do anything here that needs to be done at desctruction time // (e.g. close files, deallocate resources etc.) } // // member functions // void XGBoostExample :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { } void XGBoostExample :: beginJob () { BoosterHandle booster_ ; XGBoosterCreate ( NULL , 0 , & booster_ ); cout << \"Hello World No.2\" << endl ; XGBoosterLoadModel ( booster_ , model_path . c_str ()); unsigned long numFeature = 0 ; cout << \"Hello World No.3\" << endl ; vector < vector < double >> TestDataVector = readinCSV ( test_data_path . c_str ()); cout << \"Hello World No.4\" << endl ; float TestData [ 2000 ][ 8 ]; cout << \"Hello World No.5\" << endl ; for ( unsigned i = 0 ; ( i < 2000 ); i ++ ) { for ( unsigned j = 0 ; ( j < 8 ); j ++ ) { TestData [ i ][ j ] = TestDataVector [ i ][ j ]; // cout<<TestData[i][j]<<\"\\t\"; } //cout<<endl; } cout << \"Hello World No.6\" << endl ; DMatrixHandle data_ ; XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); cout << \"Hello World No.7\" << endl ; bst_ulong out_len = 0 ; const float * f ; cout << out_len << endl ; auto ret = XGBoosterPredict ( booster_ , data_ , 0 , 0 , & out_len , & f ); cout << ret << endl ; for ( unsigned int i = 0 ; i < 2 ; i ++ ) std :: cout << i << \" \\t \" << f [ i ] << std :: endl ; cout << \"Hello World No.8\" << endl ; } void XGBoostExample :: endJob () { } void XGBoostExample :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { //The following says we do not know what parameters are allowed so do no validation // Please change this to state exactly what you do use, even if it is no parameters edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"test_data_path\" ); desc . add < std :: string > ( \"model_path\" ); descriptions . addWithDefaultLabel ( desc ); //Specify that only 'tracks' is allowed //To use, remove the default given above and uncomment below //ParameterSetDescription desc; //desc.addUntracked<edm::InputTag>(\"tracks\",\"ctfWithMaterialTracks\"); //descriptions.addDefault(desc); } //define this as a plug-in DEFINE_FWK_MODULE ( XGBoostExample ); plugins/BuildFile.xml for lower version XGBoost 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"DataFormats/TrackReco\" /> <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> python/xgboost_cfg.py for lower version XGBoost plugins/XGBoostExample.cc for higher version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 // -*- C++ -*- // // Package: XGB_Example/XGBoostExample // Class: XGBoostExample // /**\\class XGBoostExample XGBoostExample.cc XGB_Example/XGBoostExample/plugins/XGBoostExample.cc Description: [one line class summary] Implementation: [Notes on implementation] */ // // Original Author: Qian Sitian // Created: Sat, 19 Jun 2021 08:38:51 GMT // // // system include files #include <memory> // user include files #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"FWCore/Utilities/interface/InputTag.h\" #include \"DataFormats/TrackReco/interface/Track.h\" #include \"DataFormats/TrackReco/interface/TrackFwd.h\" #include <xgboost/c_api.h> #include <vector> #include <tuple> #include <string> #include <iostream> #include <fstream> #include <sstream> using namespace std ; vector < vector < double >> readinCSV ( const char * name ){ auto fin = ifstream ( name ); vector < vector < double >> floatVec ; string strFloat ; float fNum ; int counter = 0 ; getline ( fin , strFloat ); while ( getline ( fin , strFloat )) { std :: stringstream linestream ( strFloat ); floatVec . push_back ( std :: vector < double > ()); while ( linestream >> fNum ) { floatVec [ counter ]. push_back ( fNum ); if ( linestream . peek () == ',' ) linestream . ignore (); } ++ counter ; } return floatVec ; } // // class declaration // // If the analyzer does not use TFileService, please remove // the template argument to the base class so the class inherits // from edm::one::EDAnalyzer<> // This will improve performance in multithreaded jobs. class XGBoostExample : public edm :: one :: EDAnalyzer <> { public : explicit XGBoostExample ( const edm :: ParameterSet & ); ~ XGBoostExample (); static void fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ); private : virtual void beginJob () ; virtual void analyze ( const edm :: Event & , const edm :: EventSetup & ) ; virtual void endJob () ; // ----------member data --------------------------- std :: string test_data_path ; std :: string model_path ; }; // // constants, enums and typedefs // // // static data member definitions // // // constructors and destructor // XGBoostExample :: XGBoostExample ( const edm :: ParameterSet & config ) : test_data_path ( config . getParameter < std :: string > ( \"test_data_path\" )), model_path ( config . getParameter < std :: string > ( \"model_path\" )) { } XGBoostExample ::~ XGBoostExample () { // do anything here that needs to be done at desctruction time // (e.g. close files, deallocate resources etc.) } // // member functions // void XGBoostExample :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { } void XGBoostExample :: beginJob () { BoosterHandle booster_ ; XGBoosterCreate ( NULL , 0 , & booster_ ); XGBoosterLoadModel ( booster_ , model_path . c_str ()); unsigned long numFeature = 0 ; vector < vector < double >> TestDataVector = readinCSV ( test_data_path . c_str ()); float TestData [ 2000 ][ 8 ]; for ( unsigned i = 0 ; ( i < 2000 ); i ++ ) { for ( unsigned j = 0 ; ( j < 8 ); j ++ ) { TestData [ i ][ j ] = TestDataVector [ i ][ j ]; // cout<<TestData[i][j]<<\"\\t\"; } //cout<<endl; } DMatrixHandle data_ ; XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); bst_ulong out_len = 0 ; const float * f ; auto ret = XGBoosterPredict ( booster_ , data_ , 0 , 0 , 0 , & out_len , & f ); for ( unsigned int i = 0 ; i < out_len ; i ++ ) std :: cout << i << \" \\t \" << f [ i ] << std :: endl ; } void XGBoostExample :: endJob () { } void XGBoostExample :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { //The following says we do not know what parameters are allowed so do no validation // Please change this to state exactly what you do use, even if it is no parameters edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"test_data_path\" ); desc . add < std :: string > ( \"model_path\" ); descriptions . addWithDefaultLabel ( desc ); //Specify that only 'tracks' is allowed //To use, remove the default given above and uncomment below //ParameterSetDescription desc; //desc.addUntracked<edm::InputTag>(\"tracks\",\"ctfWithMaterialTracks\"); //descriptions.addDefault(desc); } //define this as a plug-in DEFINE_FWK_MODULE ( XGBoostExample ); plugins/BuildFile.xml for higher version XGBoost 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"DataFormats/TrackReco\" /> <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> python/xgboost_cfg.py for higher version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # setup minimal options #options = VarParsing(\"python\") #options.setDefault(\"inputFiles\", \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\") # noqa #options.parseArguments() # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) #process.source = cms.Source(\"PoolSource\", # fileNames=cms.untracked.vstring('file:/afs/cern.ch/cms/Tutorials/TWIKI_DATA/TTJets_8TeV_53X.root')) process . source = cms . Source ( \"EmptySource\" ) #process.source = cms.Source(\"PoolSource\", # fileNames=cms.untracked.vstring(options.inputFiles)) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) process . XGBoostExample = cms . EDAnalyzer ( \"XGBoostExample\" ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) #process.load(\"XGB_Example.XGBoostExample.XGBoostExample_cfi\") process . XGBoostExample . model_path = cms . string ( \"/Your/Path/data/highVer.model\" ) process . XGBoostExample . test_data_path = cms . string ( \"/Your/Path/data/Test_data.csv\" ) # define what to run in the path process . p = cms . Path ( process . XGBoostExample )","title":"Full Example"},{"location":"inference/xgboost.html#python-usage","text":"To use XGBoost's python interface, using the snippet below under CMSSW environment # importing necessary models import numpy as np import pandas as pd from xgboost import XGBClassifier import matplotlib.pyplot as plt import pandas as pd xgb = XGBClassifier () xgb . load_model ( 'ModelName.model' ) # After loading model, usage is the same as discussed in the model preparation section.","title":"Python Usage"},{"location":"inference/xgboost.html#caveat","text":"It is worth mentioning that both behavior and APIs of different XGBoost version can have difference. When using c_api for C/C++ inference, for ver.<1 , the API is XGB_DLL int XGBoosterPredict(BoosterHandle handle, DMatrixHandle dmat,int option_mask, int training, bst_ulong * out_len,const float ** out_result) , while for ver.>=1 the API changes to XGB_DLL int XGBoosterPredict(BoosterHandle handle, DMatrixHandle dmat,int option_mask, unsigned int ntree_limit, int training, bst_ulong * out_len,const float ** out_result) . Model from ver.>=1 cannot be used for ver.<1 . Other important issue for C/C++ user is that DMatrix only takes in single precision floats ( float ), not double precision floats ( double ).","title":"Caveat"},{"location":"inference/xgboost.html#appendix-tips-for-xgboost-users","text":"","title":"Appendix: Tips for XGBoost users"},{"location":"inference/xgboost.html#importance-plot","text":"XGBoost uses F-score to describe feature importance quantatitively. XGBoost's python API provides a nice tool, plot_importance , to plot the feature importance conveniently after finishing train . # Once the training is done, the plot_importance function can thus be used to plot the feature importance. from xgboost import plot_importance # Import the function plot_importance ( xgb ) # suppose the xgboost object is named \"xgb\" plt . savefig ( \"importance_plot.pdf\" ) # plot_importance is based on matplotlib, so the plot can be saved use plt.savefig() The importance plot is consistent with our expectation, as in our toy-model, the data points differ by most on the feature \"7\". (see toy model setup ).","title":"Importance Plot"},{"location":"inference/xgboost.html#roc-curve-and-auc","text":"The receiver operating characteristic (ROC) and auccrency (AUC) are key quantities to describe the model performance. For XGBoost, ROC curve and auc score can be easily obtained with the help of sci-kit learn (sklearn) functionals, which is also in CMSSW software. from sklearn.metrics import roc_auc_score , roc_curve , auc # ROC and AUC should be obtained on test set # Suppose the ground truth is 'y_test', and the output score is named as 'y_score' fpr , tpr , _ = roc_curve ( y_test , y_score ) roc_auc = auc ( fpr , tpr ) plt . figure () lw = 2 plt . plot ( fpr , tpr , color = 'darkorange' , lw = lw , label = 'ROC curve (area = %0.2f )' % roc_auc ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], color = 'navy' , lw = lw , linestyle = '--' ) plt . xlim ([ 0.0 , 1.0 ]) plt . ylim ([ 0.0 , 1.05 ]) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . title ( 'Receiver operating characteristic example' ) plt . legend ( loc = \"lower right\" ) # plt.show() # display the figure when not using jupyter display plt . savefig ( \"roc.png\" ) # resulting plot is shown below","title":"ROC Curve and AUC"},{"location":"inference/xgboost.html#reference-of-xgboost","text":"XGBoost Wiki: https://en.wikipedia.org/wiki/XGBoost XGBoost Github Repo.: https://github.com/dmlc/xgboost XGBoost offical api tutorial Latest, Python: https://xgboost.readthedocs.io/en/latest/python/index.html Latest, C/C++: https://xgboost.readthedocs.io/en/latest/tutorials/c_api_tutorial.html Older (0.80), Python: https://xgboost.readthedocs.io/en/release_0.80/python/index.html No Tutorial for older version C/C++ api, source code: https://github.com/dmlc/xgboost/blob/release_0.80/src/c_api/c_api.cc","title":"Reference of XGBoost"},{"location":"innovation/hackathons.html","text":"CMS Machine Learning Hackathons \u00b6 Welcome to the CMS ML Hackathons! Here we encourage the exploration of cutting edge ML methods to particle physics problems through multi-day focused work. Form hackathon teams and work together with the ML Innovation group to get support with organization and announcements, hardware/software infrastructure, follow-up meetings and ML-related technical advise. If you are interested in proposing a hackathon, please send an e-mail to the CMS ML Innovation conveners with a potential topic and we will get in touch! Below follows a list of previous successful hackathons. HGCAL TICL reconstruction \u00b6 20 Jun 2022 - 24 Jun 2022 https://indico.cern.ch/e/ticlhack Abstract: The HGCAL reconstruction relies on \u201cThe Iterative CLustering\u201d (TICL) framework. It follows an iterative approach, first clusters energy deposits in the same layer (layer clusters) and then connect these layer clusters to reconstruct the particle shower by forming 3-D objects, the \u201ctracksters\u201d. There are multiple areas that could benefit from advanced ML techniques to further improve the reconstruction performance. In this project we plan to tackle the following topics using ML: trackster identification (ie, identification of the type of particle initiating the shower) and energy regression linking of tracksters stemming from the same particle to reconstruct the full shower and/or use a high-purity trackster as a seed and collect 2D (ie. layer clusters) and/or 3D (ie, tracksters) energy deposits in the vicinity of the seed trackster to fully reconstruct the particle shower tuning of the existing pattern recognition algorithms reconstruction under HL-LHC pile-up scenarios (eg., PU=150-200) trackster characterization, ie. predict if a trackster is a sound object in itself or determine if it is more likely to be a composite one. Material: \u00b6 A CodiMD document has been created with an overview of the topics and to keep track of the activities during the hackathon: https://codimd.web.cern.ch/s/hMd74Yi7J Jet tagging \u00b6 8 Nov 2021 - 11 Nov 2021 https://indico.cern.ch/e/jethack Abstract: The identification of the initial particle (quark, gluon, W/Z boson, etc..) responsible for the formation of the jet, also known as jet tagging, provides a powerful handle in both standard model (SM) measurements and searches for physics beyond the SM (BSM). In this project we propose the development of jet tagging algorithms both for small-radius (i.e. AK4) and large-radius (i.e., AK8) jets using as inputs the PF candidates. Two main projects are covered: Jet tagging for scouting Jet tagging for Level-1 Jet tagging for scouting \u00b6 Using as inputs the PF candidates and local pixel tracks reconstructed in the scouting streams, the main goals of this project are the following: Develop a jet-tagging baseline for scouting and compare the performance with the offline reconstruction Understand the importance of the different input variables and the impact of -various configurations (e.g., on pixel track reconstruction) in the performance Compare different jet tagging approaches with mind performance as well as inference time. Proof of concept: ggF H->bb, ggF HH->4b, VBF HH->4b Jet tagging for Level-1 \u00b6 Using as input the newly developed particle flow candidates of Seeded Cone jets in the Level1 Correlator trigger, the following tasks will be worked on: Developing a quark, gluon, b, pileup jet classifier for Seeded Cone R=0.4 jets using a combination of tt,VBF(H) and Drell-Yan Level1 samples Develop tools to demonstrate the gain of such a jet tagging algorithm on a signal sample (like q vs g on VBF jets) Study tagging performance as a function of the number of jet constituents Study tagging performance for a \"real\" input vector (zero-paddes, perhaps unsorted) Optimise jet constituent list of SeededCone Jets (N constituents, zero-removal, sorting etc) Develop q/g/W/Z/t/H classifier for Seeded Cone R=0.8 jets GNN-4-tracking \u00b6 27 Sept 2021 - 1 Oct 2021 https://indico.cern.ch/e/gnn4tracks Abstract: The aim of this hackathon is to integrate graph neural nets (GNNs) for particle tracking into CMSSW. The hackathon will make use of a GNN model reported by the paper Charged particle tracking via edge-classifying interaction networks by Gage DeZoort, Savannah Thais, et.al. They used a GNN to predict connections between detector pixel hits, and achieved accurate track building. They did this with the TrackML dataset, which uses a generic detector designed to be similar to CMS or ATLAS. Work is ongoing to apply this GNN approach to CMS data. Tasks: The hackathon aims to create a workflow that allows graph building and GNN inference within the framework of CMSSW. This would enable accurate testing of future GNN models and comparison to existing CMSSW track building methods. The hackathon will be divided into the following subtasks: Task 1: Create a package for extracting graph features and building graphs in CMSSW. Task 2. GNN inference on Sonic servers Task 3: Track fitting after GNN track building Task 4. Performance evaluation for the new track collection Material: \u00b6 Code is provided at this GitHub organisation . Project are listed here . Anomaly detection \u00b6 In this four day Machine Learning Hackathon, we will develop new anomaly detection algorithms for New Physics detection, intended for deployment in the two main stages of the CMS data aquisition system: The Level-1 trigger and the High Level Trigger. There are two main projects: Event-based anomaly detection algorithms for the Level-1 Trigger \u00b6 Jet-based anomaly detection algorithms for the High Level Trigger, specifically targeting Run 3 scouting \u00b6 Material: \u00b6 A list of projects can be found in this document . Instructions for fetching the data and example code for the two projects can be found at Level-1 Anomaly Detection .","title":"ML Hackathons"},{"location":"innovation/hackathons.html#cms-machine-learning-hackathons","text":"Welcome to the CMS ML Hackathons! Here we encourage the exploration of cutting edge ML methods to particle physics problems through multi-day focused work. Form hackathon teams and work together with the ML Innovation group to get support with organization and announcements, hardware/software infrastructure, follow-up meetings and ML-related technical advise. If you are interested in proposing a hackathon, please send an e-mail to the CMS ML Innovation conveners with a potential topic and we will get in touch! Below follows a list of previous successful hackathons.","title":"CMS Machine Learning Hackathons"},{"location":"innovation/hackathons.html#hgcal-ticl-reconstruction","text":"20 Jun 2022 - 24 Jun 2022 https://indico.cern.ch/e/ticlhack Abstract: The HGCAL reconstruction relies on \u201cThe Iterative CLustering\u201d (TICL) framework. It follows an iterative approach, first clusters energy deposits in the same layer (layer clusters) and then connect these layer clusters to reconstruct the particle shower by forming 3-D objects, the \u201ctracksters\u201d. There are multiple areas that could benefit from advanced ML techniques to further improve the reconstruction performance. In this project we plan to tackle the following topics using ML: trackster identification (ie, identification of the type of particle initiating the shower) and energy regression linking of tracksters stemming from the same particle to reconstruct the full shower and/or use a high-purity trackster as a seed and collect 2D (ie. layer clusters) and/or 3D (ie, tracksters) energy deposits in the vicinity of the seed trackster to fully reconstruct the particle shower tuning of the existing pattern recognition algorithms reconstruction under HL-LHC pile-up scenarios (eg., PU=150-200) trackster characterization, ie. predict if a trackster is a sound object in itself or determine if it is more likely to be a composite one.","title":"HGCAL TICL reconstruction"},{"location":"innovation/hackathons.html#material","text":"A CodiMD document has been created with an overview of the topics and to keep track of the activities during the hackathon: https://codimd.web.cern.ch/s/hMd74Yi7J","title":"Material:"},{"location":"innovation/hackathons.html#jet-tagging","text":"8 Nov 2021 - 11 Nov 2021 https://indico.cern.ch/e/jethack Abstract: The identification of the initial particle (quark, gluon, W/Z boson, etc..) responsible for the formation of the jet, also known as jet tagging, provides a powerful handle in both standard model (SM) measurements and searches for physics beyond the SM (BSM). In this project we propose the development of jet tagging algorithms both for small-radius (i.e. AK4) and large-radius (i.e., AK8) jets using as inputs the PF candidates. Two main projects are covered: Jet tagging for scouting Jet tagging for Level-1","title":"Jet tagging"},{"location":"innovation/hackathons.html#jet-tagging-for-scouting","text":"Using as inputs the PF candidates and local pixel tracks reconstructed in the scouting streams, the main goals of this project are the following: Develop a jet-tagging baseline for scouting and compare the performance with the offline reconstruction Understand the importance of the different input variables and the impact of -various configurations (e.g., on pixel track reconstruction) in the performance Compare different jet tagging approaches with mind performance as well as inference time. Proof of concept: ggF H->bb, ggF HH->4b, VBF HH->4b","title":"Jet tagging for scouting"},{"location":"innovation/hackathons.html#jet-tagging-for-level-1","text":"Using as input the newly developed particle flow candidates of Seeded Cone jets in the Level1 Correlator trigger, the following tasks will be worked on: Developing a quark, gluon, b, pileup jet classifier for Seeded Cone R=0.4 jets using a combination of tt,VBF(H) and Drell-Yan Level1 samples Develop tools to demonstrate the gain of such a jet tagging algorithm on a signal sample (like q vs g on VBF jets) Study tagging performance as a function of the number of jet constituents Study tagging performance for a \"real\" input vector (zero-paddes, perhaps unsorted) Optimise jet constituent list of SeededCone Jets (N constituents, zero-removal, sorting etc) Develop q/g/W/Z/t/H classifier for Seeded Cone R=0.8 jets","title":"Jet tagging for Level-1"},{"location":"innovation/hackathons.html#gnn-4-tracking","text":"27 Sept 2021 - 1 Oct 2021 https://indico.cern.ch/e/gnn4tracks Abstract: The aim of this hackathon is to integrate graph neural nets (GNNs) for particle tracking into CMSSW. The hackathon will make use of a GNN model reported by the paper Charged particle tracking via edge-classifying interaction networks by Gage DeZoort, Savannah Thais, et.al. They used a GNN to predict connections between detector pixel hits, and achieved accurate track building. They did this with the TrackML dataset, which uses a generic detector designed to be similar to CMS or ATLAS. Work is ongoing to apply this GNN approach to CMS data. Tasks: The hackathon aims to create a workflow that allows graph building and GNN inference within the framework of CMSSW. This would enable accurate testing of future GNN models and comparison to existing CMSSW track building methods. The hackathon will be divided into the following subtasks: Task 1: Create a package for extracting graph features and building graphs in CMSSW. Task 2. GNN inference on Sonic servers Task 3: Track fitting after GNN track building Task 4. Performance evaluation for the new track collection","title":"GNN-4-tracking"},{"location":"innovation/hackathons.html#material_1","text":"Code is provided at this GitHub organisation . Project are listed here .","title":"Material:"},{"location":"innovation/hackathons.html#anomaly-detection","text":"In this four day Machine Learning Hackathon, we will develop new anomaly detection algorithms for New Physics detection, intended for deployment in the two main stages of the CMS data aquisition system: The Level-1 trigger and the High Level Trigger. There are two main projects:","title":"Anomaly detection"},{"location":"innovation/hackathons.html#event-based-anomaly-detection-algorithms-for-the-level-1-trigger","text":"","title":"Event-based anomaly detection algorithms for the Level-1 Trigger"},{"location":"innovation/hackathons.html#jet-based-anomaly-detection-algorithms-for-the-high-level-trigger-specifically-targeting-run-3-scouting","text":"","title":"Jet-based anomaly detection algorithms for the High Level Trigger, specifically targeting Run 3 scouting"},{"location":"innovation/hackathons.html#material_2","text":"A list of projects can be found in this document . Instructions for fetching the data and example code for the two projects can be found at Level-1 Anomaly Detection .","title":"Material:"},{"location":"innovation/journal_club.html","text":"CMS Machine Learning Journal Club \u00b6 Welcome to the CMS Machine Learning Journal Club (JC)! Here we read an discuss new cutting edge ML papers, with an emphasis on how these can be used within the collaboration. Below you can find a summary of each JC as well as some code examples demonstrating how to use the tools or methods introduced. To vote for or to propose new papers for discussion, go to https://cms-ml-journalclub.web.cern.ch/ . Below follows a complete list of all the previous CMS ML JHournal clubs, together with relevant documentation and code examples. Dealing with Nuisance Parameters using Machine Learning in High Energy Physics: a Review \u00b6 Tommaso Dorigo, Pablo de Castro Abstract: In this work we discuss the impact of nuisance parameters on the effectiveness of machine learning in high-energy physics problems, and provide a review of techniques that allow to include their effect and reduce their impact in the search for optimal selection criteria and variable transformations. The introduction of nuisance parameters complicates the supervised learning task and its correspondence with the data analysis goal, due to their contribution degrading the model performances in real data, and the necessary addition of uncertainties in the resulting statistical inference. The approaches discussed include nuisance-parameterized models, modified or adversary losses, semi-supervised learning approaches, and inference-aware techniques. Indico Paper Mapping Machine-Learned Physics into a Human-Readable Space \u00b6 Taylor Faucett, Jesse Thaler, Daniel Whiteson Abstract: We present a technique for translating a black-box machine-learned classifier operating on a high-dimensional input space into a small set of human-interpretable observables that can be combined to make the same classification decisions. We iteratively select these observables from a large space of high-level discriminants by finding those with the highest decision similarity relative to the black box, quantified via a metric we introduce that evaluates the relative ordering of pairs of inputs. Successive iterations focus only on the subset of input pairs that are misordered by the current set of observables. This method enables simplification of the machine-learning strategy, interpretation of the results in terms of well-understood physical concepts, validation of the physical model, and the potential for new insights into the nature of the problem itself. As a demonstration, we apply our approach to the benchmark task of jet classification in collider physics, where a convolutional neural network acting on calorimeter jet images outperforms a set of six well-known jet substructure observables. Our method maps the convolutional neural network into a set of observables called energy flow polynomials, and it closes the performance gap by identifying a class of observables with an interesting physical interpretation that has been previously overlooked in the jet substructure literature. - Indico - Paper Model Interpretability (2 papers): \u00b6 Indico Identifying the relevant dependencies of the neural network response on characteristics of the input space \u00b6 Stefan Wunsch, Raphael Friese, Roger Wolf, G\u00fcnter Quast Abstract: The relation between the input and output spaces of neural networks (NNs) is investigated to identify those characteristics of the input space that have a large influence on the output for a given task. For this purpose, the NN function is decomposed into a Taylor expansion in each element of the input space. The Taylor coefficients contain information about the sensitivity of the NN response to the inputs. A metric is introduced that allows for the identification of the characteristics that mostly determine the performance of the NN in solving a given task. Finally, the capability of this metric to analyze the performance of the NN is evaluated based on a task common to data analyses in high-energy particle physics experiments. Paper iNNvestigate neural networks! \u00b6 Maximilian Alber, Sebastian Lapuschkin, Philipp Seegerer, Miriam H\u00e4gele, Kristof T. Sch\u00fctt, Gr\u00e9goire Montavon, Wojciech Samek, Klaus-Robert M\u00fcller, Sven D\u00e4hne, Pieter-Jan Kindermans In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and pre- dictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this short- coming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the- box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures. Paper Code Simulation-based inference in particle physics and beyond (and beyond) \u00b6 Johann Brehmer, Kyle Cranmer Abstract: Our predictions for particle physics processes are realized in a chain of complex simulators. They allow us to generate high-fidelity simulated data, but they are not well-suited for inference on the theory parameters with observed data. We explain why the likelihood function of high-dimensional LHC data cannot be explicitly evaluated, why this matters for data analysis, and reframe what the field has traditionally done to circumvent this problem. We then review new simulation-based inference methods that let us directly analyze high-dimensional data by combining machine learning techniques and information from the simulator. Initial studies indicate that these techniques have the potential to substantially improve the precision of LHC measurements. Finally, we discuss probabilistic programming, an emerging paradigm that lets us extend inference to the latent process of the simulator. Indico Paper Code Efficiency Parameterization with Neural Networks \u00b6 C. Badiali, F.A. Di Bello, G. Frattari, E. Gross, V. Ippolito, M. Kado, J. Shlomi Abstract: Multidimensional efficiency maps are commonly used in high energy physics experiments to mitigate the limitations in the generation of large samples of simulated events. Binned multidimensional efficiency maps are however strongly limited by statistics. We propose a neural network approach to learn ratios of local densities to estimate in an optimal fashion efficiencies as a function of a set of parameters. Graph neural network techniques are used to account for the high dimensional correlations between different physics objects in the event. We show in a specific toy model how this method is applicable to produce accurate multidimensional efficiency maps for heavy flavor tagging classifiers in HEP experiments, including for processes on which it was not trained. - Indico - Paper - Code A General Framework for Uncertainty Estimation in Deep Learning \u00b6 Antonio Loquercio, Mattia Seg\u00f9, Davide Scaramuzza Neural networks predictions are unreliable when the input sample is out of the training distribution or corrupted by noise. Being able to detect such failures automatically is fundamental to integrate deep learning algorithms into robotics. Current approaches for uncertainty estimation of neural networks require changes to the network and optimization process, typically ignore prior knowledge about the data, and tend to make over-simplifying assumptions which underestimate uncertainty. To address these limitations, we propose a novel framework for uncertainty estimation. Based on Bayesian belief networks and Monte-Carlo sampling, our framework not only fully models the different sources of prediction uncertainty, but also incorporates prior data information, e.g. sensor noise. We show theoretically that this gives us the ability to capture uncertainty better than existing methods. In addition, our framework has several desirable properties: (i) it is agnostic to the network architecture and task; (ii) it does not require changes in the optimization process; (iii) it can be applied to already trained architectures. We thoroughly validate the proposed framework through extensive experiments on both computer vision and control tasks, where we outperform previous methods by up to 23% in accuracy. Indico Paper Code","title":"ML Journal Club"},{"location":"innovation/journal_club.html#cms-machine-learning-journal-club","text":"Welcome to the CMS Machine Learning Journal Club (JC)! Here we read an discuss new cutting edge ML papers, with an emphasis on how these can be used within the collaboration. Below you can find a summary of each JC as well as some code examples demonstrating how to use the tools or methods introduced. To vote for or to propose new papers for discussion, go to https://cms-ml-journalclub.web.cern.ch/ . Below follows a complete list of all the previous CMS ML JHournal clubs, together with relevant documentation and code examples.","title":"CMS Machine Learning Journal Club"},{"location":"innovation/journal_club.html#dealing-with-nuisance-parameters-using-machine-learning-in-high-energy-physics-a-review","text":"Tommaso Dorigo, Pablo de Castro Abstract: In this work we discuss the impact of nuisance parameters on the effectiveness of machine learning in high-energy physics problems, and provide a review of techniques that allow to include their effect and reduce their impact in the search for optimal selection criteria and variable transformations. The introduction of nuisance parameters complicates the supervised learning task and its correspondence with the data analysis goal, due to their contribution degrading the model performances in real data, and the necessary addition of uncertainties in the resulting statistical inference. The approaches discussed include nuisance-parameterized models, modified or adversary losses, semi-supervised learning approaches, and inference-aware techniques. Indico Paper","title":"Dealing with Nuisance Parameters using Machine Learning in High Energy Physics: a Review"},{"location":"innovation/journal_club.html#mapping-machine-learned-physics-into-a-human-readable-space","text":"Taylor Faucett, Jesse Thaler, Daniel Whiteson Abstract: We present a technique for translating a black-box machine-learned classifier operating on a high-dimensional input space into a small set of human-interpretable observables that can be combined to make the same classification decisions. We iteratively select these observables from a large space of high-level discriminants by finding those with the highest decision similarity relative to the black box, quantified via a metric we introduce that evaluates the relative ordering of pairs of inputs. Successive iterations focus only on the subset of input pairs that are misordered by the current set of observables. This method enables simplification of the machine-learning strategy, interpretation of the results in terms of well-understood physical concepts, validation of the physical model, and the potential for new insights into the nature of the problem itself. As a demonstration, we apply our approach to the benchmark task of jet classification in collider physics, where a convolutional neural network acting on calorimeter jet images outperforms a set of six well-known jet substructure observables. Our method maps the convolutional neural network into a set of observables called energy flow polynomials, and it closes the performance gap by identifying a class of observables with an interesting physical interpretation that has been previously overlooked in the jet substructure literature. - Indico - Paper","title":"Mapping Machine-Learned Physics into a Human-Readable Space"},{"location":"innovation/journal_club.html#model-interpretability-2-papers","text":"Indico","title":"Model Interpretability (2 papers):"},{"location":"innovation/journal_club.html#identifying-the-relevant-dependencies-of-the-neural-network-response-on-characteristics-of-the-input-space","text":"Stefan Wunsch, Raphael Friese, Roger Wolf, G\u00fcnter Quast Abstract: The relation between the input and output spaces of neural networks (NNs) is investigated to identify those characteristics of the input space that have a large influence on the output for a given task. For this purpose, the NN function is decomposed into a Taylor expansion in each element of the input space. The Taylor coefficients contain information about the sensitivity of the NN response to the inputs. A metric is introduced that allows for the identification of the characteristics that mostly determine the performance of the NN in solving a given task. Finally, the capability of this metric to analyze the performance of the NN is evaluated based on a task common to data analyses in high-energy particle physics experiments. Paper","title":"Identifying the relevant dependencies of the neural network response on characteristics of the input space"},{"location":"innovation/journal_club.html#innvestigate-neural-networks","text":"Maximilian Alber, Sebastian Lapuschkin, Philipp Seegerer, Miriam H\u00e4gele, Kristof T. Sch\u00fctt, Gr\u00e9goire Montavon, Wojciech Samek, Klaus-Robert M\u00fcller, Sven D\u00e4hne, Pieter-Jan Kindermans In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and pre- dictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this short- coming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the- box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures. Paper Code","title":"iNNvestigate neural networks!"},{"location":"innovation/journal_club.html#simulation-based-inference-in-particle-physics-and-beyond-and-beyond","text":"Johann Brehmer, Kyle Cranmer Abstract: Our predictions for particle physics processes are realized in a chain of complex simulators. They allow us to generate high-fidelity simulated data, but they are not well-suited for inference on the theory parameters with observed data. We explain why the likelihood function of high-dimensional LHC data cannot be explicitly evaluated, why this matters for data analysis, and reframe what the field has traditionally done to circumvent this problem. We then review new simulation-based inference methods that let us directly analyze high-dimensional data by combining machine learning techniques and information from the simulator. Initial studies indicate that these techniques have the potential to substantially improve the precision of LHC measurements. Finally, we discuss probabilistic programming, an emerging paradigm that lets us extend inference to the latent process of the simulator. Indico Paper Code","title":"Simulation-based inference in particle physics and beyond (and beyond)"},{"location":"innovation/journal_club.html#efficiency-parameterization-with-neural-networks","text":"C. Badiali, F.A. Di Bello, G. Frattari, E. Gross, V. Ippolito, M. Kado, J. Shlomi Abstract: Multidimensional efficiency maps are commonly used in high energy physics experiments to mitigate the limitations in the generation of large samples of simulated events. Binned multidimensional efficiency maps are however strongly limited by statistics. We propose a neural network approach to learn ratios of local densities to estimate in an optimal fashion efficiencies as a function of a set of parameters. Graph neural network techniques are used to account for the high dimensional correlations between different physics objects in the event. We show in a specific toy model how this method is applicable to produce accurate multidimensional efficiency maps for heavy flavor tagging classifiers in HEP experiments, including for processes on which it was not trained. - Indico - Paper - Code","title":"Efficiency Parameterization with Neural Networks"},{"location":"innovation/journal_club.html#a-general-framework-for-uncertainty-estimation-in-deep-learning","text":"Antonio Loquercio, Mattia Seg\u00f9, Davide Scaramuzza Neural networks predictions are unreliable when the input sample is out of the training distribution or corrupted by noise. Being able to detect such failures automatically is fundamental to integrate deep learning algorithms into robotics. Current approaches for uncertainty estimation of neural networks require changes to the network and optimization process, typically ignore prior knowledge about the data, and tend to make over-simplifying assumptions which underestimate uncertainty. To address these limitations, we propose a novel framework for uncertainty estimation. Based on Bayesian belief networks and Monte-Carlo sampling, our framework not only fully models the different sources of prediction uncertainty, but also incorporates prior data information, e.g. sensor noise. We show theoretically that this gives us the ability to capture uncertainty better than existing methods. In addition, our framework has several desirable properties: (i) it is agnostic to the network architecture and task; (ii) it does not require changes in the optimization process; (iii) it can be applied to already trained architectures. We thoroughly validate the proposed framework through extensive experiments on both computer vision and control tasks, where we outperform previous methods by up to 23% in accuracy. Indico Paper Code","title":"A General Framework for Uncertainty Estimation in Deep Learning"},{"location":"optimization/data_augmentation.html","text":"Todo.","title":"Data augmentation"},{"location":"optimization/importance.html","text":"Todo. Idea: Methods to determine feature importance Sources: Javier Giles - Perturbation ranking Roger Wolf - 1803.08782 Identifying the relevant dependencies of the neural network response on characteristics of the input space https://arxiv.org/abs/1808.04260 6 iNNvestigate neural networks! https://arxiv.org/abs/1803.08782 4","title":"Feature importance"},{"location":"optimization/model_optimization.html","text":"Model optimization \u00b6 This page summarizes the concepts shown in a contribution on Bayesian Optimization to the ML Forum and may be edited and published elsewhere by the author. What we talk about when we talk about model optimization \u00b6 Given some data \\(x\\) and a family of functionals parameterized by (a vector of) parameters \\(\\theta\\) (e.g. for DNN training weights), the problem of learning consists in finding \\(argmin_\\theta Loss(f_\\theta(x) - y_{true})\\) . The treatment below focusses on gradient descent, but the formalization is completely general, i.e. it can be applied also to methods that are not explicitly formulated in terms of gradient descent (e.g. BDTs). The mathematical formalism for the problem of learning is briefly explained in a contribution on statistical learning to the ML forum : for the purposes of this documentation we will proceed through two illustrations. The first illustration, elaborated from an image by the huawei forums shows the general idea behind learning through gradient descent in a multidimensional parameter space, where the minimum of a loss function is found by following the function's gradient until the minimum. The cartoon illustrates the general idea behind gradient descent to find the minimum of a function in a multidimensional parameter space (figure elaborated from an image by the huawei forums ). The model to be optimized via a loss function typically is a parametric function, where the set of parameters (e.g. the network weights in neural networks) corresponds to a certain fixed structure of the network. For example, a network with two inputs, two inner layers of two neurons, and one output neuron will have six parameters whose values will be changed until the loss function reaches its minimum. When we talk about model optimization we refer to the fact that often we are interested in finding which model structure is the best to describe our data. The main concern is to design a model that has a sufficient complexity to store all the information contained in the training data. We can therefore think of parameterizing the network structure itself, e.g. in terms of the number of inner layers and number of neurons per layer: these hyperparameters define a space where we want to again minimize a loss function. Formally, the parametric function \\(f_\\theta\\) is also a function of these hyperparameters \\(\\lambda\\) : \\(f_{(\\theta, \\lambda)}\\) , and the \\(\\lambda\\) can be optimized The second illustration, also elaborated from an image by the huawei forums , broadly illustrates this concept: for each point in the hyperparameters space (that is, for each configuration of the model), the individual model is optimized as usual. The global minimum over the hyperparameters space is then sought. The cartoon illustrates the general idea behind gradient descent to optimize the model complexity (in terms of the choice of hyperparameters) multidimensional parameter and hyperparameter space (figure elaborated from an image by the huawei forums ). Caveat: which data should you use to optimize your model \u00b6 In typical machine learning studies, you should divide your dataset into three parts. One is used for training the model ( training sample ), one is used for testing the performance of the model ( test sample ), and the third one is the one where you actually use your trained model, e.g. for inference ( application sample ). Sometimes you may get away with using test data as application data: Helge Voss (Chap 5 of Behnke et al.) states that this is acceptable under three conditions that must be simultaneously valid: no hyperparameter optimization is performed; no overtraining is found; the number of training data is high enough to make statistical fluctuations negligible. If you are doing any kind of hyperparamters optimization, thou shalt NOT use the test sample as application sample . You should have at least three distinct sets, and ideally you should use four (training, testing, hyperparameter optimization, application). Grid Search \u00b6 The most simple hyperparameters optimization algorithm is the grid search , where you train all the models in the hyperparameters space to build the full landscape of the global loss function, as illustrated in Goodfellow, Bengio, Courville: \"Deep Learning\" . The cartoon illustrates the general idea behind grid search (image taken from Goodfellow, Bengio, Courville: \"Deep Learning\" ). To perform a meaningful grid search, you have to provide a set of values within the acceptable range of each hyperparameters, then for each point in the cross-product space you have to train the corresponding model. The main issue with grid search is that when there are nonimportant hyperparameters (i.e. hyperparameters whose value doesn't influence much the model performance) the algorithm spends an exponentially large time (in the number of nonimportant hyperparameters) in the noninteresting configurations: having \\(m\\) parameters and testing \\(n\\) values for each of them leads to \\(\\mathcal{O}(n^m)\\) tested configurations. While the issue may be mitigated by parallelization, when the number of hyperparameters (the dimension of hyperparameters space) surpasses a handful, even parallelization can't help. Another issue is that the search is binned: depending on the granularity in the scan, the global minimum may be invisible. Despite these issues, grid search is sometimes still a feasible choice, and gives its best when done iteratively . For example, if you start from the interval \\(\\{-1, 0, 1\\}\\) : if the best parameter is found to be at the boundary (1), then extend range ( \\(\\{1, 2, 3\\}\\) ) and do the search in the new range; if the best parameter is e.g. at 0, then maybe zoom in and do a search in the range \\(\\{-0.1, 0, 0.1\\}\\) . Random search \u00b6 An improvement of the grid search is the random search , which proceeds like this: you provide a marginal p.d.f. for each hyperparameter; you sample from the joint p.d.f. a certain number of training configurations; you train for each of these configurations to build the loss function landscape. This procedure has significant advantages over a simple grid search: random search is not binned, because you are sampling from a continuous p.d.f., so the pool of explorable hyperparameter values is larger; random search is exponentially more efficient, because it tests a unique value for each influential hyperparameter on nearly every trial. Random search also work best when done iteratively. The differences between grid and random search are again illustrated in Goodfellow, Bengio, Courville: \"Deep Learning\" . The cartoon illustrates the general idea behind random search, as opposed to grid search (image taken from Goodfellow, Bengio, Courville: \"Deep Learning\" ). Model-based optimization by gradient descent \u00b6 Now that we have looked at the most basic model optimization techniques, we are ready to look into using gradient descent to solve a model optimization problem. We will proceed by recasting the problem as one of model selection , where the hyperparameters are the input (decision) variables, and the model selection criterion is a differentiable validation set error. The validation set error attempts to describe the complexity of the network by a single hyperparameter (details in [ a contribution on statistical learning to the ML forum ]) The problem may be solved with standard gradient descent, as illustrated above, if we assume that the training criterion \\(C\\) is continuous and differentiable with respect to both the parameters \\(\\theta\\) (e.g. weights) and hyperparameters \\(\\lambda\\) Unfortunately, the gradient is seldom available (either because it has a prohibitive computational cost, or because it is non-differentiable as is the case when there are discrete variables). A diagram illustrating the way gradient-based model optimization works has been prepared by Bengio, doi:10.1162/089976600300015187 . The diagram illustrates the way model optimization can be recast as a model selection problem, where a model selection criterion involves a differentiable validation set error (image taken from Bengio, doi:10.1162/089976600300015187 ). Model-based optimization by surrogates \u00b6 Sequential Model-based Global Optimization (SMBO) consists in replacing the loss function with a surrogate model of it, when the loss function (i.e. the validation set error) is not available. The surrogate is typically built as a Bayesian regression model, when one estimates the expected value of the validation set error for each hyperparameter together with the uncertainty in this expectation . The pseudocode for the SMBO algorithm is illustrated by Bergstra et al . The diagram illustrates the pseudocode for the Sequential Model-based Global Optimization (image taken from Bergstra et al ). This procedure results in a tradeoff between: exploration , i.e. proposing hyperparameters with high uncertainty, which may result in substantial improvement or not; and exploitation (propose hyperparameters that will likely perform as well as the current proposal---usually this mean close to the current ones). The disadvantage is that the whole procedure must run until completion before giving as an output any usable information. By comparison, manual or random searches tend to give hints on the location of the minimum faster. Bayesian Optimization \u00b6 We are now ready to tackle in full what is referred to as Bayesian optimization . Bayesian optimization assumes that the unknown function \\(f(\\theta, \\lambda)\\) was sampled from a Gaussian process (GP), and that after the observations it maintains the corresponding posterior. In this context, observations are the various validation set errors for different values of the hyperparameters \\(\\lambda\\) . In order to pick the next value to probe, one maximizes some estimate of the expected improvement (see below). To understand the meaning of \"sampled from a Gaussian process\", we need to define what a Gaussian process is. Gaussian processes \u00b6 Gaussian processes (GPs) generalize the concept of Gaussian distribution over discrete random variables to the concept of Gaussian distribution over continuous functions . Given some data and an estimate of the Gaussian noise, by fitting a function one can estimate also the noise at the interpolated points. This estimate is made by similarity with contiguous points, adjusted by the distance between points. A GP is therefore fully described by its mean and its covariance function. An illustration of Gaussian processes is given in Kevin Jamieson's CSE599 lecture notes . The diagram illustrates the evolution of a Gaussian process, when adding interpolating points (image taken from Kevin Jamieson's CSE599 lecture notes ). GPs are great for Bayesian optimization because they out-of-the-box provide the expected value (i.e. the mean of the process) and its uncertainty (covariance function). The basic idea behind Bayesian optimization \u00b6 Gradient descent methods are intrinsically local: the decision on the next step is taken based on the local gradient and Hessian approximations- Bayesian optimization (BO) with GP priors uses a model that uses all the information from the previous steps by encoding it in the model giving the expectation and its uncertainty. The consequence is that GP-based BO can find the minimum of difficult nonconvex functions in relatively few evaluations, at the cost of performing more computations to find the next point to try in the hyperparameters space. The BO prior is a prior over the space of the functions. GPs are especially suited to play the role of BO prior, because marginals and conditionals can be computed in closed form (thanks to the properties of the Gaussian distribution). There are several methods to choose the acquisition function (the function that selects the next step for the algorithm), but there is no omnipurpose recipe: the best approach is problem-dependent. The acquisition function involves an accessory optimization to maximize a certain quantity; typical choices are: maximize the probability of improvement over the current best value: can be calculated analytically for a GP; maximize the expected improvement over the current best value: can also be calculated analytically for a GP; maximize the GP Upper confidence bound: minimize \"regret\" over the course of the optimization. Historical note \u00b6 Gaussian process regression is also called kriging in geostatistics, after Daniel G. Krige (1951) who pioneered the concept later formalized by Matheron (1962) Bayesian optimization in practice \u00b6 The figure below, taken by a tutorial on BO by Martin Krasser , clarifies rather well the procedure. The task is to approximate the target function (labelled noise free objective in the figure), given some noisy samples of it (the black crosses). At the first iteration, one starts from a flat surrogate function, with a given uncertainty, and fits it to the noisy samples. To choose the next sampling location, a certain acquisition function is computed, and the value that maximizes it is chosen as the next sampling location At each iteration, more noisy samples are added, until the distance between consecutive sampling locations is minimized (or, equivalently, a measure of the value of the best selected sample is maximized). Practical illustration of Bayesian Optimization (images taken from a tutorial on BO by Martin Krasser] ). Limitations (and some workaround) of Bayesian Optimization \u00b6 There are three main limitations to the BO approach. A good overview of these limitations and of possible solutions can be found in arXiv:1206.2944 . First of all, it is unclear what is an appropriate choice for the covariance function and its associated hyperparameters. In particular, the standard squared exponential kernel is often too smooth. As a workaround, alternative kernels may be used: a common choice is the Mat\u00e9rn 5/2 kernel, which is similar to the squared exponential one but allows for non-smoothness. Another issue is that, for certain problems, the function evaluation may take very long to compute. To overcome this, often one can replace the function evaluation with the Monte Carlo integration of the expected improvement over the GP hyperparameters, which is faster. The third main issue is that for complex problems one would ideally like to take advantage of parallel computation. The procedure is iterative, however, and it is not easy to come up with a scheme to make it parallelizable. The referenced paper proposed sampling over the expected acquisition, conditioned on all the pending evaluations : this is computationally cheap and is intrinsically parallelizable. Alternatives to Gaussian processes: Tree-based models \u00b6 Gaussian Processes model directly \\(P(hyperpar | data)\\) byt are not the only suitable surrogate models for Bayesian optimization The so-called Tree-structured Parzen Estimator (TPE), described in Bergstra et al , models separately \\(P(data | hyperpar)\\) and \\(P(hyperpar)\\) , to then obtain the posterior by explicit application of the Bayes theorem TPEs exploit the fact that the choice of hyperparameters is intrinsically graph-structured, in the sense that e.g. you first choose the number of layers, then choose neurons per layer, etc. TPEs run over this generative process by replacing the hyperparameters priors with nonparametric densities. These generative nonparametric densities are built by classifying them into those that result in worse/better loss than the current proposal. TPEs have been used in CMS already around 2017 in a VHbb analysis (see repository by Sean-Jiun Wang ) and in a charged Higgs to tb search ( HIG-18-004, doi:10.1007/JHEP01(2020)096 ). Implementations of Bayesian Optimization \u00b6 Implementations in R are readily available as the R-studio tuning package ; Scikit-learn provides a handy implementation of Gaussian processes ; **scipy* provides a handy implementation of the optimization routines; hyperopt provides a handy implementation of distributed hyperparameter optimization routines ; GPs not coded by default, hence must rely on scikit-learn; Parzen tree estimators are implemented by default (together with random search); Several handy tutorials online focussed on hyperparameters optimization Tutorial by Martin Krasser ; Tutorial by Jason Brownlee ; Early example of hyperopt in CMS VHbb analysis: repository by Sean-Jiun Wang ), for optimization of a BDT; Charged Higgs HIG-18-004, doi:10.1007/JHEP01(2020)096 ) for optimization of a DNN (no public link for the code, contact me if needed) Several expansions and improvements (particularly targeted at HPC clusters) are available, see e.g. this talk by Eric Wulff . Caveats: don't get too obsessed with model optimization \u00b6 In general, optimizing model structure is a good thing. F. Chollet e.g. says \"If you want to get to the very limit of what can be achieved on a given task, you can't be content with arbitrary choices made by a fallible human\" . On the other side, for many problems hyperparameter optimization does result in small improvements, and there is a tradeoff between improvement and time spent on the task: sometimes the time spent on optimization may not be worth, e.g. when the gradient of the loss in hyperparameters space is very flat (i.e. different hyperparameter sets give more or less the same results), particularly if you already know that small improvements will be eaten up by e.g. systematic uncertainties. On the other side, before you perform the optimization you don't know if the landscape is flat or if you can expect substantial improvements. Sometimes broad grid or random searches may give you a hint on whether the landscape of hyperparameters space is flat or not. Sometimes you may get good (and faster) improvements by model ensembling rather than by model optimization. To do model ensembling, you first train a handful models (either different methods---BDT, SVM, NN, etc---or different hyperparameters sets): \\(pred\\_a = model\\_a.predict(x)\\) , ..., \\(pred\\_d = model\\_d.predict(x)\\) . You then pool the predictions: \\(pooled\\_pred = (pred\\_a + pred\\_b + pred\\_c + pred\\_d)/4.\\) . THis works if all models are kind of good: if one is significantly worse than the others, then \\(pooled\\_pred\\) may not be as good as the best model of the pool. You can also find ways of ensembling in a smarter way, e.g. by doing weighted rather than simple averages: \\(pooled\\_pred = 0.5\\cdot pred\\_a + 0.25\\cdot pred\\_b + 0.1\\cdot pred\\_c + 0.15\\cdot pred\\_d)/4.\\) . Here the idea is to give more weight to better classifiers. However, you transfer the problem to having to choose the weights. These can be found empirically empirically by using random search or other algorithms like Nelder-Mead ( result = scipy.optimize.minimize(objective, pt, method='nelder-mead' ), where you build simplexes (polytope with N+1 vertices in N dimensions, generalization of triangle) and stretch them towards higher values of the objective. Nelder-Mead can converge to nonstationary points, but there are extensions of the algorithm that may help. This page summarizes the concepts shown in a contribution on Bayesian Optimization to the ML Forum . Content may be edited and published elsewhere by the author. Page author: Pietro Vischia, 2022","title":"Model optimization"},{"location":"optimization/model_optimization.html#model-optimization","text":"This page summarizes the concepts shown in a contribution on Bayesian Optimization to the ML Forum and may be edited and published elsewhere by the author.","title":"Model optimization"},{"location":"optimization/model_optimization.html#what-we-talk-about-when-we-talk-about-model-optimization","text":"Given some data \\(x\\) and a family of functionals parameterized by (a vector of) parameters \\(\\theta\\) (e.g. for DNN training weights), the problem of learning consists in finding \\(argmin_\\theta Loss(f_\\theta(x) - y_{true})\\) . The treatment below focusses on gradient descent, but the formalization is completely general, i.e. it can be applied also to methods that are not explicitly formulated in terms of gradient descent (e.g. BDTs). The mathematical formalism for the problem of learning is briefly explained in a contribution on statistical learning to the ML forum : for the purposes of this documentation we will proceed through two illustrations. The first illustration, elaborated from an image by the huawei forums shows the general idea behind learning through gradient descent in a multidimensional parameter space, where the minimum of a loss function is found by following the function's gradient until the minimum. The cartoon illustrates the general idea behind gradient descent to find the minimum of a function in a multidimensional parameter space (figure elaborated from an image by the huawei forums ). The model to be optimized via a loss function typically is a parametric function, where the set of parameters (e.g. the network weights in neural networks) corresponds to a certain fixed structure of the network. For example, a network with two inputs, two inner layers of two neurons, and one output neuron will have six parameters whose values will be changed until the loss function reaches its minimum. When we talk about model optimization we refer to the fact that often we are interested in finding which model structure is the best to describe our data. The main concern is to design a model that has a sufficient complexity to store all the information contained in the training data. We can therefore think of parameterizing the network structure itself, e.g. in terms of the number of inner layers and number of neurons per layer: these hyperparameters define a space where we want to again minimize a loss function. Formally, the parametric function \\(f_\\theta\\) is also a function of these hyperparameters \\(\\lambda\\) : \\(f_{(\\theta, \\lambda)}\\) , and the \\(\\lambda\\) can be optimized The second illustration, also elaborated from an image by the huawei forums , broadly illustrates this concept: for each point in the hyperparameters space (that is, for each configuration of the model), the individual model is optimized as usual. The global minimum over the hyperparameters space is then sought. The cartoon illustrates the general idea behind gradient descent to optimize the model complexity (in terms of the choice of hyperparameters) multidimensional parameter and hyperparameter space (figure elaborated from an image by the huawei forums ).","title":"What we talk about when we talk about model optimization"},{"location":"optimization/model_optimization.html#caveat-which-data-should-you-use-to-optimize-your-model","text":"In typical machine learning studies, you should divide your dataset into three parts. One is used for training the model ( training sample ), one is used for testing the performance of the model ( test sample ), and the third one is the one where you actually use your trained model, e.g. for inference ( application sample ). Sometimes you may get away with using test data as application data: Helge Voss (Chap 5 of Behnke et al.) states that this is acceptable under three conditions that must be simultaneously valid: no hyperparameter optimization is performed; no overtraining is found; the number of training data is high enough to make statistical fluctuations negligible. If you are doing any kind of hyperparamters optimization, thou shalt NOT use the test sample as application sample . You should have at least three distinct sets, and ideally you should use four (training, testing, hyperparameter optimization, application).","title":"Caveat: which data should you use to optimize your model"},{"location":"optimization/model_optimization.html#grid-search","text":"The most simple hyperparameters optimization algorithm is the grid search , where you train all the models in the hyperparameters space to build the full landscape of the global loss function, as illustrated in Goodfellow, Bengio, Courville: \"Deep Learning\" . The cartoon illustrates the general idea behind grid search (image taken from Goodfellow, Bengio, Courville: \"Deep Learning\" ). To perform a meaningful grid search, you have to provide a set of values within the acceptable range of each hyperparameters, then for each point in the cross-product space you have to train the corresponding model. The main issue with grid search is that when there are nonimportant hyperparameters (i.e. hyperparameters whose value doesn't influence much the model performance) the algorithm spends an exponentially large time (in the number of nonimportant hyperparameters) in the noninteresting configurations: having \\(m\\) parameters and testing \\(n\\) values for each of them leads to \\(\\mathcal{O}(n^m)\\) tested configurations. While the issue may be mitigated by parallelization, when the number of hyperparameters (the dimension of hyperparameters space) surpasses a handful, even parallelization can't help. Another issue is that the search is binned: depending on the granularity in the scan, the global minimum may be invisible. Despite these issues, grid search is sometimes still a feasible choice, and gives its best when done iteratively . For example, if you start from the interval \\(\\{-1, 0, 1\\}\\) : if the best parameter is found to be at the boundary (1), then extend range ( \\(\\{1, 2, 3\\}\\) ) and do the search in the new range; if the best parameter is e.g. at 0, then maybe zoom in and do a search in the range \\(\\{-0.1, 0, 0.1\\}\\) .","title":"Grid Search"},{"location":"optimization/model_optimization.html#random-search","text":"An improvement of the grid search is the random search , which proceeds like this: you provide a marginal p.d.f. for each hyperparameter; you sample from the joint p.d.f. a certain number of training configurations; you train for each of these configurations to build the loss function landscape. This procedure has significant advantages over a simple grid search: random search is not binned, because you are sampling from a continuous p.d.f., so the pool of explorable hyperparameter values is larger; random search is exponentially more efficient, because it tests a unique value for each influential hyperparameter on nearly every trial. Random search also work best when done iteratively. The differences between grid and random search are again illustrated in Goodfellow, Bengio, Courville: \"Deep Learning\" . The cartoon illustrates the general idea behind random search, as opposed to grid search (image taken from Goodfellow, Bengio, Courville: \"Deep Learning\" ).","title":"Random search"},{"location":"optimization/model_optimization.html#model-based-optimization-by-gradient-descent","text":"Now that we have looked at the most basic model optimization techniques, we are ready to look into using gradient descent to solve a model optimization problem. We will proceed by recasting the problem as one of model selection , where the hyperparameters are the input (decision) variables, and the model selection criterion is a differentiable validation set error. The validation set error attempts to describe the complexity of the network by a single hyperparameter (details in [ a contribution on statistical learning to the ML forum ]) The problem may be solved with standard gradient descent, as illustrated above, if we assume that the training criterion \\(C\\) is continuous and differentiable with respect to both the parameters \\(\\theta\\) (e.g. weights) and hyperparameters \\(\\lambda\\) Unfortunately, the gradient is seldom available (either because it has a prohibitive computational cost, or because it is non-differentiable as is the case when there are discrete variables). A diagram illustrating the way gradient-based model optimization works has been prepared by Bengio, doi:10.1162/089976600300015187 . The diagram illustrates the way model optimization can be recast as a model selection problem, where a model selection criterion involves a differentiable validation set error (image taken from Bengio, doi:10.1162/089976600300015187 ).","title":"Model-based optimization by gradient descent"},{"location":"optimization/model_optimization.html#model-based-optimization-by-surrogates","text":"Sequential Model-based Global Optimization (SMBO) consists in replacing the loss function with a surrogate model of it, when the loss function (i.e. the validation set error) is not available. The surrogate is typically built as a Bayesian regression model, when one estimates the expected value of the validation set error for each hyperparameter together with the uncertainty in this expectation . The pseudocode for the SMBO algorithm is illustrated by Bergstra et al . The diagram illustrates the pseudocode for the Sequential Model-based Global Optimization (image taken from Bergstra et al ). This procedure results in a tradeoff between: exploration , i.e. proposing hyperparameters with high uncertainty, which may result in substantial improvement or not; and exploitation (propose hyperparameters that will likely perform as well as the current proposal---usually this mean close to the current ones). The disadvantage is that the whole procedure must run until completion before giving as an output any usable information. By comparison, manual or random searches tend to give hints on the location of the minimum faster.","title":"Model-based optimization by surrogates"},{"location":"optimization/model_optimization.html#bayesian-optimization","text":"We are now ready to tackle in full what is referred to as Bayesian optimization . Bayesian optimization assumes that the unknown function \\(f(\\theta, \\lambda)\\) was sampled from a Gaussian process (GP), and that after the observations it maintains the corresponding posterior. In this context, observations are the various validation set errors for different values of the hyperparameters \\(\\lambda\\) . In order to pick the next value to probe, one maximizes some estimate of the expected improvement (see below). To understand the meaning of \"sampled from a Gaussian process\", we need to define what a Gaussian process is.","title":"Bayesian Optimization"},{"location":"optimization/model_optimization.html#gaussian-processes","text":"Gaussian processes (GPs) generalize the concept of Gaussian distribution over discrete random variables to the concept of Gaussian distribution over continuous functions . Given some data and an estimate of the Gaussian noise, by fitting a function one can estimate also the noise at the interpolated points. This estimate is made by similarity with contiguous points, adjusted by the distance between points. A GP is therefore fully described by its mean and its covariance function. An illustration of Gaussian processes is given in Kevin Jamieson's CSE599 lecture notes . The diagram illustrates the evolution of a Gaussian process, when adding interpolating points (image taken from Kevin Jamieson's CSE599 lecture notes ). GPs are great for Bayesian optimization because they out-of-the-box provide the expected value (i.e. the mean of the process) and its uncertainty (covariance function).","title":"Gaussian processes"},{"location":"optimization/model_optimization.html#the-basic-idea-behind-bayesian-optimization","text":"Gradient descent methods are intrinsically local: the decision on the next step is taken based on the local gradient and Hessian approximations- Bayesian optimization (BO) with GP priors uses a model that uses all the information from the previous steps by encoding it in the model giving the expectation and its uncertainty. The consequence is that GP-based BO can find the minimum of difficult nonconvex functions in relatively few evaluations, at the cost of performing more computations to find the next point to try in the hyperparameters space. The BO prior is a prior over the space of the functions. GPs are especially suited to play the role of BO prior, because marginals and conditionals can be computed in closed form (thanks to the properties of the Gaussian distribution). There are several methods to choose the acquisition function (the function that selects the next step for the algorithm), but there is no omnipurpose recipe: the best approach is problem-dependent. The acquisition function involves an accessory optimization to maximize a certain quantity; typical choices are: maximize the probability of improvement over the current best value: can be calculated analytically for a GP; maximize the expected improvement over the current best value: can also be calculated analytically for a GP; maximize the GP Upper confidence bound: minimize \"regret\" over the course of the optimization.","title":"The basic idea behind Bayesian optimization"},{"location":"optimization/model_optimization.html#historical-note","text":"Gaussian process regression is also called kriging in geostatistics, after Daniel G. Krige (1951) who pioneered the concept later formalized by Matheron (1962)","title":"Historical note"},{"location":"optimization/model_optimization.html#bayesian-optimization-in-practice","text":"The figure below, taken by a tutorial on BO by Martin Krasser , clarifies rather well the procedure. The task is to approximate the target function (labelled noise free objective in the figure), given some noisy samples of it (the black crosses). At the first iteration, one starts from a flat surrogate function, with a given uncertainty, and fits it to the noisy samples. To choose the next sampling location, a certain acquisition function is computed, and the value that maximizes it is chosen as the next sampling location At each iteration, more noisy samples are added, until the distance between consecutive sampling locations is minimized (or, equivalently, a measure of the value of the best selected sample is maximized). Practical illustration of Bayesian Optimization (images taken from a tutorial on BO by Martin Krasser] ).","title":"Bayesian optimization in practice"},{"location":"optimization/model_optimization.html#limitations-and-some-workaround-of-bayesian-optimization","text":"There are three main limitations to the BO approach. A good overview of these limitations and of possible solutions can be found in arXiv:1206.2944 . First of all, it is unclear what is an appropriate choice for the covariance function and its associated hyperparameters. In particular, the standard squared exponential kernel is often too smooth. As a workaround, alternative kernels may be used: a common choice is the Mat\u00e9rn 5/2 kernel, which is similar to the squared exponential one but allows for non-smoothness. Another issue is that, for certain problems, the function evaluation may take very long to compute. To overcome this, often one can replace the function evaluation with the Monte Carlo integration of the expected improvement over the GP hyperparameters, which is faster. The third main issue is that for complex problems one would ideally like to take advantage of parallel computation. The procedure is iterative, however, and it is not easy to come up with a scheme to make it parallelizable. The referenced paper proposed sampling over the expected acquisition, conditioned on all the pending evaluations : this is computationally cheap and is intrinsically parallelizable.","title":"Limitations (and some workaround) of Bayesian Optimization"},{"location":"optimization/model_optimization.html#alternatives-to-gaussian-processes-tree-based-models","text":"Gaussian Processes model directly \\(P(hyperpar | data)\\) byt are not the only suitable surrogate models for Bayesian optimization The so-called Tree-structured Parzen Estimator (TPE), described in Bergstra et al , models separately \\(P(data | hyperpar)\\) and \\(P(hyperpar)\\) , to then obtain the posterior by explicit application of the Bayes theorem TPEs exploit the fact that the choice of hyperparameters is intrinsically graph-structured, in the sense that e.g. you first choose the number of layers, then choose neurons per layer, etc. TPEs run over this generative process by replacing the hyperparameters priors with nonparametric densities. These generative nonparametric densities are built by classifying them into those that result in worse/better loss than the current proposal. TPEs have been used in CMS already around 2017 in a VHbb analysis (see repository by Sean-Jiun Wang ) and in a charged Higgs to tb search ( HIG-18-004, doi:10.1007/JHEP01(2020)096 ).","title":"Alternatives to Gaussian processes: Tree-based models"},{"location":"optimization/model_optimization.html#implementations-of-bayesian-optimization","text":"Implementations in R are readily available as the R-studio tuning package ; Scikit-learn provides a handy implementation of Gaussian processes ; **scipy* provides a handy implementation of the optimization routines; hyperopt provides a handy implementation of distributed hyperparameter optimization routines ; GPs not coded by default, hence must rely on scikit-learn; Parzen tree estimators are implemented by default (together with random search); Several handy tutorials online focussed on hyperparameters optimization Tutorial by Martin Krasser ; Tutorial by Jason Brownlee ; Early example of hyperopt in CMS VHbb analysis: repository by Sean-Jiun Wang ), for optimization of a BDT; Charged Higgs HIG-18-004, doi:10.1007/JHEP01(2020)096 ) for optimization of a DNN (no public link for the code, contact me if needed) Several expansions and improvements (particularly targeted at HPC clusters) are available, see e.g. this talk by Eric Wulff .","title":"Implementations of Bayesian Optimization"},{"location":"optimization/model_optimization.html#caveats-dont-get-too-obsessed-with-model-optimization","text":"In general, optimizing model structure is a good thing. F. Chollet e.g. says \"If you want to get to the very limit of what can be achieved on a given task, you can't be content with arbitrary choices made by a fallible human\" . On the other side, for many problems hyperparameter optimization does result in small improvements, and there is a tradeoff between improvement and time spent on the task: sometimes the time spent on optimization may not be worth, e.g. when the gradient of the loss in hyperparameters space is very flat (i.e. different hyperparameter sets give more or less the same results), particularly if you already know that small improvements will be eaten up by e.g. systematic uncertainties. On the other side, before you perform the optimization you don't know if the landscape is flat or if you can expect substantial improvements. Sometimes broad grid or random searches may give you a hint on whether the landscape of hyperparameters space is flat or not. Sometimes you may get good (and faster) improvements by model ensembling rather than by model optimization. To do model ensembling, you first train a handful models (either different methods---BDT, SVM, NN, etc---or different hyperparameters sets): \\(pred\\_a = model\\_a.predict(x)\\) , ..., \\(pred\\_d = model\\_d.predict(x)\\) . You then pool the predictions: \\(pooled\\_pred = (pred\\_a + pred\\_b + pred\\_c + pred\\_d)/4.\\) . THis works if all models are kind of good: if one is significantly worse than the others, then \\(pooled\\_pred\\) may not be as good as the best model of the pool. You can also find ways of ensembling in a smarter way, e.g. by doing weighted rather than simple averages: \\(pooled\\_pred = 0.5\\cdot pred\\_a + 0.25\\cdot pred\\_b + 0.1\\cdot pred\\_c + 0.15\\cdot pred\\_d)/4.\\) . Here the idea is to give more weight to better classifiers. However, you transfer the problem to having to choose the weights. These can be found empirically empirically by using random search or other algorithms like Nelder-Mead ( result = scipy.optimize.minimize(objective, pt, method='nelder-mead' ), where you build simplexes (polytope with N+1 vertices in N dimensions, generalization of triangle) and stretch them towards higher values of the objective. Nelder-Mead can converge to nonstationary points, but there are extensions of the algorithm that may help. This page summarizes the concepts shown in a contribution on Bayesian Optimization to the ML Forum . Content may be edited and published elsewhere by the author. Page author: Pietro Vischia, 2022","title":"Caveats: don't get too obsessed with model optimization"},{"location":"training/MLaaS4HEP.html","text":"Machine Learning as a Service for HEP \u00b6 MLaaS for HEP is a set of Python-based modules to support reading HEP data and stream them to the ML tool of the user's choice. It consists of three independent layers: - Data Streaming layer to handle remote data, see reader.py - Data Training layer to train ML model for given HEP data, see workflow.py - Data Inference layer, see tfaas_client.py The MLaaS4HEP resopitory can be found here . The general architecture of MLaaS4HEP looks like this: Even though this architecture was originally developed for dealing with HEP ROOT files, we extend it to other data formats. As of right now, following data formats are supported: JSON, CSV, Parquet, and ROOT. All of the formats support reading files from the local file system or HDFS, while the ROOT format supports reading files via the XRootD protocol. The pre-trained models can be easily uploaded to TFaaS inference server for serving them to clients. The TFaaS documentation can be found here . Dependencies \u00b6 Here is a list of the dependencies: - pyarrow for reading data from HDFS file system - uproot for reading ROOT files - numpy , pandas for data representation - modin for fast panda support - numba for speeing up individual functions Installation \u00b6 The easiest way to install and run MLaaS4HEP and TFaaS is to use pre-build docker images # run MLaaS4HEP docker container docker run veknet/mlaas4hep # run TFaaS docker container docker run veknet/tfaas Reading ROOT files \u00b6 MLaaS4HEP python repository provides the reader.py module that defines a DataReader class able to read either local or remote ROOT files (via xrootd) in chunks. It is based on the uproot framework. Basic usage # setup the proper environment, e.g. # export PYTHONPATH=/path/src/python # path to MLaaS4HEP python framework # export PATH=/path/bin:$PATH # path to MLaaS4HEP binaries # get help and option description reader --help # here is a concrete example of reading local ROOT file: reader --fin=/opt/cms/data/Tau_Run2017F-31Mar2018-v1_NANOAOD.root --info --verbose=1 --nevts=2000 # here is an example of reading remote ROOT file: reader --fin=root://cms-xrd-global.cern.ch//store/data/Run2017F/Tau/NANOAOD/31Mar2018-v1/20000/6C6F7EAE-7880-E811-82C1-008CFA165F28.root --verbose=1 --nevts=2000 --info # both of aforementioned commands produce the following output Reading root://cms-xrd-global.cern.ch//store/data/Run2017F/Tau/NANOAOD/31Mar2018-v1/20000/6C6F7EAE-7880-E811-82C1-008CFA165F28.root # 1000 entries, 883 branches, 4.113945007324219 MB, 0.6002757549285889 sec, 6.853425235896175 MB/sec, 1.6659010326328503 kHz # 1000 entries, 883 branches, 4.067909240722656 MB, 1.3497390747070312 sec, 3.0138486148558896 MB/sec, 0.740883937302516 kHz ###total time elapsed for reading + specs computing: 2.2570559978485107 sec; number of chunks 2 ###total time elapsed for reading: 1.9500117301940918 sec; number of chunks 2 --- first pass: 1131872 events, (648-flat, 232-jagged) branches, 2463 attrs VMEM used: 29.896704 (MB) SWAP used: 0.0 (MB) <__main__.RootDataReader object at 0x7fb0cdfe4a00> init is complete in 2.265552043914795 sec Number of events : 1131872 # flat branches : 648 CaloMET_phi values in [-3.140625, 3.13671875] range, dim=N/A CaloMET_pt values in [0.783203125, 257.75] range, dim=N/A CaloMET_sumEt values in [820.0, 3790.0] range, dim=N/A More examples about using uproot may be found here and here . How to train ML models on HEP ROOT data \u00b6 The MLaaS4HEP framework allows to train ML models in different ways: - using full dataset (i.e. the entire amount of events stored in input ROOT files) - using chunks, as subsets of a dataset, which dimension can be chosen directly by the user and can vary between 1 and the total number of events - using local or remote ROOT files. The training phase is managed by the workflow.py module which performs the following actions: - read all input ROOT files in chunks to compute a specs file (where the main information about the ROOT files are stored: the dimension of branches, the minimum and the maximum for each branch, and the number of events for each ROOT file) - perform the training cycle (each time using a new chunk of events) - create a new chunk of events taken proportionally from the input ROOT files - extract and convert each event in a list of NumPy arrays - normalize the events - fix the Jagged Arrays dimension - create the masking vector - use the chunk to train the ML model provided by the user A schematic representation of the steps performed in the MLaaS4HEP pipeline, in particular those inside the Data Streaming and Data Training layers, is: If the dataset is large and exceed the amount of RAM on the training node, then the user should consider the chunk approach. This allows to train the ML model each time using a different chunk, until the entire dataset is completely read. In this case the user should pay close attention to the ML model convergence, and validate it after each chunk. For more information look at this , this and this . Using different training approach has pros and cons. For instance, training on entire dataset can guarantee the ML model convergence, but the dataset should fits into RAM of the training node. While chunk approach allows to split the dataset to fit in the hardware resources, but it requires proper model evaluation after each chunk training. In terms of training speed, this choice should be faster than training on the entire dataset, since after having used a chunk for training, that chunk is no longer read and used subsequently (this effect is prominent when remote ROOT files are used). Finally, user should be aware of potential divergence of ML model when training last chunk of the dataset and check for bias towards last chunk. For instance, user may implement a K-fold cross validation approach to train on N-1 chunks (i.e. folds in this case) and use one chunk for validation. A detailed description of how to use the workflow.py module for training a ML model reading ROOT files from the opendata portal, can be found here . Please see how the user has to provide several information when run the workflow.py module, e.g. the definition of the ML model, and then is task of MLaaS4HEP framework to perform all the training procedure using the ML model provided by the user. For a complete description of MLaaS4HEP see this paper.","title":"MLaaS4HEP"},{"location":"training/MLaaS4HEP.html#machine-learning-as-a-service-for-hep","text":"MLaaS for HEP is a set of Python-based modules to support reading HEP data and stream them to the ML tool of the user's choice. It consists of three independent layers: - Data Streaming layer to handle remote data, see reader.py - Data Training layer to train ML model for given HEP data, see workflow.py - Data Inference layer, see tfaas_client.py The MLaaS4HEP resopitory can be found here . The general architecture of MLaaS4HEP looks like this: Even though this architecture was originally developed for dealing with HEP ROOT files, we extend it to other data formats. As of right now, following data formats are supported: JSON, CSV, Parquet, and ROOT. All of the formats support reading files from the local file system or HDFS, while the ROOT format supports reading files via the XRootD protocol. The pre-trained models can be easily uploaded to TFaaS inference server for serving them to clients. The TFaaS documentation can be found here .","title":"Machine Learning as a Service for HEP"},{"location":"training/MLaaS4HEP.html#dependencies","text":"Here is a list of the dependencies: - pyarrow for reading data from HDFS file system - uproot for reading ROOT files - numpy , pandas for data representation - modin for fast panda support - numba for speeing up individual functions","title":"Dependencies"},{"location":"training/MLaaS4HEP.html#installation","text":"The easiest way to install and run MLaaS4HEP and TFaaS is to use pre-build docker images # run MLaaS4HEP docker container docker run veknet/mlaas4hep # run TFaaS docker container docker run veknet/tfaas","title":"Installation"},{"location":"training/MLaaS4HEP.html#reading-root-files","text":"MLaaS4HEP python repository provides the reader.py module that defines a DataReader class able to read either local or remote ROOT files (via xrootd) in chunks. It is based on the uproot framework. Basic usage # setup the proper environment, e.g. # export PYTHONPATH=/path/src/python # path to MLaaS4HEP python framework # export PATH=/path/bin:$PATH # path to MLaaS4HEP binaries # get help and option description reader --help # here is a concrete example of reading local ROOT file: reader --fin=/opt/cms/data/Tau_Run2017F-31Mar2018-v1_NANOAOD.root --info --verbose=1 --nevts=2000 # here is an example of reading remote ROOT file: reader --fin=root://cms-xrd-global.cern.ch//store/data/Run2017F/Tau/NANOAOD/31Mar2018-v1/20000/6C6F7EAE-7880-E811-82C1-008CFA165F28.root --verbose=1 --nevts=2000 --info # both of aforementioned commands produce the following output Reading root://cms-xrd-global.cern.ch//store/data/Run2017F/Tau/NANOAOD/31Mar2018-v1/20000/6C6F7EAE-7880-E811-82C1-008CFA165F28.root # 1000 entries, 883 branches, 4.113945007324219 MB, 0.6002757549285889 sec, 6.853425235896175 MB/sec, 1.6659010326328503 kHz # 1000 entries, 883 branches, 4.067909240722656 MB, 1.3497390747070312 sec, 3.0138486148558896 MB/sec, 0.740883937302516 kHz ###total time elapsed for reading + specs computing: 2.2570559978485107 sec; number of chunks 2 ###total time elapsed for reading: 1.9500117301940918 sec; number of chunks 2 --- first pass: 1131872 events, (648-flat, 232-jagged) branches, 2463 attrs VMEM used: 29.896704 (MB) SWAP used: 0.0 (MB) <__main__.RootDataReader object at 0x7fb0cdfe4a00> init is complete in 2.265552043914795 sec Number of events : 1131872 # flat branches : 648 CaloMET_phi values in [-3.140625, 3.13671875] range, dim=N/A CaloMET_pt values in [0.783203125, 257.75] range, dim=N/A CaloMET_sumEt values in [820.0, 3790.0] range, dim=N/A More examples about using uproot may be found here and here .","title":"Reading ROOT files"},{"location":"training/MLaaS4HEP.html#how-to-train-ml-models-on-hep-root-data","text":"The MLaaS4HEP framework allows to train ML models in different ways: - using full dataset (i.e. the entire amount of events stored in input ROOT files) - using chunks, as subsets of a dataset, which dimension can be chosen directly by the user and can vary between 1 and the total number of events - using local or remote ROOT files. The training phase is managed by the workflow.py module which performs the following actions: - read all input ROOT files in chunks to compute a specs file (where the main information about the ROOT files are stored: the dimension of branches, the minimum and the maximum for each branch, and the number of events for each ROOT file) - perform the training cycle (each time using a new chunk of events) - create a new chunk of events taken proportionally from the input ROOT files - extract and convert each event in a list of NumPy arrays - normalize the events - fix the Jagged Arrays dimension - create the masking vector - use the chunk to train the ML model provided by the user A schematic representation of the steps performed in the MLaaS4HEP pipeline, in particular those inside the Data Streaming and Data Training layers, is: If the dataset is large and exceed the amount of RAM on the training node, then the user should consider the chunk approach. This allows to train the ML model each time using a different chunk, until the entire dataset is completely read. In this case the user should pay close attention to the ML model convergence, and validate it after each chunk. For more information look at this , this and this . Using different training approach has pros and cons. For instance, training on entire dataset can guarantee the ML model convergence, but the dataset should fits into RAM of the training node. While chunk approach allows to split the dataset to fit in the hardware resources, but it requires proper model evaluation after each chunk training. In terms of training speed, this choice should be faster than training on the entire dataset, since after having used a chunk for training, that chunk is no longer read and used subsequently (this effect is prominent when remote ROOT files are used). Finally, user should be aware of potential divergence of ML model when training last chunk of the dataset and check for bias towards last chunk. For instance, user may implement a K-fold cross validation approach to train on N-1 chunks (i.e. folds in this case) and use one chunk for validation. A detailed description of how to use the workflow.py module for training a ML model reading ROOT files from the opendata portal, can be found here . Please see how the user has to provide several information when run the workflow.py module, e.g. the definition of the ML model, and then is task of MLaaS4HEP framework to perform all the training procedure using the ML model provided by the user. For a complete description of MLaaS4HEP see this paper.","title":"How to train ML models on HEP ROOT data"},{"location":"validation/cross_validation.html","text":"How to do cross validation Other sources of information","title":"Cross validation"},{"location":"validation/overtraining.html","text":"In progress... Clear examples of overtraining, how to spot it, and how to prevent it.","title":"Overtraining"}]}